# Finetuner un mod√®le de classification musicale

Dans cette section, nous pr√©senterons un guide √©tape par √©tape sur le *finetuning* d'un *transformer* encodeur pour la classification de la musique.
Nous utiliserons un mod√®le l√©ger pour cette d√©monstration et un jeu de donn√©es assez petit, ce qui signifie que le code est ex√©cutable de bout en bout sur n'importe quel GPU grand public, y compris le GPU T4 16GB fourni gratuitement par Google Colab. La section comprend divers conseils que vous pouvez essayer si vous avez un GPU plus petit et rencontrez des probl√®mes de m√©moire en cours de route.

## Le de donn√©es

Pour entra√Æner notre mod√®le, nous utiliserons le jeu de donn√©es [GTZAN](https://huggingface.co/datasets/marsyas/gtzan), qui est un jeu de donn√©es populaire de 1 000 chansons pour la classification des genres musicaux. Chaque chanson dure 30 secondes et fait partie de l'un des 10 genres de musique disponible, allant du disco au m√©tal. Nous pouvons obtenir les fichiers audio et leurs √©tiquettes correspondantes √† partir du *Hub* avec la fonction `load_dataset()`  de ü§ó *Datasets* :

```python
from datasets import load_dataset

gtzan = load_dataset("marsyas/gtzan", "all")
gtzan
``` 

**Sortie :**

```out
Dataset({
    features: ['file', 'audio', 'genre'],
    num_rows: 999
})
``` 

<Tip warning={true}>
L'un des enregistrements dans GTZAN est corrompu, il a donc √©t√© supprim√© du jeu de donn√©es. C'est pourquoi nous avons 999 exemples au lieu de 1 000.
</Tip>

GTZAN ne fournit pas de jeu de validation pr√©d√©fini, nous devrons donc en cr√©er un nous-m√™mes. Le jeu de donn√©es est √©quilibr√© entre les genres, nous pouvons donc utiliser la m√©thode `train_test_split()` pour cr√©er rapidement une r√©partition 90/10 comme suit :

```python
gtzan = gtzan["train"].train_test_split(seed=42, shuffle=True, test_size=0.1)
gtzan
``` 

**Sortie :**

```out
DatasetDict({
    train: Dataset({
        features: ['file', 'audio', 'genre'],
        num_rows: 899
    })
    test: Dataset({
        features: ['file', 'audio', 'genre'],
        num_rows: 100
    })
})
``` 

Maintenant que nous avons nos jeux d‚Äôentra√Ænement et de validation, jetons un coup d'≈ìil √† l'un des fichiers audio :

```python
gtzan["train"][0]
``` 

**Sortie :**

```out
{
    "file": "~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav",
    "audio": {
        "path": "~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav",
        "array": array(
            [
                0.10720825,
                0.16122437,
                0.28585815,
                ...,
                -0.22924805,
                -0.20629883,
                -0.11334229,
            ],
            dtype=float32,
        ),
        "sampling_rate": 22050,
    },
    "genre": 7,
}
``` 

Comme nous l'avons vu dans [Unit√© 1](.. /chapter1/audio_data), les fichiers audio sont repr√©sent√©s sous forme de tableaux NumPy √† 1 dimension, o√π la valeur du tableau repr√©sente l'amplitude √† ce pas de temps. Pour ces chansons, la fr√©quence d'√©chantillonnage est de 22 050 Hz, ce qui signifie qu'il y a 22 050 valeurs d'amplitude √©chantillonn√©es par seconde. Nous devrons garder cela √† l'esprit lorsque nous utiliserons un mod√®le pr√©-entra√Æn√© avec un taux d'√©chantillonnage diff√©rent, en convertissant nous-m√™mes les taux d'√©chantillonnage pour nous assurer qu'ils correspondent. Nous pouvons √©galement voir que le genre est repr√©sent√© sous la forme d'un entier, ou _class label_, qui est le format dans lequel le mod√®le fera ses pr√©dictions. Utilisons la m√©thode `int2str()` de la caract√©ristique `genre` pour associer ces entiers √† des noms lisibles par l'homme :

```python
id2label_fn = gtzan["train"].features["genre"].int2str
id2label_fn(gtzan["train"][0]["genre"])
``` 

**Sortie :**

```out
'pop'
```

Cette √©tiquette semble correcte, car elle correspond au nom de fichier du fichier audio. √âcoutons maintenant quelques exemples suppl√©mentaires en utilisant Gradio pour cr√©er une interface simple avec l'API `Blocks` :

```python
def generate_audio():
    example = gtzan["train"].shuffle()[0]
    audio = example["audio"]
    return (
        audio["sampling_rate"],
        audio["array"],
    ), id2label_fn(example["genre"])


with gr.Blocks() as demo:
    with gr.Column():
        for _ in range(4):
            audio, label = generate_audio()
            output = gr.Audio(audio, label=label)

demo.launch(debug=True)
```

<iframe src="https://course-demos-gtzan-samples.hf.space" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

√Ä partir de ces √©chantillons, nous pouvons certainement entendre la diff√©rence entre les genres, mais un *transformer* peut-il le faire aussi ? Entra√Ænons un mod√®le pour le d√©couvrir ! Tout d'abord, nous devrons trouver un mod√®le pr√©-entra√Æn√© appropri√© pour cette t√¢che. Voyons comment nous pouvons le faire.

## Choisir un mod√®le pr√©-entra√Æn√© pour la classification audio

Pour commencer, choisissons un mod√®le pr√©-entra√Æn√© appropri√© pour la classification audio. Dans ce domaine, le pr√©-entra√Ænement est g√©n√©ralement effectu√©e sur de grandes quantit√©s de donn√©es audio non √©tiquet√©es, en utilisant des jeux de donn√©es tels que [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) et [Voxpopuli](https://huggingface.co/datasets/facebook/voxpopuli). La meilleure fa√ßon de trouver ces mod√®les sur le *Hub* est d'utiliser le filtre ¬´ classification audio ¬ª, comme d√©crit dans la section pr√©c√©dente. Bien que des mod√®les comme Wav2Vec2 et HuBERT soient tr√®s populaires, nous utiliserons un mod√®le appel√© _DistilHuBERT_. Il s'agit d'une version beaucoup plus petite (ou distill√©e) du mod√®le [HuBERT](https://huggingface.co/docs/transentra√Æners/model_doc/hubert), qui s‚Äôentra√Æne environ 73% plus vite, tout en pr√©servant la plupart des performances.

<iframe src="https://autoevaluate-leaderboards.hf.space" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

## De l'audio aux caract√©ristiques d'apprentissage automatique

## Pr√©traitement des donn√©es

√Ä l'instar de la tokenisation en NLP, les mod√®les audio et vocaux n√©cessitent que l'entr√©e soit encod√©e dans un format que le mod√®le peut traiter. Dans ü§ó *Transformers*, la conversion de l'audio au format d'entr√©e est g√©r√©e par l‚Äôextracteur de caract√©ristique du mod√®le. ü§ó *Transformers* fournit une classe pratique `AutoFeatureExtractor` qui peut s√©lectionner automatiquement le bon extracteur de caract√©ristiques pour un mod√®le donn√©. Pour voir comment nous pouvons traiter nos fichiers audio, commen√ßons par instancier l'extracteur de caract√©ristiques pour DistilHuBERT √† partir du *checkpoint* pr√©-entra√Æn√© :

```python
from transformers import AutoFeatureExtractor

model_id = "ntu-spml/distilhubert"
feature_extractor = AutoFeatureExtractor.from_pretrained(
    model_id, do_normalize=True, return_attention_mask=True
)
``` 

Comme le taux d'√©chantillonnage du mod√®le et de le jeu de donn√©es est diff√©rent, nous devons r√©√©chantillonner le fichier audio √† 16 000 Hz avant de le transmettre √† l'extracteur de caract√©ristiques. Nous pouvons le faire en obtenant d'abord la fr√©quence d'√©chantillonnage du mod√®le √† partir de l'extracteur de caract√©ristiques :

```python
sampling_rate = feature_extractor.sampling_rate
sampling_rate
```

**Sortie :**

```out
16000
``` 

Ensuite, nous r√©√©chantillonnons le jeu de donn√©es √† l'aide de la m√©thode `cast_column()` et de la fonction `Audio`  de ü§ó *Datasets* :

```python
from datasets import Audio

gtzan = gtzan.cast_column("audio", Audio(sampling_rate=sampling_rate))
``` 

Nous pouvons maintenant v√©rifier le premier √©chantillon de l‚Äô√©chantillon d‚Äôentra√Ænement de notre jeu de donn√©es pour v√©rifier qu'il est bien √† 16 000 Hz. ü§ó *Datasets* r√©√©chantillonnent le fichier audio *√† la vol√©e* lorsque nous chargeons chaque √©chantillon audio :

```python
gtzan["train"][0]
``` 

**Sortie :**

```out
{
    "file": "~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav",
    "audio": {
        "path": "~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav",
        "array": array(
            [
                0.0873509,
                0.20183384,
                0.4790867,
                ...,
                -0.18743178,
                -0.23294401,
                -0.13517427,
            ],
            dtype=float32,
        ),
        "sampling_rate": 16000,
    },
    "genre": 7,
}
``` 

Bien. Nous pouvons voir que le taux d'√©chantillonnage a √©t√© sous-√©chantillonn√© √† 16kHz. Les valeurs de tableau sont √©galement diff√©rentes car nous n'avons maintenant qu'environ une valeur d'amplitude pour chaque 1,5 pas que nous avions auparavant.
Une caract√©ristique de conception des mod√®les de type Wav2Vec2 et HuBERT est qu'ils acceptent un tableau de flottants correspondant √† la forme d'onde brute du signal vocal comme entr√©e. Cela contraste avec d'autres mod√®les, comme Whisper, o√π nous pr√©traitons la forme d'onde audio brute au format spectrogramme.
Nous avons mentionn√© que les donn√©es audio sont repr√©sent√©es comme un tableau √† 1 dimension, elles sont donc d√©j√† dans le bon format pour √™tre lues par le mod√®le (un ensemble d'entr√©es continues √† pas de temps discrets). Alors, que fait exactement l'extracteur de caract√©ristiques ?
Eh bien, les donn√©es audio sont dans le bon format, mais nous n'avons impos√© aucune restriction sur les valeurs qu'elles peuvent prendre. Pour que notre mod√®le fonctionne de mani√®re optimale, nous voulons conserver toutes les entr√©es dans la m√™me plage dynamique. Cela va nous assurer d'obtenir une plage similaire d'activations et de gradients pour nos √©chantillons, ce qui nous aidera √† la stabilit√© et √† la convergence pendant l'entra√Ænement.
Pour ce faire, nous *normalisons* nos donn√©es audio, en redimensionnant chaque √©chantillon √† une moyenne nulle et une variance unitaire pour avoir des variables centr√©es r√©duites. C'est exactement cette normalisation des caract√©ristiques que notre extracteur de caract√©ristiques effectue.
Nous pouvons jeter un coup d'≈ìil √† l'extracteur de caract√©ristiques en fonctionnement en l'appliquant √† notre premier √©chantillon audio. Tout d'abord, calculons la moyenne et la variance de nos donn√©es audio brutes :

```python
import numpy as np

sample = gtzan["train"][0]["audio"]

print(f"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}")
```

**Sortie :**

```out
Mean: 0.000185, Variance: 0.0493
``` 

Nous pouvons voir que la moyenne est d√©j√† proche de 0, mais la variance est plus proche de 0,05. Si la variance de l'√©chantillon √©tait plus grande, cela pourrait causer des probl√®mes √† notre mod√®le, car la plage dynamique des donn√©es audio serait tr√®s petite et donc difficile √† s√©parer. Appliquons l'extracteur de caract√©ristiques et voyons √† quoi ressemblent les sorties:

```python
inputs = feature_extractor(sample["array"], sampling_rate=sample["sampling_rate"])

print(f"inputs keys: {list(inputs.keys())}")

print(
    f"Mean: {np.mean(inputs['input_values']):.3}, Variance: {np.var(inputs['input_values']):.3}"
)
``` 

**Sortie :**

```out
inputs keys: ['input_values', 'attention_mask']
Mean: -4.53e-09, Variance: 1.0
``` 

Notre extracteur de caract√©ristiques renvoie un dictionnaire de deux tableaux : `input_values` et `attention_mask`. Les `input_values` sont les entr√©es audio pr√©trait√©es que nous passerons au mod√®le HuBERT. L‚Äô[`attention_mask`](https://huggingface.co/docs/transentra√Æners/glossary#attention-mask) est utilis√© lorsque nous traitons un _batch_ d'entr√©es audio √† la fois. Il est utilis√© pour indiquer au mod√®le o√π nous avons des entr√©es rembourr√©es de diff√©rentes longueurs.
Nous pouvons voir que la valeur moyenne est maintenant beaucoup plus proche de 0, et la variance est √† 1 ! C'est exactement la forme sous laquelle nous voulons que nos √©chantillons audio soient avant de les passer dans notre HuBERT.

<Tip warning={true}>
Notez comment nous avons transmis le taux d'√©chantillonnage de nos donn√©es audio √† notre extracteur de caract√©ristiques. Il s'agit d'une bonne pratique, car l'extracteur de caract√©ristiques effectue une v√©rification sous le capot pour s'assurer que le taux d'√©chantillonnage de nos donn√©es audio correspond au taux d'√©chantillonnage attendu par le mod√®le. Si le taux d'√©chantillonnage de nos donn√©es audio ne correspondait pas au taux d'√©chantillonnage de notre mod√®le, nous aurions besoin de sur√©chantillonner ou de sous-√©chantillonner les donn√©es audio au taux d'√©chantillonnage correct.
</Tip>

Alors maintenant que nous savons comment traiter nos fichiers audio r√©√©chantillonn√©s, la derni√®re chose √† faire est de d√©finir une fonction que nous pouvons appliquer √† tous les exemples du jeu de donn√©es. √âtant donn√© que nous nous attendons √† des √©chantillons de 30 secondes, nous tronquerons aussi les √©chantillons plus longs en utilisant les arguments `max_length` et `truncation` de l'extracteur de caract√©ristiques comme suit :

```python
max_duration = 30.0


def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=feature_extractor.sampling_rate,
        max_length=int(feature_extractor.sampling_rate * max_duration),
        truncation=True,
        return_attention_mask=True,
    )
    return inputs
```

Une fois cette fonction d√©finie, nous pouvons maintenant l'appliquer au jeu de donn√©es √† l'aide de la m√©thode `map()`.

```python
gtzan_encoded = gtzan.map(
    preprocess_function, remove_columns=["audio", "file"], batched=True, num_proc=1
)
gtzan_encoded
```

**Sortie :**

```out
DatasetDict({
    train: Dataset({
        features: ['genre', 'input_values'],
        num_rows: 899
    })
    test: Dataset({
        features: ['genre', 'input_values'],
        num_rows: 100
    })
})
```

Pour simplifier l‚Äôentra√Ænement, nous avons supprim√© les colonnes `audio` et `file` du jeu de donn√©es. La colonne `input_values` contient les fichiers audio encod√©s, la colonne `attention_mask` est un masque binaire de valeurs 0/1 qui indique o√π nous avons rempli l'entr√©e audio, et la colonne `genre` contient les √©tiquettes (ou cibles) correspondantes. Pour permettre au `Trainer` de traiter les √©tiquettes de classe, nous devons renommer la colonne `genre` en `label` :

```python
gtzan_encoded = gtzan_encoded.rename_column("genre", "label")
```

Enfin, nous devons obtenir les associations d'√©tiquettes √† partir du jeu de donn√©es. L‚Äôassociation nous m√®nera des identifiants entiers (par exemple `7`) aux √©tiquettes de classe lisibles par l'homme (par exemple `pop`) et inversement. Ce faisant, nous pouvons convertir la pr√©diction de notre mod√®le dans un format lisible par l'homme, ce qui nous permet d'utiliser le mod√®le dans n'importe quelle application en aval. Nous pouvons le faire en utilisant la m√©thode `int2str()` comme suit:

```python
id2label = {
    str(i): id2label_fn(i)
    for i in range(len(gtzan_encoded["train"].features["label"].names))
}
label2id = {v: k for k, v in id2label.items()}

id2label["7"]
```

**Sortie :**

```out
'pop'
```

Nous avons maintenant un jeu de donn√©es pr√™t pour l‚Äôentra√Ænement. Voyons comment nous pouvons entra√Æner un mod√®le sur ce jeu de donn√©es.

## Finetuner le mod√®le

Pour affiner le mod√®le, nous utiliserons la classe 'Trainer' de ü§ó *Transformers*. Comme nous l'avons vu dans d'autres chapitres, le ¬´ Trainer ¬ª est une API de haut niveau con√ßue pour g√©rer les sc√©narios de formation les plus courants. Dans ce cas, nous utiliserons le 'Trainer' pour affiner le mod√®le sur GTZAN. Pour ce faire, nous devons d'abord charger un mod√®le pour cette t√¢che. Nous pouvons le faire en utilisant la classe 'AutoModelForAudioClassification', qui ajoutera automatiquement la t√™te de classification appropri√©e √† notre mod√®le DistilHuBERT pr√©entra√Æn√©. Allons de l'avant et instancions le mod√®le :

'''python
from transformers import AutoModelForAudioClassification

num_labels = len(id2label)

mod√®le = AutoModelForAudioClassification.from_pretrained(
    model_id,
    num_labels=num_labels,
    label2id=label2id,
    id2label=id2label,
)
```

Nous vous conseillons fortement de t√©l√©charger les *checkpoints* du mod√®le directement sur le [*Hub*](https://huggingface.co/) pendant l'entra√Ænement.
Le *Hub* fournit :
- Un contr√¥le de version int√©gr√© : vous pouvez √™tre s√ªr qu'aucun *checkpoint* du mod√®le n'est perdu pendant l‚Äôentra√Ænement.
- Tensorboard : suivez les m√©triques importantes au cours de l‚Äôentra√Ænement.
- La carte du mod√®le : documentant ce que fait un mod√®le et ses cas d'utilisation pr√©vus.
- Communaut√© : un moyen facile de partager et de collaborer avec la communaut√© ! 
Lier le *notebook* au *Hub* est simple. Il suffit d'entrer votre *token* d'authentification au *Hub* lorsque vous y √™tes invit√©.
Vous pouvez trouver votre *token* d'authentification [ici](https://huggingface.co/settings/tokens) et le saisir dans

```python
from huggingface_hub import notebook_login

notebook_login()
```

**Sortie :**

```bash
Login successful
Your token has been saved to /root/.huggingface/token
```

L'√©tape suivante consiste √† d√©finir les arguments d'apprentissage, y compris la taille du batch, les √©tapes d'accumulation du gradient, le nombre d'√©poques d'apprentissage et le taux d'apprentissage :

```python
from transformers import TrainingArguments

model_name = model_id.split("/")[-1]
batch_size = 8
gradient_accumulation_steps = 1
num_train_epochs = 10

training_args = TrainingArguments(
    f"{model_name}-finetuned-gtzan",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_train_epochs,
    warmup_ratio=0.1,
    logging_steps=5,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    fp16=True,
    push_to_hub=True,
)
```

<Tip warning={true}>
Ici, nous avons d√©fini `push_to_hub=True` pour activer le t√©l√©chargement automatique de nos *checkpoints* *finetun√©s* pendant l'entra√Ænement. Si vous ne souhaitez pas que vos *checkpoints*  soient t√©l√©charg√©s sur le *Hub*, vous pouvez d√©finir cette option sur `False`.
</Tip>

La derni√®re chose que nous devons faire est de d√©finir les m√©triques. √âtant donn√© que le jeu de donn√©es est √©quilibr√©, nous utiliserons la pr√©cision comme m√©trique et le chargerons √† l'aide de la biblioth√®que ü§ó *Evaluate* :

```python
import evaluate

metric = evaluate.load("accuracy")


def compute_metrics(eval_pred):
    """Computes accuracy on a batch of predictions"""
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return metric.compute(predictions=predictions, references=eval_pred.label_ids)
``` 

Nous avons maintenant toutes les pi√®ces ! Instancions le `Trainer` et entra√Ænons le mod√®le :

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=gtzan_encoded["train"],
    eval_dataset=gtzan_encoded["test"],
    tokenizer=feature_extractor,
    compute_metrics=compute_metrics,
)

trainer.train()
``` 

<Tip warning={true}>
Selon votre GPU, il est possible que vous rencontriez une erreur CUDA `"out-of-memory"` lorsque vous commencez √† vous entra√Æner.
Dans ce cas, vous pouvez r√©duire la taille du batch de mani√®re incr√©mentielle par des facteurs de 2 et utiliser ['gradient_accumulation_steps'](https://huggingface.co/docs/transentra√Æners/main_classes/trainer#transentra√Æners.TrainingArguments.gradient_accumulation_steps) pour compenser.
</Tip>

**Sortie :**

```out
| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 1.7297        | 1.0   | 113  | 1.8011          | 0.44     |
| 1.24          | 2.0   | 226  | 1.3045          | 0.64     |
| 0.9805        | 3.0   | 339  | 0.9888          | 0.7      |
| 0.6853        | 4.0   | 452  | 0.7508          | 0.79     |
| 0.4502        | 5.0   | 565  | 0.6224          | 0.81     |
| 0.3015        | 6.0   | 678  | 0.5411          | 0.83     |
| 0.2244        | 7.0   | 791  | 0.6293          | 0.78     |
| 0.3108        | 8.0   | 904  | 0.5857          | 0.81     |
| 0.1644        | 9.0   | 1017 | 0.5355          | 0.83     |
| 0.1198        | 10.0  | 1130 | 0.5716          | 0.82     |
``` 

L‚Äôentra√Ænement durera environ 1 heure en fonction de votre GPU ou de celui allou√© par Google Colab. Notre meilleure pr√©cision d'√©valuation est de 83%. Pas mal pour seulement 10 √©poques avec 899 exemples d'entra√Ænement ! Nous pourrions certainement am√©liorer ce r√©sultat en entra√Ænant sur plus d'√©poques, en utilisant des techniques de r√©gularisation telles que le *dropout*, ou en d√©coupant chaque segment d‚Äôaudio de 30s en segments de 15s pour utiliser une strat√©gie de pr√©traitement de donn√©es plus efficace.
La grande question est de savoir comment ce syst√®me se compare √† d'autres syst√®mes de classification de musique ü§î
Pour cela, nous pouvons afficher le [classement *autoevaluate*](https://huggingface.co/spaces/autoevaluate/leaderboards?dataset=marsyas%2Fgtzan&only_verified=0&task=audio-classification&config=all&split=train&metric=accuracy), un classement qui cat√©gorise les mod√®les par langue et jeu de donn√©es, puis les classe en fonction de leur pr√©cision.
Nous pouvons automatiquement soumettre notre *checkpoint* au classement lorsque nous transmettons les r√©sultats de l'entra√Ænement au *Hub*. Nous devons simplement d√©finir les arguments appropri√©s (kwargs). Vous pouvez modifier ces valeurs pour qu'elles correspondent √† votre jeu de donn√©es, √† votre langue et au nom de votre mod√®le en cons√©quence :

```python
kwargs = {
    "dataset_tags": "marsyas/gtzan",
    "dataset": "GTZAN",
    "model_name": f"{model_name}-finetuned-gtzan",
    "finetuned_from": model_id,
    "tasks": "audio-classification",
}
``` 

Les r√©sultats de l‚Äôentra√Ænement peuvent maintenant √™tre t√©l√©charg√©s sur le *Hub*. Pour ce faire, ex√©cutez la commande `.push_to_hub` :

```python
trainer.push_to_hub(**kwargs)
```

Cela enregistrera les logs d'entra√Ænement et les poids du mod√®le sous `"your-username/distilhubert-finetuned-gtzan"`. Pour cet exemple, consultez [`"sanchit-gandhi/distilhubert-finetuned-gtzan"`](https://huggingface.co/sanchit-gandhi/distilhubert-finetuned-gtzan).

## Partager le mod√®le

Vous pouvez d√©sormais partager votre mod√®le avec n'importe qui en utilisant le lien sur le *Hub*. Il sera utilisable avec l'identifiant `"your-username/distilhubert-finetuned-gtzan"` directement dans la classe `pipeline()`. Par exemple, pour charger le *checkpoint* *finetun√©* ['"sanchit-gandhi/distilhubert-finetuned-gtzan"'](https://huggingface.co/sanchit-gandhi/distilhubert-finetuned-gtzan):

```python
from transformers import pipeline

pipe = pipeline(
    "audio-classification", model="sanchit-gandhi/distilhubert-finetuned-gtzan"
)
```

## Conclusion

Dans cette section, nous avons couvert un guide √©tape par √©tape pour *finetuner* le mod√®le DistilHuBERT pour la t√¢che de classification de la musique. Bien que nous nous soyons concentr√©s sur cette t√¢che et le jeu de donn√©es GTZAN, les √©tapes pr√©sent√©es ici s'appliquent plus g√©n√©ralement √† toute t√¢che de classification audio. Le m√™me script peut √™tre utilis√© pour les t√¢ches de d√©tection de mots-cl√©s ou l'identification de la langue. Il vous suffit de changer le jeu de donn√©es avec celui de votre t√¢che d'int√©r√™t ! Si vous souhaitez *finetuner* d'autres mod√®les du *Hub* pour la classification d‚Äôaudio, nous vous encourageons √† consulter les [exemples](https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification) trouvables sur le d√©p√¥t ü§ó *Transformers*.
Dans la section suivante, nous prendrons le mod√®le que nous venons de *finetuner* et construirons une d√©mo avec *Gradio* que nous pourrons partager sur le Hub.
