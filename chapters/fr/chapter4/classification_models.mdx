# Mod√®les et jeux de donn√©es pr√©-entra√Æn√©s pour la classification d‚Äôaudio

Le *Hub* abrite [plusieurs centaines de mod√®les pr√©-entra√Æn√©s pour la classification d‚Äôaudio](https://huggingface.co/models?pipeline_tag=audio-classification). Dans cette section, nous passerons en revue certaines des t√¢ches de classification d‚Äôaudio les plus courantes et sugg√©rerons des mod√®les pr√©-entra√Æn√©s appropri√©s pour chacune. En utilisant la classe `pipeline()`, la commutation entre les mod√®les et les t√¢ches est simple : une fois que vous savez comment utiliser `pipeline()` pour un mod√®le, vous pourrez l'utiliser pour n'importe quel mod√®le sur le *Hub*, sans modification du code ! Cela rend l'exp√©rimentation de la classe `pipeline()` extr√™mement rapide, ce qui vous permet de s√©lectionner rapidement le meilleur mod√®le pr√©-entra√Æn√© pour vos besoins.
Avant de passer aux diff√©rents probl√®mes de classification d‚Äôaudio, r√©capitulons rapidement les architectures de *transformers* g√©n√©ralement utilis√©es. L'architecture standard de classification d‚Äôaudio est motiv√©e par la nature de la t√¢che. Nous voulons transformer une s√©quence d'entr√©es audio (c'est-√†-dire notre r√©seau audio d'entr√©e) en une pr√©diction d'√©tiquette de classe unique. Les mod√®les d'encodeur associent d'abord la s√©quence audio d'entr√©e dans une s√©quence de repr√©sentations √† l'√©tat cach√© en faisant passer les entr√©es √† travers un bloc *transformer*. La s√©quence de repr√©sentations d'√©tats masqu√©s est ensuite associ√©e √† une sortie d'√©tiquette de classe en prenant la moyenne sur les √©tats masqu√©s et en faisant passer le vecteur r√©sultant √† travers une couche de classification lin√©aire. Par cons√©quent, il y a une pr√©f√©rence pour les mod√®les *encodeur* pour la classification d‚Äôaudio.
Les mod√®les de d√©codeur introduisent une complexit√© inutile √† la t√¢che car ils supposent que les sorties peuvent √©galement √™tre une *s√©quence* de pr√©dictions (plut√¥t qu'une pr√©diction d'√©tiquette de classe unique), et g√©n√®rent ainsi plusieurs sorties. Par cons√©quent, ils ont une vitesse d'inf√©rence plus lente et ont tendance √† ne pas √™tre utilis√©s. Les mod√®les encodeur-d√©codeur sont largement omis pour la m√™me raison. Ces choix d'architecture sont analogues √† ceux de NLP, o√π les mod√®les d'encodeur tels que BERT sont privil√©gi√©s pour les t√¢ches de classification de s√©quences, et les mod√®les de d√©codeur tels que GPT r√©serv√©s aux t√¢ches de g√©n√©ration de s√©quences.
Maintenant que nous avons r√©capitul√© l'architecture du *transformer* standard pour la classification d‚Äôaudio, passons aux diff√©rents sous-ensembles de la classification d‚Äôaudio et couvrons les mod√®les les plus populaires !

## ü§ó Installation de Transformers

Au moment de la r√©daction de cette section, les derni√®res mises √† jour requises pour le pipeline de classification d‚Äôaudio se trouvent uniquement sur la version ¬´ principale ¬ª du d√©p√¥t ü§ó Transformers, plut√¥t que sur la derni√®re version de PyPi. Pour nous assurer que nous avons ces mises √† jour localement, nous allons installer Transformers √† partir de la branche `main` avec la commande suivante :

```
pip install git+https://github.com/huggingface/transformers
```

## Rep√©rage de mots-cl√©s

Le rep√©rage de mots cl√©s (KWS pour *Keyword spotting*) est la t√¢che d'identifier un mot-cl√© dans un discours. L'ensemble des mots-cl√©s possibles forme l'ensemble des √©tiquettes de classe pr√©dites. Par cons√©quent, pour utiliser un mod√®le de rep√©rage de mots cl√©s pr√©-entra√Æn√©, vous devez vous assurer que vos mots-cl√©s correspondent √† ceux sur lesquels le mod√®le a √©t√© pr√©-entra√Æn√©. Ci-dessous, nous pr√©senterons deux jeux de donn√©es et mod√®les pour la d√©tection de mots cl√©s.

### MINDS-14

Commen√ßons en utilisant le m√™me jeu de donn√©es [MINDS-14](https://huggingface.co/datasets/PolyAI/minds14) explor√© dans l'unit√© pr√©c√©dente. Si vous vous souvenez, MINDS-14 contient des enregistrements de personnes posant des questions √† un syst√®me bancaire √©lectronique dans plusieurs langues et dialectes, et a indique une classe d‚Äôintention pour chaque enregistrement. Nous pouvons donc classer les enregistrements par intention de l'appel.

```python
from datasets import load_dataset

minds = load_dataset("PolyAI/minds14", name="en-AU", split="train")
```

Nous allons charger le *checkpoint* [`"anton-l/xtreme_s_xlsr_300m_minds14"`](https://huggingface.co/anton-l/xtreme_s_xlsr_300m_minds14), qui est un mod√®le XLS-R *finetun√©* sur MINDS-14 pendant environ 50 √©poques. Il atteint une pr√©cision de 90% sur toutes les langues de MINDS-14 sur l'ensemble d'√©valuation.

```python
from transformers import pipeline

classifier = pipeline(
    "audio-classification",
    model="anton-l/xtreme_s_xlsr_300m_minds14",
)
```

Enfin, nous pouvons passer un √©chantillon au pipeline de classification pour faire une pr√©diction :

```python
classifier(minds[0]["path"])
```

**Sortie :**

```
[
    {"score": 0.9631525278091431, "label": "pay_bill"},
    {"score": 0.02819698303937912, "label": "freeze"},
    {"score": 0.0032787492964416742, "label": "card_issues"},
    {"score": 0.0019414445850998163, "label": "abroad"},
    {"score": 0.0008378693601116538, "label": "high_value_payment"},
]
```

Nous avons identifi√© que l'intention de l'appel √©tait de payer une facture, avec une probabilit√© de 96%. Vous pouvez imaginer que ce type de syst√®me de rep√©rage de mots-cl√©s soit utilis√© comme premi√®re √©tape d'un centre d'appels automatis√©, o√π nous voulons cat√©goriser les appels entrants des clients en fonction de leur requ√™te et leur offrir un support contextualis√© en cons√©quence.

### Speech Commands

Speech Commands est un jeu de donn√©es de mots parl√©s con√ßu pour √©valuer les mod√®les de classification d‚Äôaudio sur des mots de commande simples.
Le jeu de donn√©es se compose de 15 classes de mots-cl√©s, d'une classe pour le silence et d'une classe inconnue pour inclure le faux positif.
Les 15 mots-cl√©s sont des mots uniques qui seraient g√©n√©ralement utilis√©s dans les param√®tres sur l'appareil pour contr√¥ler les t√¢ches de base ou lancer d'autres processus.
Un mod√®le similaire fonctionne en continu sur votre t√©l√©phone mobile. Ici, au lieu d'avoir des mots de commande uniques, nous avons des mots de r√©veil sp√©cifiques √† votre appareil, tels que ¬´ Hey Google ¬ª ou ¬´ Hey Siri ¬ª. Lorsque le mod√®le de classification d‚Äôaudio d√©tecte ces mots de r√©veil, il d√©clenche votre t√©l√©phone pour commencer √† √©couter le microphone et transcrire votre discours √† l'aide d'un mod√®le de reconnaissance vocale.
Le mod√®le de classification d‚Äôaudio est beaucoup plus petit et plus l√©ger que le mod√®le de reconnaissance vocale, souvent seulement quelques millions de param√®tres contre plusieurs centaines de millions pour la reconnaissance vocale. Ainsi, il peut fonctionner en continu sur votre appareil sans vider votre batterie ! Ce n'est que lorsque le mot de r√©veil est d√©tect√© que le mod√®le de reconnaissance vocale plus large est lanc√©, puis qu'il est √† nouveau arr√™t√©. Nous couvrirons les mod√®les de *transformers* pour la reconnaissance vocale dans la prochaine unit√©, donc √† la fin du cours, vous devriez avoir les outils dont vous avez besoin pour construire votre propre assistant √† commande vocale !
Comme pour tout jeu de donn√©es sur le *Hub*, nous pouvons avoir une id√©e de la t√™te des donn√©es sans avoir √† les t√©l√©charger ou les avoir en m√©moire. Apr√®s avoir acc√©d√© √† la [carte du jeu de donn√©es Speech Commands](https://huggingface.co/datasets/speech_commands) sur le *Hub*, nous pouvons utiliser la visionneuse de donn√©es pour faire d√©filer les 100 premiers √©chantillons du jeu de donn√©es, √©couter les fichiers audio et v√©rifier toute autre information de m√©tadonn√©es :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/speech_commands.png" alt="Diagram of datasets viewer.">
 </div>

L'aper√ßu du jeu de donn√©es est un moyen de d√©couvrir les jeux de donn√©es audio avant de s'engager √† les utiliser. Vous pouvez choisir n'importe quel jeu de donn√©es sur le *Hub*, faire d√©filer les √©chantillons et √©couter l'audio pour les diff√©rents sous-ensembles et √©chantillons, en √©valuant s'il s'agit du bon jeu de donn√©es pour vos besoins. Une fois que vous avez s√©lectionn√© un jeu de donn√©es, il est trivial de t√©l√©charger les donn√©es afin de pouvoir commencer √† les utiliser.
Faisons cela et chargeons un √©chantillon du jeu de donn√©es Speech Commands en utilisant le mode streaming :

```python
speech_commands = load_dataset(
    "speech_commands", "v0.02", split="validation", streaming=True
)
sample = next(iter(speech_commands))
``` 

Nous allons charger un *checkpoint* d‚Äôun [*transformer* d‚Äôaudio sous la forme de spectrogramme](https://huggingface.co/docs/transformers/model_doc/audio-spectrogramme-transformer) *finetun√©* sur le jeu de donn√©es Speech Commands :

```python
classifier = pipeline(
    "audio-classification", model="MIT/ast-finetuned-speech-commands-v2"
)
classifier(sample["audio"])
``` 

**Sortie :**

```
[{'score': 0.9999892711639404, 'label': 'backward'},
 {'score': 1.7504888774055871e-06, 'label': 'happy'},
 {'score': 6.703040185129794e-07, 'label': 'follow'},
 {'score': 5.805884484288981e-07, 'label': 'stop'},
 {'score': 5.614546694232558e-07, 'label': 'up'}]
``` 

On dirait que l'exemple contient le mot `backward` avec une forte probabilit√©. Nous pouvons √©couter l'√©chantillon et v√©rifier qu'il est correct:

```
from IPython.display import Audio

Audio(sample["audio"]["array"], rate=sample["audio"]["sampling_rate"])
```

Vous vous demandez peut-√™tre comment nous avons s√©lectionn√© les mod√®les pr√©-entra√Æn√©s montr√©s dans ces exemples de classification d‚Äôaudio.
C‚Äôest tr√®s simple ! La premi√®re chose que nous devons faire est de nous diriger sur le *Hub* et de cliquer sur l'onglet ¬´ *Models* ¬ª: https://huggingface.co/models
Cela va faire appara√Ætre tous les mod√®les sur le *Hub* :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/all_models.png">
 </div>

Vous remarquerez sur le c√¥t√© gauche que nous avons plusieurs onglets que nous pouvons s√©lectionner pour filtrer les mod√®les par t√¢che, biblioth√®que, jeu de donn√©es, etc. Faites d√©filer vers le bas et s√©lectionnez la t√¢che ¬´ Classification d‚Äôaudio ¬ª dans la liste des t√¢ches audio:

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/by_audio_classification.png">
 </div>

Nous voyons alors le sous-ensemble de mod√®les de classification d‚Äôaudio pr√©sent sur le *Hub*. Pour affiner davantage cette s√©lection, nous pouvons filtrer les mod√®les par jeu de donn√©es. Cliquez sur l'onglet ¬´ Jeux de donn√©es ¬ª, et dans la zone de recherche, tapez ¬´ speech_commands ¬ª. Lorsque vous commencez √† taper, vous verrez la s√©lection pour 'speech_commands' appara√Ætre sous l'onglet de recherche. Vous pouvez cliquer sur ce bouton pour filtrer tous les mod√®les de classification d‚Äôaudio *finetun√©* sur le jeu de donn√©es Speech Commands :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/by_speech_commands.png">
 </div> 

Bien, nous voyons que nous avons 6 mod√®les pr√©-entra√Æn√©s √† notre disposition pour ce jeu de donn√©es et cette t√¢che sp√©cifiques. Le premier list√© est celui que nous avons utilis√© dans l'exemple pr√©c√©dent. Ce processus de filtrage des mod√®les du *Hub* est exactement la fa√ßon dont nous avons proc√©d√© pour choisir ce mod√®le.

## Identification de la langue

L'identification de la langue est la t√¢che d'identifier la langue parl√©e dans un √©chantillon audio √† partir d'une liste de langues candidates. Cette t√¢che peut jouer un r√¥le important dans de nombreux pipelines de parole. Par exemple, √©tant donn√© un √©chantillon audio dans une langue inconnue, un mod√®le d‚Äôidentification de langue peut √™tre utilis√© pour cat√©goriser la ou les langues parl√©es dans l'√©chantillon audio, puis s√©lectionner un mod√®le de reconnaissance vocale appropri√© entra√Æn√© sur cette langue pour transcrire l'audio.

### FLEURS

FLEURS (*Few-shot Learning Evaluation of Universal Representations of Speech*) est un jeu de donn√©es permettant d'√©valuer les syst√®mes de reconnaissance vocale dans 102 langues, dont beaucoup sont class√©es comme √† faibles ressources. Jetez un coup d'≈ìil √† la carte de FLEURS sur le *Hub* et explorez les diff√©rentes langues pr√©sentes : [google/fleurs](https://huggingface.co/datasets/google/fleurs).
Pouvez-vous trouver votre langue maternelle ici ? Si ce n'est pas le cas, quelle est la langue la plus proche ?
Chargeons un √©chantillon √† partir de l‚Äô√©chantillon de validation de FLEURS en utilisant le mode streaming :

```python
fleurs = load_dataset("google/fleurs", "all", split="validation", streaming=True)
sample = next(iter(fleurs))
``` 

G√©nial ! Nous pouvons maintenant charger notre mod√®le de classification d‚Äôaudio. Pour cela, nous utiliserons une version de [Whisper](https://arxiv.org/pdf/2212.04356.pdf) *finetun√©* sur FLEURS, qui est actuellement le mod√®le de d√©tection de langue le plus performant sur le Hub:

```python
classifier = pipeline(
    "audio-classification", model="sanchit-gandhi/whisper-medium-fleurs-lang-id"
)
``` 

Nous pouvons ensuite passer l'audio √† travers notre classifieur et g√©n√©rer une pr√©diction :

```python
classifier(sample["audio"])
``` 

**Sortie :**

```
[{'score': 0.9999330043792725, 'label': 'Afrikaans'},
 {'score': 7.093023668858223e-06, 'label': 'Northern-Sotho'},
 {'score': 4.269149485480739e-06, 'label': 'Icelandic'},
 {'score': 3.2661141631251667e-06, 'label': 'Danish'},
 {'score': 3.2580724109720904e-06, 'label': 'Cantonese Chinese'}]
```

Nous pouvons voir que le mod√®le a pr√©dit que l'audio √©tait en Afrikaans avec une probabilit√© extr√™mement √©lev√©e. FLEURS contient des donn√©es audio provenant d'un large √©ventail de langues : nous pouvons voir que les √©tiquettes de classe possibles incluent le sotho du Nord, l'islandais, le danois et le cantonais, entre autres. Vous pouvez trouver la liste compl√®te des langues ici : [google/fleurs](https://huggingface.co/datasets/google/fleurs).
√Ä vous de jouer ! Quels autres *checkpoints* pouvez-vous trouver sur le *Hub*  afin de d√©tecter les langues pr√©sentes dans FLEURS ? Quels mod√®les de *transformers* utilisent-ils sous le capot ?

## Classification d‚Äôaudio en z√©ro-shot

Dans le paradigme traditionnel de la classification d‚Äôaudio, le mod√®le pr√©dit une √©tiquette de classe √† partir d'un ensemble de classes pr√©d√©finies possibles. Cela constitue un obstacle √† l'utilisation de mod√®les pr√©-entra√Æn√©s pour la classification d‚Äôaudio, car les √©tiquettes du mod√®le pr√©-entra√Æn√© doit correspondre √† celui de la t√¢che en aval. Pour l'exemple pr√©c√©dent de d√©tection de langues, le mod√®le doit pr√©dire l'une des 102 classes de langue sur lesquelles il a √©t√© entra√Æn√©. Si la t√¢che en aval n√©cessite en fait 110 langues, le mod√®le ne serait pas en mesure de pr√©dire 8 des 110 langues, et n√©cessiterait donc un nouvel entra√Ænement pour atteindre une couverture compl√®te. Cela limite l'efficacit√© de l'apprentissage par transfert pour les t√¢ches de classification d‚Äôaudio.
La classification d‚Äôaudio z√©ro-shot est une m√©thode permettant de prendre un mod√®le de classification d‚Äôaudio pr√©-entra√Æn√© entra√Æn√© sur un ensemble d'exemples √©tiquet√©s et de lui permettre de classer de nouveaux exemples de classes in√©dites. Voyons comment nous pouvons y parvenir.
Actuellement, ü§ó *Transformers* prend en charge un type de mod√®le pour la classification d‚Äôaudio en z√©ro-shot : le [mod√®le CLAP](https://huggingface.co/docs/transformers/model_doc/clap).
CLAP est un mod√®le bas√© sur un *transformer* qui prend √† la fois l'audio et le texte comme entr√©es, et calcule la *similitude* entre les deux.
Si nous passons une entr√©e de texte fortement corr√©l√©e √† une entr√©e audio, nous obtiendrons un score de similarit√© √©lev√©. Inversement, passer une entr√©e de texte qui n'a aucun rapport avec l'entr√©e audio renverra une faible similitude.
Nous pouvons utiliser cette pr√©diction de similarit√© pour la classification d‚Äôaudio en z√©ro-shot en passant une entr√©e audio au mod√®le et plusieurs √©tiquettes candidates. Le mod√®le renverra un score de similarit√© pour chacune des √©tiquettes candidates, et nous pouvons choisir celle qui a le score le plus √©lev√© comme pr√©diction.

Prenons un exemple o√π nous utilisons une entr√©e audio du jeu de donn√©es [Environmental Speech Challenge (ESC)](https://huggingface.co/datasets/ashraq/esc50) :

```python
dataset = load_dataset("ashraq/esc50", split="train", streaming=True)
audio_sample = next(iter(dataset))["audio"]["array"]
``` 

Nous d√©finissons ensuite nos √©tiquettes candidates, qui forment l'ensemble des √©tiquettes de classification possibles. Le mod√®le renverra une probabilit√© de classification pour chacune des √©tiquettes que nous d√©finissons. Cela signifie que nous devons conna√Ætre _a-priori_ l'ensemble des √©tiquettes possibles dans notre probl√®me de classification, de sorte que l'√©tiquette correcte soit contenue dans l'ensemble et se voie donc attribuer un score de probabilit√© valide. Notez que nous pouvons soit transmettre l'ensemble complet des √©tiquettes au mod√®le, soit un sous-ensemble s√©lectionn√© √† la main qui, selon nous, contient l'√©tiquette correcte. Passer l'ensemble complet des √©tiquettes sera plus exhaustif, mais se fait au d√©triment d'une pr√©cision de classification plus faible puisque l'espace de classification est plus grand (√† condition que l'√©tiquette correcte soit notre sous-ensemble d'√©tiquettes choisi):

```python
candidate_labels = ["Sound of a dog", "Sound of vacuum cleaner"]
``` 

Nous pouvons parcourir les deux mod√®les pour trouver l'√©tiquette candidate qui est la plus similaire √† l'entr√©e audio:

```python
classifier = pipeline(
    task="zero-shot-audio-classification", model="laion/clap-htsat-unfused"
)
classifier(audio_sample, candidate_labels=candidate_labels)
``` 

**Sortie :**

```
[{'score': 0.9997242093086243, 'label': 'Sound of a dog'}, {'score': 0.0002758323971647769, 'label': 'Sound of vacuum cleaner'}]
```
Le mod√®le semble assez confiant (probabilit√© de 99,97%) que nous ayons le son d'un chien .Nous allons donc prendre cela comme notre pr√©diction. Confirmons si nous avions raison en √©coutant l'√©chantillon audio (n'augmentez pas trop le volume, sinon vous risquez de sursauter !):

```python
Audio(audio, rate=16000)
``` 

Parfait ! Nous avons le son d'un chien qui aboie üêï, ce qui correspond √† la pr√©diction du mod√®le. Jouez avec diff√©rents √©chantillons audio et diff√©rentes √©tiquettes candidates. Pouvez-vous d√©finir un ensemble d'√©tiquettes qui donnent une bonne g√©n√©ralisation √† travers le jeu de donn√©es ESC ? Astuce : pensez √† l'endroit o√π vous pourriez trouver des informations sur les sons possibles dans ESC et construisez vos √©tiquettes en cons√©quence.
Vous vous demandez peut-√™tre pourquoi nous n'utilisons pas le pipeline de classification d‚Äôaudio zero-shot pour **toutes** les t√¢ches de classification d‚Äôaudio ? 
Il semble que nous puissions faire des pr√©dictions pour n'importe quel probl√®me de classification d‚Äôaudio en d√©finissant des √©tiquettes de classe appropri√©es _√† priori_, contournant ainsi la contrainte dont notre t√¢che de classification a besoin pour correspondre aux √©tiquettes sur lesquelles le mod√®le a √©t√© pr√©-entra√Æn√©.
Cela se r√©sume √† la nature du mod√®le CLAP utilis√© dans le pipeline z√©ro-shot. CLAP est pr√©-entra√Æn√© sur des donn√©es de classification d‚Äôaudio _g√©n√©riques_, similaires aux sons environnementaux dans le jeu de donn√©es ESC, plut√¥t que sur des donn√©es vocales sp√©cifiques, comme nous l'avions dans la t√¢che de d√©tection de langue. Si vous lui donnez un discours en anglais et un discours en espagnol, CLAP saurait que les deux exemples √©taient des donn√©es vocales.  Mais il ne serait pas capable de diff√©rencier les langues de la m√™me mani√®re qu'un mod√®le de d√©tection de langue d√©di√© √† cette t√¢che.

## Et ensuite ?

Nous avons couvert un certain nombre de t√¢ches de classification d‚Äôaudio, pr√©sent√© les jeux de donn√©es et les mod√®les les plus pertinents que vous pouvez t√©l√©charger √† partir du *Hub* et comment les utiliser en quelques lignes de code √† l'aide de la classe `pipeline()`. Ces t√¢ches comprenent la d√©tection de mots-cl√©s, l'identification de la langue et la classification d‚Äôaudio en z√©ro-shot.
Mais que se passe-t-il si nous voulons faire quelque chose de **nouveau** ? Nous avons beaucoup travaill√© sur les t√¢ches de traitement de la parole, mais ce n'est qu'un aspect de la classification d‚Äôaudio. Un autre domaine populaire du traitement d‚Äôaudio est la **musique**. Bien que la musique ait des caract√©ristiques intrins√®quement diff√©rentes √† la parole, bon nombre des m√™mes principes que nous avons d√©j√† appris peuvent √™tre appliqu√©s.
Dans la section suivante, nous allons passer en revue un guide √©tape par √©tape sur la fa√ßon dont vous pouvez *finetuner* un *transformer* avec ü§ó *Transformers* sur la t√¢che de classification de la musique. √Ä la fin, vous aurez un *checkpoint* *finetun√©* que vous pourrez brancher dans la classe `pipeline()`, vous permettant de classer les chansons exactement de la m√™me mani√®re que nous avons class√© la parole ici.
