# Mod√®les pr√©-entra√Æn√©s pour la reconnaissance automatique de la parole

Dans cette section, nous verrons comment utiliser le `pipeline()` pour tirer parti des mod√®les pr√©-entra√Æn√©s pour la reconnaissance automatique de la parole. Dans l‚Äô[unit√© 2](.. /chapter2/asr_pipeline), nous avons introduit le `pipeline()` comme un moyen facile d'ex√©cuter des t√¢ches de reconnaissance de la parole, avec tout le pr√©traitement et le post-traitement g√©r√©s sous le capot et la flexibilit√© d'exp√©rimenter rapidement avec n'importe quel *checkpoint* pr√©-entra√Æn√© disponible sur le *Hub*.
Dans cette unit√©, nous irons plus loin et explorerons les diff√©rents attributs des mod√®les de reconnaissance automatique de la parole et comment nous pouvons les utiliser pour aborder une gamme de t√¢ches diff√©rentes.

Comme d√©taill√© dans l'unit√© 3, le mod√®le de reconnaissance automatique de la parole se divise g√©n√©ralement dans l'une des deux cat√©gories suivantes :

1. Mod√®le avec Classification temporelle connexionniste (CTC) : mod√®les avec que l‚Äôencodeur du *transformer* avec une t√™te de classification lin√©aire sur le dessus
2. Mod√®le de s√©quence √† s√©quence (Seq2Seq) : mod√®les encodeur-d√©codeur, avec un m√©canisme d'attention crois√©e entre l'encodeur et le d√©codeur

Avant 2022, la variante avec CTC √©tait la plus populaire des deux architectures, avec des mod√®les tels que Wav2Vec2, HuBERT et XLSR r√©alisant des perc√©es dans le paradigme de pr√©-entra√Ænement / *finetuning* de la parole. 
De grandes entreprises, telles que Meta et Microsoft, ont pr√©-entra√Æn√© l'encodeur sur de grandes quantit√©s de donn√©es audio non √©tiquet√©es pendant plusieurs jours ou semaines. 
Les utilisateurs pouvent ensuite prendre un *checkpoint* pr√©-entra√Æn√© et le finetuner avec une t√™te CTC sur √† peine **10 minutes** de donn√©es audio √©tiquet√©es pour obtenir de solides performances sur une t√¢che en aval de reconnaissance automatique de la parole.
Cependant, les mod√®les CTC ont des lacunes. L'ajout d'une simple couche lin√©aire √† un encodeur donne un petit mod√®le global rapide, mais peut √™tre sujet √† des fautes d'orthographe phon√©tiques. Nous allons le d√©montrer ci-dessous pour le mod√®le Wav2Vec2.

## Sonder les mod√®les CTC

Chargeons un petit extrait du jeu de donn√©es [LibriSpeech ASR](hf-internal-testing/librispeech_asr_dummy) pour d√©montrer les capacit√©s de transcription de Wav2Vec2 :

```python
from datasets import load_dataset

dataset = load_dataset(
    "hf-internal-testing/librispeech_asr_dummy", "clean", split="validation"
)
dataset
```

**Sortie :**

```
Dataset({
    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],
    num_rows: 73
})
```

Nous pouvons choisir l'un des 73 √©chantillons audio et en inspecter l‚Äôaudio ainsi que la transcription :

```python
from IPython.display import Audio

sample = dataset[2]

print(sample["text"])
Audio(sample["audio"]["array"], rate=sample["audio"]["sampling_rate"])
```

**Sortie :**

```
HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND
```

No√´l et r√¥ti de b≈ìuf, √ßa sonne bien! üéÑ Apr√®s avoir choisi un √©chantillon de donn√©es, nous chargeons maintenant un *checkpoint* *finetun√©* dans le `pipeline()`. 
Pour cela, nous utiliserons le *checkpoint* officiel [base Wav2Vec2] (facebook/wav2vec2-base-100h) *finetun√©* sur 100 heures de donn√©es LibriSpeech :

```python
from transformers import pipeline

pipe = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-100h")
```

Ensuite, nous allons prendre un exemple du jeu de donn√©es et transmettre ses donn√©es brutes au pipeline. 
Puisque le pipeline *consomme* n'importe quel dictionnaire que nous lui passons (ce qui signifie qu'il ne peut pas √™tre r√©utilis√©), nous transmettrons une copie des donn√©es. De cette fa√ßon, nous pouvons r√©utiliser en toute s√©curit√© le m√™me √©chantillon audio dans les exemples suivants:

```python
pipe(sample["audio"].copy())
{
    "text": "HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAUS AND ROSE BEEF LOOMING BEFORE US SIMALYIS DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND"
}
```

Nous pouvons voir que le mod√®le Wav2Vec2 fait un assez bon travail pour transcrire cet √©chantillon. A premi√®re vue, il semble g√©n√©ralement correct.
Mettons la cible et la pr√©diction c√¥te √† c√¥te et mettons en √©vidence les diff√©rences:

```
Target:      HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND
Prediction:  HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH **CHRISTMAUS** AND **ROSE** BEEF LOOMING BEFORE US **SIMALYIS** DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND
```

En comparant le texte cible √† la transcription pr√©vue, nous pouvons voir que tous les mots _sonnent_ correctement mais que certains ne sont pas orthographi√©s avec pr√©cision. Par exemple :

* _CHRISTMAUS_ vs. _CHRISTMAS_
* _ROSE_ vs. _ROAST_
* _SIMALYIS_ vs. _SIMILES_
Cela met en √©vidence les lacunes d'un mod√®le CTC. Un mod√®le CTC est essentiellement uniquement un mod√®le ¬´ acoustique¬ª : il se compose d'un encodeur qui forme des repr√©sentations d'√©tats cach√©s √† partir des entr√©es audio, et d'une couche lin√©aire qui associe les √©tats cach√©s aux caract√®res :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/wav2vec2-ctc.png" alt="Transformer encoder with a CTC head on top">
</div>

Cela signifie que le syst√®me base presque enti√®rement sa pr√©diction sur l'entr√©e acoustique qui lui a √©t√© donn√©e (les sons phon√©tiques de l'audio), et a donc tendance √† transcrire l'audio de mani√®re phon√©tique.
Il donne moins d'importance au contexte de mod√©lisation linguistique des lettres pr√©c√©dentes et successives, et est donc sujet aux fautes d'orthographe phon√©tique. 
Un mod√®le plus intelligent identifierait que _CHRISTMAUS_ n'est pas un mot valide dans le vocabulaire anglais, et le corrigerait en _CHRISTMAS_ lors de ses pr√©dictions. Il nous manque √©galement deux grandes fonctionnalit√©s dans notre pr√©diction, la casse et la ponctuation, ce qui limite l'utilit√© des transcriptions du mod√®le aux applications r√©elles.

## Passage √† Seq2Seq

Comme indiqu√© dans l'unit√© 3, les mod√®les Seq2Seq sont form√©s d'un codeur et d'un d√©codeur reli√©s par un m√©canisme d'attention crois√©e. L'encodeur joue le m√™me r√¥le qu'auparavant, calculant des repr√©sentations d‚Äô√©tats cach√©s des entr√©es audio, tandis que le d√©codeur joue le r√¥le d'un **mod√®le de langage**. Le d√©codeur traite toute la s√©quence de repr√©sentations d‚Äô√©tats cach√©s de l'encodeur et g√©n√®re les transcriptions de texte correspondantes. Avec le contexte global de l'entr√©e audio, le d√©codeur est capable d'utiliser le contexte du mod√®le de langage lorsqu'il fait ses pr√©dictions, corrigeant les fautes d'orthographe √† la vol√©e et contournant ainsi le probl√®me des pr√©dictions phon√©tiques.
Les mod√®les Seq2Seq pr√©sentent deux inconv√©nients :
1. Ils sont intrins√®quement plus lents lors du d√©codage puisqu‚Äôil se produit une √©tape √† la fois, plut√¥t que tout √† la fois 2. Ils n√©cessitent beaucoup plus de donn√©es d'entra√Ænement pour converger

Le besoin de grandes quantit√©s de donn√©es d'apprentissage a √©t√© un goulot d'√©tranglement dans l'avancement des architectures Seq2Seq pour la parole. Les donn√©es audios √©tiquet√©es sont difficiles √† obtenir, les plus grands jeux de donn√©es annot√©es √† l'√©poque ne totalisant que 10 000 heures. Tout cela a chang√© en 2022 avec la sortie de **Whisper**. Whisper est un mod√®le pr√©-entra√Æn√© pour la reconnaissance automatique de la parole publi√© en [septembre 2022](https://openai.com/blog/whisper/) par Alec Radford et al. d'OpenAI. Contrairement √† ses pr√©d√©cesseurs CTC, qui √©taient enti√®rement pr√©-entra√Æn√©s sur des donn√©es audio **non √©tiquet√©es**, Whisper est pr√©-entra√Æn√© sur une grande quantit√© de donn√©es de transcription audio **√©tiquet√©es**, 680 000 heures pour √™tre pr√©cis.
Il s'agit d'un ordre de grandeur de donn√©es plus grand que les donn√©es audio non √©tiquet√©es utilis√©es pour entra√Æner Wav2Vec 2.0 (60 000 heures). De plus, 117 000 heures de ces donn√©es de pr√©-entra√Ænement sont des donn√©es multilingues. Il en r√©sulte des *checkpoints* qui peuvent √™tre appliqu√©s √† plus de 96 langues, dont beaucoup sont consid√©r√©es comme _√† faible ressource_.
Lorsqu'ils sont pass√©s √† l'√©chelle, les mod√®les Whisper d√©montrent une forte capacit√© √† g√©n√©raliser sur de nombreux jeux de donn√©es et domaines. Obtenant des r√©sultats comp√©titifs de pointe, avec un taux d'erreur de mots (WER) de pr√®s de 3% sur le sous-ensemble de test LibriSpeech et de 4,7% WER  sur le sous-ensemble de test TED-LIUM (_cf._ Tableau 8 du [papier de Whisper](https://cdn.openai.com/papers/whisper.pdf)).
La capacit√© de Whisper √† g√©rer de longs √©chantillons audio, sa robustesse au bruit et sa capacit√© √† pr√©dire les transcriptions en casse et ponctu√©es rev√™tent une importance particuli√®re. Cela en fait un candidat viable pour les syst√®mes de reconnaissance automatique de la parole du monde r√©el.
Le reste de cette section vous montrera comment utiliser les mod√®les Whisper pr√©-entra√Æn√©s pour la reconnaissance automatique de la parole √† l'aide de ü§ó *Transformers*. Dans de nombreuses situations, les *checkpoints* pr√©-entra√Æn√©s sont extr√™mement performants et donnent d'excellents r√©sultats, nous vous encourageons donc √† essayer de les utiliser comme premi√®re √©tape pour r√©soudre tout probl√®me de reconnaissance automatique de la parole.
Gr√¢ce √† un *finetuning*, les *checkpoints* peuvent √™tre adapt√©s √† des jeux de donn√©es et √† des langues sp√©cifiques afin d'am√©liorer encore ces r√©sultats. Nous montrerons comment faire cela dans la section sur [le *finetuning*](fine-tuning).
Whisper est disponible en cinq tailles diff√©rentes de mod√®les. Les quatre plus petits sont entra√Æn√©s soit sur des donn√©es en anglais soit sur des donn√©es multilingues. Le *checkpoint* le plus grand est uniquement multilingue. Les neuf *checkpoints* pr√©-entra√Æn√©s sont disponibles sur le [*Hub*](https://huggingface.co/models?search=openai/whisper). Ils sont r√©sum√©s dans le tableau suivant. ¬´ VRAM ¬ª indique la m√©moire GPU requise pour ex√©cuter le mod√®le avec une taille de batch minimale de 1. ¬´ *Rel Speed* ¬ª est la vitesse relative d'un *checkpoint* par rapport au plus grand mod√®le.
Sur la base de ces informations, vous pouvez s√©lectionner le *checkpoint* le plus adapt√© √† votre mat√©riel.

| Taille   | Param√®tres | VRAM / Go | *Rel Speed* | Anglais                                         | Multilingue                                        |
|--------|------------|-----------|-----------|------------------------------------------------------|-----------------------------------------------------|
| tiny   | 39 M       | 1.4       | 32        | [‚úì](https://huggingface.co/openai/whisper-tiny.en)   | [‚úì](https://huggingface.co/openai/whisper-tiny)     |
| base   | 74 M       | 1.5       | 16        | [‚úì](https://huggingface.co/openai/whisper-base.en)   | [‚úì](https://huggingface.co/openai/whisper-base)     |
| small  | 244 M      | 2.3       | 6         | [‚úì](https://huggingface.co/openai/whisper-small.en)  | [‚úì](https://huggingface.co/openai/whisper-small)    |
| medium | 769 M      | 4.2       | 2         | [‚úì](https://huggingface.co/openai/whisper-medium.en) | [‚úì](https://huggingface.co/openai/whisper-medium)   |
| large  | 1550 M     | 7.5       | 1         | x                                                    | [‚úì](https://huggingface.co/openai/whisper-large-v2) |

Chargeons le [Whisper Base](https://huggingface.co/openai/whisper-base), qui est de taille comparable au Wav2Vec2 que nous avons utilis√© pr√©c√©demment. Anticipant notre passage √† la reconnaissance automatique de la parole multilingue, nous allons charger la variante multilingue du *checkpoint* de base. Nous chargons √©galement le mod√®le sur le GPU s'il est disponible, ou sur le CPU dans le cas contraire. Le `pipeline()` se chargera ensuite de d√©placer toutes les entr√©es / sorties du CPU vers le GPU selon les besoins:

```python
import torch
from transformers import pipeline

device = "cuda:0" if torch.cuda.is_available() else "cpu"
pipe = pipeline(
    "automatic-speech-recognition", model="openai/whisper-base", device=device
)
```

Bien, maintenant, transcrivons l'audio comme avant. Le seul changement que nous apportons est de passer un argument suppl√©mentaire, `max_new_tokens`, qui indique au mod√®le le nombre maximum de *tokens* √† g√©n√©rer lors de sa pr√©diction :

```python
pipe(sample["audio"], max_new_tokens=256)
```

**Sortie :**

```
{'text': ' He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.'}
```

La premi√®re chose que vous remarquerez est la pr√©sence de la casse et de la ponctuation. Cela rend imm√©diatement la transcription plus facile √† lire par rapport √† la transcription non cas√©e et non ponctu√©e de Wav2Vec2. Mettons la transcription c√¥te √† c√¥te avec la cible :

```
Target:     HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND
Prediction: He tells us that at this festive season of the year, with **Christmas** and **roast** beef looming before us, **similarly** is drawn from eating and its results occur most readily to the mind.
```

Whisper a fait un excellent travail pour corriger les erreurs phon√©tiques que nous avons vues avec Wav2Vec2 : _Christmas_ et _roast_ sont orthographi√©s correctement. Nous voyons que le mod√®le a encore du mal avec _SIMILES_, √©tant incorrectement transcrit comme _similarly_, mais cette fois la pr√©diction est un mot valide du vocabulaire anglais. L'utilisation d'un Whisper plus grand peut aider √† r√©duire davantage les erreurs de transcription, au d√©triment d'un calcul plus important et d'un temps de transcription plus long.
On nous a promis un mod√®le capable de g√©rer 96 langues, alors passons √† de la reconnaissance automatique multilingue üåé !
Le jeu de donn√©es [Multilingual LibriSpeech](https://huggingface.co/datasets/facebook/multilingual_librispeech) (MLS) est l'√©quivalent multilingue du jeu de donn√©es LibriSpeech, avec des donn√©es audio √©tiquet√©es en six langues. Nous allons charger un √©chantillon de l‚Äô√©chantillon espagnol de MLS, en utilisant le mode _streaming_ afin de ne pas avoir √† t√©l√©charger l'ensemble de donn√©es :

```python
dataset = load_dataset(
    "facebook/multilingual_librispeech", "spanish", split="validation", streaming=True
)
sample = next(iter(dataset))
```

Encore une fois, nous allons inspecter la transcription du texte et √©couter le segment audio:

```python
print(sample["text"])
Audio(sample["audio"]["array"], rate=sample["audio"]["sampling_rate"])
```

**Sortie :**

```
entonces te delelitar√°s en jehov√° y yo te har√© subir sobre las alturas de la tierra y te dar√© √° comer la heredad de jacob tu padre porque la boca de jehov√° lo ha hablado
```

C'est le texte cible que nous visons avec notre transcription Whisper. Bien que nous sachions maintenant que nous pouvons probablement faire mieux, puisque notre mod√®le va aussi pr√©dire la ponctuation et la casse, qui ne sont pas pr√©sents dans la r√©f√©rence. Transmettons l'√©chantillon audio au pipeline pour obtenir notre pr√©diction de texte. Une chose √† noter est que le pipeline _consomme_ le dictionnaire des entr√©es audio que nous entrons, ce qui signifie que le dictionnaire ne peut pas √™tre r√©utilis√©. Pour contourner ce probl√®me, nous allons passer une _copie_ de l'√©chantillon audio, afin de pouvoir r√©utiliser le m√™me √©chantillon audio dans les exemples de code suivants :

```python
pipe(sample["audio"].copy(), max_new_tokens=256, generate_kwargs={"task": "transcribe"})
```

**Sortie :**

```
{'text': ' Entonces te deleitar√°s en Jehov√° y yo te har√© subir sobre las alturas de la tierra y te dar√© a comer la heredad de Jacob tu padre porque la boca de Jehov√° lo ha hablado.'}
```

Super, cela ressemble √©norm√©ment √† notre texte de r√©f√©rence (sans doute mieux car il a une ponctuation et une casse !). Vous remarquerez que nous avons transf√©r√© `"task"` en tant que _generate kwarg_. D√©finir la `"task"` √† `"transcribe"` oblige Whisper √† effectuer la t√¢che de reconnaissance de la parole, o√π l'audio est transcrit dans la m√™me langue que le discours a √©t√© prononc√©. Whisper est √©galement capable d'effectuer la t√¢che √©troitement li√©e de traduction de la parole, o√π l'audio en espagnol peut √™tre traduit en texte en anglais. Pour y parvenir, nous d√©finissons la `"task"` sur `"translate"` :

```python
pipe(sample["audio"], max_new_tokens=256, generate_kwargs={"task": "translate"})
```

**Sortie :**

```
{'text': ' So you will choose in Jehovah and I will raise you on the heights of the earth and I will give you the honor of Jacob to your father because the voice of Jehovah has spoken to you.'}
```

Maintenant que nous savons que nous pouvons basculer entre la reconnaissance automatique de la parole et la traduction de la parole, nous pouvons choisir notre t√¢che en fonction de nos besoins. Soit nous reconnaissons l'audio dans la langue X vers le texte dans la m√™me langue X (par exemple, l'audio espagnol vers le texte espagnol), soit nous traduisons de l'audio dans n'importe quelle langue X vers du texte en anglais (par exemple, l'audio espagnol vers le texte anglais).
Pour en savoir plus sur la fa√ßon dont l'argument `"task"` est utilis√© pour contr√¥ler les propri√©t√©s du texte g√©n√©r√©, reportez-vous √† la [carte de mod√®le](https://huggingface.co/openai/whisper-base#usage) pour le mod√®le Whisper base.

## Longue transcription et horodatage

Jusqu'√† pr√©sent, nous nous sommes concentr√©s sur la transcription de courts √©chantillons audio de moins de 30 secondes. Nous avons mentionn√© que l'un des attraits de Whisper √©tait sa capacit√© √† travailler sur de longs √©chantillons audio. Nous allons nous attaquer √† cette t√¢che ici !
Cr√©ons un long fichier audio en concat√©nant des √©chantillons successifs √† partir du jeu de donn√©es MLS. √âtant donn√© que MLS est organis√© en divisant de longs enregistrements de livres audio en segments plus courts, la concat√©nation d'√©chantillons est un moyen de reconstruire des passages de livres audio plus longs. Par cons√©quent, l'audio r√©sultant doit √™tre coh√©rent sur l'ensemble de l'√©chantillon. Nous allons d√©finir notre dur√©e audio cible sur 5 minutes et arr√™ter de concat√©ner des √©chantillons une fois que nous aurons atteint cette valeur :

```python
import numpy as np

target_length_in_m = 5

# convertir les minutes en secondes (* 60) en nombre d'√©chantillons (* taux d'√©chantillonnage)
sampling_rate = pipe.feature_extractor.sampling_rate
target_length_in_samples = target_length_in_m * 60 * sampling_rate

# it√©rer sur notre jeu de donn√©es en streaming, en concat√©nant les √©chantillons jusqu'√† ce que nous atteignions notre cible
long_audio = []
for sample in dataset:
    long_audio.extend(sample["audio"]["array"])
    if len(long_audio) > target_length_in_samples:
        break

long_audio = np.asarray(long_audio)

# R√©sultat
seconds = len(long_audio) / 16000
minutes, seconds = divmod(seconds, 60)
print(f"Length of audio sample is {minutes} minutes {seconds:.2f} seconds")
```

**Sortie :**

```
Length of audio sample is 5.0 minutes 17.22 seconds
```

5 minutes et 17 secondes d'audio √† transcrire. Le transfert de ce long √©chantillon audio directement vers le mod√®le pose deux probl√®mes :
1. Whisper est intrins√®quement con√ßu pour fonctionner avec des √©chantillons de 30 secondes. Tout ce qui est inf√©rieur √† 30s est rembourr√© √† 30s avec du silence, et tout ce qui d√©passe 30s est tronqu√© √† 30s en coupant l'audio exc√©dentaire. Donc si nous passons notre audio directement, nous n'obtiendrons la transcription que pour les 30 premi√®res secondes
2. La m√©moire dans un *transformer* √©volue quadratiquement avec la longueur de s√©quence a : doubler la longueur d'entr√©e quadruple le besoin en m√©moire, de sorte que le passage de fichiers audio tr√®s longs entra√Ænera in√©vitablement une erreur de m√©moire insuffisante (OOM : out-of-memory)
La transcription de longs audio fonctionne dans ü§ó *Transformers* en segmentant l'audio d'entr√©e en segments plus petits et plus faciles √† g√©rer.
Chaque segment a un petit chevauchement avec le pr√©c√©dent. Cela nous permet de recoudre avec pr√©cision les segments aux bornes, car nous pouvons trouver le chevauchement entre les segments et fusionner les transcriptions en cons√©quence :

<div class="flex justify-center">
     <img src="https://huggingface.co/blog/assets/49_asr_chunking/Striding.png" alt="ü§ó Transformers chunking algorithm. Source: https://huggingface.co/blog/asr-chunking.">
</div>

L'avantage de segmenter les √©chantillons est que nous n'avons pas besoin du r√©sultat du bloc \\( i \\) pour transcrire le morceau suivant \\( i + 1 \\). La couture est effectu√©e apr√®s que nous ayons transcrit tous les morceaux aux bornes des morceaux. Donc peu importe l'ordre dans lequel nous transcrivons les morceaux. L'algorithme est enti√®rement **sans √©tats**, donc nous pouvons m√™me faire du d√©coupage sur \\( i + 1 \\) en m√™me temps que du d√©coupage sur \\( i \\) ! Cela nous permet de former des _batch_ de morceaux et de les ex√©cuter dans le mod√®le en parall√®le, offrant une grande acc√©l√©ration de calcul par rapport √† leur transcription s√©quentielle. Pour en savoir plus sur la segmentation d‚Äôaudio dans ü§ó *Transformers*, vous pouvez vous r√©f√©rer √† cet [article de blog] (https://huggingface.co/blog/asr-chunking) (en anglais).
Pour activer les longues transcriptions, nous devons ajouter un argument suppl√©mentaire lorsque nous appelons le pipeline. Cet argument, `chunk_length_s`, contr√¥le la longueur des segments divis√©s en secondes. Pour Whisper, des morceaux de 30 secondes sont optimaux, car cela correspond √† la longueur d'entr√©e attendue par Whisper.
Pour activer le traitement par batch, nous devons passer l'argument `batch_size` au pipeline. En mettant tout cela ensemble, nous pouvons transcrire le long √©chantillon audio comme suit:

```python
pipe(
    long_audio,
    max_new_tokens=256,
    generate_kwargs={"task": "transcribe"},
    chunk_length_s=30,
    batch_size=8,
)
```

**Sortie :**

```
{'text': ' Entonces te deleitar√°s en Jehov√°, y yo te har√© subir sobre las alturas de la tierra, y te dar√© a comer la
heredad de Jacob tu padre, porque la boca de Jehov√° lo ha hablado. nosotros curados. Todos nosotros nos descarriamos
como bejas, cada cual se apart√≥ por su camino, mas Jehov√° carg√≥ en √©l el pecado de todos nosotros...
``` 

Nous n‚Äôaffichons pas toute la sortie ici car elle est assez longue (312 mots au total). Sur un GPU V100 de 16 Go, vous pouvez vous attendre √† ce que la ligne ci-dessus prenne environ 3,45 secondes √† s‚Äô√©x√©cuter, ce qui est assez bon pour un √©chantillon audio de 317 secondes. Sur un CPU, attendez-vous √† plus de 30 secondes.
Whisper est √©galement capable de pr√©dire les _horodatages_ au niveau du segment pour les donn√©es audio. Ces horodatages indiquent l'heure de d√©but et de fin d'un court passage audio et sont particuli√®rement utiles pour aligner une transcription avec l'audio d'entr√©e. Supposons que nous voulions fournir des sous-titres pour une vid√©o. Nous avons besoin de ces horodatages pour savoir quelle partie de la transcription correspond √† un certain segment de vid√©o, afin d'afficher la transcription correcte pour cette heure.
L'activation de la pr√©diction d'horodatage est simple, il suffit de d√©finir l'argument `return_timestamps=True`. Les horodatages sont compatibles avec les m√©thodes de segmentation et de traitement par batchs que nous avons utilis√©es pr√©c√©demment. Nous pouvons donc simplement ajouter l'argument timestamp √† notre code pr√©c√©dent :

```python
pipe(
    long_audio,
    max_new_tokens=256,
    generate_kwargs={"task": "transcribe"},
    chunk_length_s=30,
    batch_size=8,
    return_timestamps=True,
)["chunks"]
``` 

**Sortie :**

```
[{'timestamp': (0.0, 26.4),
  'text': ' Entonces te deleitar√°s en Jehov√°, y yo te har√© subir sobre las alturas de la tierra, y te dar√© a comer la heredad de Jacob tu padre, porque la boca de Jehov√° lo ha hablado. nosotros curados. Todos nosotros nos descarriamos como bejas, cada cual se apart√≥ por su camino,'},
 {'timestamp': (26.4, 32.48),
  'text': ' mas Jehov√° carg√≥ en √©l el pecado de todos nosotros. No es que partas tu pan con el'},
 {'timestamp': (32.48, 38.4),
  'text': ' hambriento y a los hombres herrantes metas en casa, que cuando vieres al desnudo lo cubras y no'},
 ...
``` 

Et le tour est jou√© ! Nous avons notre texte pr√©dit ainsi que les horodatages correspondants.

## R√©sum√©

Whisper est un mod√®le pr√©-entra√Æn√© solide pour la reconnaissance automatique de la parole et la traduction. Par rapport √† Wav2Vec2, il a une plus grande pr√©cision de transcription, avec des sorties qui contiennent la ponctuation et la casse. Il peut √™tre utilis√© pour transcrire la parole en anglais ainsi que dans 96 autres langues, √† la fois sur des segments audio courts et des segments plus longs par le biais de  la segmentation. Ces attributs en font un mod√®le viable pour de nombreuses t√¢ches de reconnaissance automatique de la parole et de traduction sans avoir besoin d'√™tre *finetun√©*. La m√©thode `pipeline()` fournit un moyen facile d'inf√©rer en une ligne de code avec un contr√¥le sur les pr√©dictions g√©n√©r√©es.
Bien que le mod√®le Whisper fonctionne extr√™mement bien sur de nombreuses langues √† ressources √©lev√©es, il a une pr√©cision de transcription et de traduction plus faible sur les langues √† faibles ressources, c'est-√†-dire celles pour lesquelles les donn√©es d'apprentissage sont moins facilement disponibles. Les performances varient √©galement selon les accents et dialectes de certaines langues, y compris une pr√©cision moindre pour les locuteurs de diff√©rents sexes, races, √¢ges ou autres crit√®res d√©mographiques (_cf._ le papier de [Whisper](https://arxiv.org/pdf/2212.04356.pdf)).
Pour am√©liorer les performances sur les langues, les accents ou les dialectes √† faibles ressources, nous pouvons prendre le mod√®le Whisper pr√©-entra√Æn√© et l'entra√Æner sur un petit corpus de donn√©es s√©lectionn√©es de mani√®re appropri√©e, dans un processus appel√© _*finetuning*_. Nous montrerons qu'avec seulement dix heures de donn√©es suppl√©mentaires, nous pouvons am√©liorer les performances du mod√®le Whisper de plus de 100% sur une langue √† faibles ressources. Dans la section suivante, nous aborderons le processus de s√©lection d'un jeu de donn√©es √† *finetuner*.
