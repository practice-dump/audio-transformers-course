# √âvaluation et m√©triques pour la reconnaissance automatique de la parole

Si vous connaissez la [distance de Levenshtein](https://fr.wikipedia.org/wiki/Distance_de_Levenshtein) en NLP, les m√©triques d'√©valuation des syst√®mes de reconnaissance vocale vous seront famili√®res ! Ne vous inqui√©tez pas si ce n'est pas le cas, nous allons passer en revue les explications afin de nous assurer que vous ma√Ætrisez les diff√©rentes m√©triques et que vous comprenez ce qu'elles signifient.

Lors de l'√©valuation des syst√®mes de reconnaissance vocale, nous comparons les pr√©dictions du syst√®me aux transcriptions du texte cible, en annotant toutes les erreurs pr√©sentes. Nous classons ces erreurs dans l'une des trois cat√©gories suivantes :
1. Substitutions (Sub) : lorsque nous transcrivons le **mauvais mot** dans notre pr√©diction (par exemple "sit" au lieu de "sat").
2. Insertions (Ins) : lorsque nous ajoutons un **mot suppl√©mentaire** dans notre pr√©diction.
3. Suppressions (Sup)  : lorsque nous **supprimons un mot** dans notre pr√©diction.

Ces cat√©gories d'erreurs sont les m√™mes pour toutes les mesures de reconnaissance vocale. Ce qui diff√®re, c'est le niveau auquel nous calculons ces erreurs : nous pouvons les calculer soit au niveau du _mot_, soit au niveau du _caract√®re_.

Nous utiliserons un exemple courant pour chacune des d√©finitions des mesures. Nous avons ici une s√©quence de texte de _v√©rit√© de base_ ou de _r√©f√©rence_ :

```python
reference = "the cat sat on the mat"
```

Et une s√©quence pr√©dite par le syst√®me de reconnaissance vocale que nous essayons d'√©valuer :

```python
prediction = "the cat sit on the"
```

Nous pouvons constater que la pr√©diction est assez proche, mais que certains mots ne sont pas tout √† fait corrects. Nous allons √©valuer cette pr√©diction par rapport √† la r√©f√©rence pour les trois mesures de reconnaissance vocale les plus populaires et voir quels types de chiffres nous obtenons pour chacune d'entre elles.

## *Word Error Rate* (Taux d'erreur au niveau du mot)

Le *word error rate (WER)* est la mesure de facto pour la reconnaissance vocale. Elle calcule les substitutions, les insertions et les suppressions au niveau du *mot*. Cela signifie que les erreurs sont annot√©es mot par mot. Prenons notre exemple :


| R√©ference :  | the | cat | sat     | on  | the | mat |
|-------------|-----|-----|---------|-----|-----|-----|
| Prediction : | the | cat | **sit** | on  | the |     |  |
| √âtiquette :  | ‚úÖ   | ‚úÖ   | Sub       | ‚úÖ   | ‚úÖ   | Sup  |

Ici, nous avons
* 1 substitution ("sit" au lieu de "sat")
* 0 insertion
* 1 suppression ("mat" est manquant)

Cela donne 2 erreurs au total. Pour obtenir notre taux d'erreur, nous divisons le nombre d'erreurs par le nombre total de mots de notre r√©f√©rence (N), qui est de 6 pour cet exemple :

$$
\begin{aligned}
WER &= \frac{Sub + Ins + Sup}{N} \\
&= \frac{1 + 0 + 1}{6} \\
&= 0.333
\end{aligned}
$$

Ok, nous avons donc un WER de 0,333, soit 33,3 %. Remarquez que le mot "sit" ne comporte qu'un seul caract√®re erron√©, mais que le mot entier est marqu√© comme incorrect. Il s'agit l√† d'une caract√©ristique essentielle du WER : les fautes d'orthographe sont lourdement p√©nalis√©es, m√™me si elles sont mineures.

Le WER est d√©fini de telle sorte que *plus il est bas, mieux c'est* : un WER bas signifie qu'il y a moins d'erreurs dans notre pr√©diction, de sorte qu'un syst√®me de reconnaissance vocale parfait aurait un WER de z√©ro (pas d'erreurs).

Voyons comment nous pouvons calculer le WER en utilisant ü§ó *Evaluate*. Nous aurons besoin de deux packages pour calculer notre m√©trique WER : ü§ó *Evaluate* pour l'interface API, et JIWER pour effectuer le gros du travail de calcul :

```
pip install --upgrade evaluate jiwer
```

Nous pouvons maintenant charger la m√©trique WER et calculer le r√©sultat pour notre exemple :

```python
from evaluate import load

wer_metric = load("wer")

wer = wer_metric.compute(references=[reference], predictions=[prediction])

print(wer)
```

**Sortie :**

```
0.3333333333333333
```

0,33, soit 33,3 %, comme pr√©vu ! Nous savons maintenant ce qui se passe sous le capot avec ce calcul du WER.

Maintenant, voici quelque chose qui est assez d√©routant... D'apr√®s vous, quelle est la limite sup√©rieure du WER ? On pourrait s'attendre √† ce qu'elle soit de 1 ou de 100 %, n'est-ce pas ? Le WER √©tant le rapport entre le nombre d'erreurs et le nombre de mots (N), il n'y a pas de limite sup√©rieure au WER !
Prenons un exemple dans lequel nous pr√©disons 10 mots et la cible n'en a que 2. Si toutes nos pr√©dictions √©taient fausses (10 erreurs), nous aurions un REE de 10 / 2 = 5, soit 500 % ! Il convient de garder cela √† l'esprit si vous entra√Ænez un syst√®me ASR et que vous obtenez un WER sup√©rieur √† 100 %. Toutefois, si vous obtenez ce r√©sultat, il est probable que quelque chose n'a pas fonctionn√©... üòÖ

## *Word Accuracy* (Pr√©cision au niveau du mot)

Nous pouvons inverser le WER pour obtenir une mesure o√π *plus c'est haut, mieux c'est*. Plut√¥t que de mesurer le taux d'erreurs au niveau du mot, nous pouvons mesurer la *pr√©cision au niveau du mot* (WAcc) de notre syst√®me :

$$
\begin{equation}
WAcc = 1 - WER \nonumber
\end{equation}
$$

Le WAcc est mesur√© au niveau du mot, il s'agit simplement du WER reformul√© en tant que mesure de pr√©cision plut√¥t qu'en tant que mesure d'erreur. Le WAcc est tr√®s rarement cit√© dans la litt√©rature : nous pensons aux pr√©dictions de notre syst√®me en termes d'erreurs de mots, et nous pr√©f√©rons donc les mesures de taux d'erreur qui sont plus associ√©es √† ces annotations de type d'erreur.

## *Character Error Rate* (Taux d'erreur au niveau des caract√®res)

Il semble un peu injuste que nous ayons marqu√© l'ensemble du mot "sit" comme erron√© alors qu'en fait une seule lettre √©tait incorrecte.
Le *Character Error Rate (CER)* √©value les syst√®mes au *niveau des caract√®res*. Cela signifie que nous divisons nos mots en caract√®res individuels et que nous annotons les erreurs caract√®re par caract√®re :

| R√©ference :  | t   | h   | e   |     | c   | a   | t   |     | s   | a     | t   |     | o   | n   |     | t   | h   | e   |     | m   | a   | t   |
|-------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| Prediction : | t   | h   | e   |     | c   | a   | t   |     | s   | **i** | t   |     | o   | n   |     | t   | h   | e   |     |     |     |     |
| √âtiquette :      | ‚úÖ   | ‚úÖ   | ‚úÖ   |     | ‚úÖ   | ‚úÖ   | ‚úÖ   |     | ‚úÖ   | Sub     | ‚úÖ   |     | ‚úÖ   | ‚úÖ   |     | ‚úÖ   | ‚úÖ   | ‚úÖ   |     | D   | D   | D   |

Nous voyons maintenant que pour le mot "sit", le "s" et le "t" sont marqu√©s comme corrects. Seul le "i" est √©tiquet√© comme une erreur de substitution. Nous r√©compensons donc notre syst√®me pour la pr√©diction partiellement correcte ü§ù

Dans notre exemple, nous avons 1 substitution de caract√®re, 0 insertion et 3 suppressions. Au total, nous avons 14 caract√®res. Notre CER est donc :

$$
\begin{aligned}
CER &= \frac{S + I + D}{N} \\
&= \frac{1 + 0 + 3}{14} \\
&= 0.286
\end{aligned}
$$

Nous obtenons un CER de 0,286, soit 28,6 %. Remarquez que ce chiffre est inf√©rieur √† notre WER. Nous avons beaucoup moins p√©nalis√© l'erreur d'orthographe.

## Quelle mesure dois-je utiliser ?

En g√©n√©ral, le WER est beaucoup plus utilis√© que le CER pour √©valuer les syst√®mes vocaux. En effet, le WER exige des syst√®mes une meilleure compr√©hension du contexte des pr√©dictions. Dans notre exemple, "sit" n'est pas au bon temps. Un syst√®me qui comprend la relation entre le verbe et le temps de la phrase aurait pr√©dit le temps correct du verbe "sat". Nous voulons encourager ce niveau de compr√©hension de la part de nos syst√®mes vocaux. Ainsi, bien que le WER soit moins indulgent que le CER, il est √©galement plus propice aux types de syst√®mes intelligibles que nous souhaitons d√©velopper. C'est pourquoi nous utilisons g√©n√©ralement le WER et nous vous encourageons √† faire de m√™me ! Toutefois, dans certaines circonstances, il n'est pas possible de l'utiliser.
En effet, certaines langues, comme le mandarin et le japonais, n'ont pas de notion de "mots", et le WER n'a donc pas de sens. Dans ce cas, nous revenons √† l'utilisation du CER.

Dans notre exemple, nous n'avons utilis√© qu'une seule phrase pour calculer le WER. Lors de l'√©valuation d'un syst√®me r√©el, nous utilisons g√©n√©ralement un ensemble de test complet compos√© de plusieurs milliers de phrases. Lorsque l'√©valuation porte sur plusieurs phrases, nous agr√©geons Sub, Inv, Sup et N pour toutes les phrases, puis nous calculons le WER √† l'aide de la formule d√©finie ci-dessus. Cela permet d'obtenir une meilleure estimation du WER pour des donn√©es in√©dites.

## Normalisation

Si nous entra√Ænons un mod√®le d'ASR sur des donn√©es contenant de la ponctuation et de la casse, il apprendra √† pr√©dire la casse et la ponctuation dans ses transcriptions. C'est une bonne chose lorsque nous voulons utiliser notre mod√®le pour des applications r√©elles, telles que la transcription de r√©unions ou de dict√©es, car les transcriptions pr√©dites seront enti√®rement format√©es avec la casse et la ponctuation, un style appel√© *orthographique*.

Cependant, nous avons √©galement la possibilit√© de *normaliser* le jeu de donn√©es afin d'en supprimer la casse et la ponctuation. La normalisation du jeu de donn√©es facilite la t√¢che de reconnaissance vocale : le mod√®le n'a plus besoin de faire la distinction entre les majuscules et les minuscules, ni de pr√©dire la ponctuation √† partir des seules donn√©es audio (par exemple, quel est le son d'un point-virgule ?).
De ce fait, les taux d'erreur sur les mots sont naturellement plus faibles (ce qui signifie que les r√©sultats sont meilleurs). L'article de Whisper d√©montre
l'effet radical que la normalisation des transcriptions peut avoir sur les r√©sultats du WER (*cf.* Section 4.4 du [papier] (https://cdn.openai.com/papers/whisper.pdf)).
Bien que nous obtenions des WER plus bas, le mod√®le n'est pas n√©cessairement meilleur pour la production. L'absence de casse et de ponctuation rend le texte pr√©dit par le mod√®le nettement plus difficile √† lire. Prenons l'exemple de la [section pr√©c√©dente](asr_models), o√π nous avons utilis√© Wav2Vec2 et Whisper sur le m√™me √©chantillon audio du jeu de donn√©es LibriSpeech. Le mod√®le Wav2Vec2 ne pr√©dit ni la ponctuation ni la casse, alors que Whisper pr√©dit les deux. En comparant les transcriptions c√¥te √† c√¥te, nous constatons que la transcription de Whisper est beaucoup plus facile √† lire :

```
Wav2Vec2:  HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAUS AND ROSE BEEF LOOMING BEFORE US SIMALYIS DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND
Whisper:   He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.
```

La transcription Whisper est orthographique et donc pr√™te √† l'emploi. Au contraire, nous devrions utiliser un post-traitement suppl√©mentaire pour restaurer la ponctuation et la casse dans nos pr√©dictions Wav2Vec2 si nous voulions les utiliser pour des applications en aval.

Il existe un juste milieu entre normaliser et ne pas normaliser : nous pouvons entra√Æner nos syst√®mes sur des transcriptions orthographiques, puis normaliser les pr√©dictions et les cibles avant de calculer le WER. De cette mani√®re, nous entra√Ænons nos syst√®mes √† pr√©dire des textes enti√®rement format√©s, mais nous b√©n√©ficions √©galement des am√©liorations du WER que nous obtenons en normalisant les transcriptions.

Whisper a √©t√© publi√© avec un normaliseur qui g√®re efficacement la normalisation de la casse, de la ponctuation et du formatage des nombres, entre autres. Appliquons le aux transcriptions de Whisper pour montrer comment nous pouvons les normaliser :

```python
from transformers.models.whisper.english_normalizer import BasicTextNormalizer

normalizer = BasicTextNormalizer()

prediction = " He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind."
normalized_prediction = normalizer(prediction)

normalized_prediction
```

**Sortie :**

```
' he tells us that at this festive season of the year with christmas and roast beef looming before us similarly is drawn from eating and its results occur most readily to the mind '
```

Nous pouvons constater que le texte a √©t√© enti√®rement mis en minuscules et que toute la ponctuation a √©t√© supprim√©e. D√©finissons maintenant la transcription de r√©f√©rence et calculons le WER normalis√© entre la r√©f√©rence et la cible :

```python
reference = "HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND"
normalized_referece = normalizer(reference)

wer = wer_metric.compute(
    references=[normalized_referece], predictions=[normalized_prediction]
)
wer
```

**Sortie :**

```
0.0625
```

6,25 % ; c'est √† peu pr√®s ce √† quoi nous nous attendions pour le mod√®le de base de Whisper sur l'ensemble de validation de LibriSpeech. Comme nous le voyons ici, nous avons pr√©dit une transcription orthographique, mais nous avons b√©n√©fici√© de l'augmentation du WER obtenue en normalisant la r√©f√©rence et la pr√©diction avant de calculer le WER.

Le choix de la m√©thode de normalisation des transcriptions d√©pend en fin de compte de vos besoins. Nous recommandons d'entra√Æner sur du texte orthographique et d'√©valuer sur du texte normalis√© afin d'obtenir le meilleur des deux mondes.

## Assembler le tout

Nous avons couvert trois sujets jusqu'√† pr√©sent dans cette unit√© : les mod√®les pr√©-entra√Æn√©s, la s√©lection des jeux de donn√©es et l'√©valuation. 
Nous allons nous pr√©parer pour la prochaine section sur le *finetuning* en √©valuant le mod√®le Whisper pr√©-entra√Æn√© sur l'ensemble de test Common Voice 13 Dhivehi. Nous utiliserons le WER pour √©tablir une _base_ pour la prochaine section o√π notre objectif sera d'essayer de le battre ü•ä

Tout d'abord, nous allons charger le mod√®le Whisper pr√©-entra√Æn√© en utilisant la classe `pipeline()`. Ce processus vous est maintenant familier !
La seule nouveaut√© est de charger le mod√®le en demi-pr√©cision (float16) s'il est ex√©cut√© sur un GPU. Cela permet d'acc√©l√©rer l'inf√©rence sans que le WER n'en souffre.

```python
from transformers import pipeline
import torch

if torch.cuda.is_available():
    device = "cuda:0"
    torch_dtype = torch.float16
else:
    device = "cpu"
    torch_dtype = torch.float32

pipe = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-small",
    torch_dtype=torch_dtype,
    device=device,
)
```

Ensuite, nous allons charger l'√©chantillon test Dhivehi de Common Voice 13. Nous avons vu dans la section pr√©c√©dente que Common Voice 13 est *s√©curis√©*, ce qui signifie que nous avons d√ª accepter les conditions d'utilisation du jeu de donn√©es avant d'y avoir acc√®s. Nous pouvons maintenant relier notre compte Hugging Face √† notre *notebook*, afin d'avoir acc√®s au jeu de donn√©es √† partir de la machine que nous utilisons actuellement.

Lier le *notebook* au *Hub* est tr√®s simple, il suffit d'entrer votre *token* d'authentification au *Hub* lorsque l'on vous y invite.
Votre *token* d'authentification est trouvable [ici](https://huggingface.co/settings/tokens).

```python
from huggingface_hub import notebook_login

notebook_login()
```

Une fois que nous avons li√© le *notebook* √† notre compte Hugging Face, nous pouvons proc√©der au t√©l√©chargement du jeu de donn√©es Common Voice. Cela prendra quelques minutes de les t√©l√©charger et de les pr√©traiter automatiquement sur votre *notebook* :

```python
from datasets import load_dataset

common_voice_test = load_dataset(
    "mozilla-foundation/common_voice_13_0", "dv", split="test"
)
```

<Tip>
    Si vous rencontrez un probl√®me d'authentification lors du chargement du jeu de donn√©es, assurez-vous que vous avez accept√© les conditions d'utilisation du jeu de donn√©es sur le *Hub* via le lien suivant : https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0.
</Tip>

L'√©valuation sur un jeu de donn√©es entier peut √™tre faite de la m√™me mani√®re que sur un seul exemple. Tout ce que nous avons √† faire est de **boucler** sur les audios d'entr√©e, plut√¥t que d'inf√©rer un seul √©chantillon. Pour ce faire, nous transformons d'abord notre jeu de donn√©es en un `KeyDataset`. Cela s√©lectionne la colonne particuli√®re du jeu de donn√©es que nous voulons transmettre au mod√®le (dans notre cas, c'est la colonne `"audio"`), en ignorant le reste (comme les transcriptions cibles, que nous ne voulons pas utiliser pour l'inf√©rence). Nous it√©rons ensuite sur ce jeu de donn√©es transform√©, en ajoutant les sorties du mod√®le √† une liste pour sauvegarder les pr√©dictions. La cellule de code suivante prendra environ cinq minutes si elle est ex√©cut√©e sur un GPU en demi-pr√©cision, avec un maximum de 12 Go de m√©moire :

```python
from tqdm import tqdm
from transformers.pipelines.pt_utils import KeyDataset

all_predictions = []

# lancer l'inf√©rence en streaming
for prediction in tqdm(
    pipe(
        KeyDataset(common_voice_test, "audio"),
        max_new_tokens=128,
        generate_kwargs={"task": "transcribe"},
        batch_size=32,
    ),
    total=len(common_voice_test),
):
    all_predictions.append(prediction["text"])
```

<Tip>
Si vous rencontrez une erreur de m√©moire CUDA lors de l'ex√©cution de la cellule ci-dessus, r√©duisez progressivement la taille du batch par un facteur de 2 jusqu'√† ce que vous trouviez une taille de batch qui convienne √† votre appareil.
</Tip>

Enfin, nous pouvons calculer le WER. Calculons d'abord le WER orthographique, c'est-√†-dire le WER sans post-traitement :

```python
from evaluate import load

wer_metric = load("wer")

wer_ortho = 100 * wer_metric.compute(
    references=common_voice_test["sentence"], predictions=all_predictions
)
wer_ortho
```

**Sortie :**

```
167.29577268612022
```

D'accord... 167% signifie essentiellement que notre mod√®le produit n'imprte quoi üòú Ne vous inqui√©tez pas, notre objectif est d'am√©liorer ce r√©sultat en *fientunant* le mod√®le sur l'ensemble d'entra√Ænement Dhivehi !

Ensuite, nous √©valuons le WER normalis√©, c'est-√†-dire le WER avec post-traitement de normalisation. Nous devons filtrer nos √©chantillons qui seraient vides apr√®s normalisation, car sinon le nombre total de mots dans notre r√©f√©rence (N) serait nul, ce qui donnerait une erreur de division par z√©ro dans notre calcul :

```python
from transformers.models.whisper.english_normalizer import BasicTextNormalizer

normalizer = BasicTextNormalizer()

# calculer le WER normalis√©
all_predictions_norm = [normalizer(pred) for pred in all_predictions]
all_references_norm = [normalizer(label) for label in common_voice_test["sentence"]]

# √©tape de filtrage pour n'√©valuer que les √©chantillons qui correspondent √† des r√©f√©rences non nulles
all_predictions_norm = [
    all_predictions_norm[i]
    for i in range(len(all_predictions_norm))
    if len(all_references_norm[i]) > 0
]
all_references_norm = [
    all_references_norm[i]
    for i in range(len(all_references_norm))
    if len(all_references_norm[i]) > 0
]

wer = 100 * wer_metric.compute(
    references=all_references_norm, predictions=all_predictions_norm
)

wer
```

**Sortie :**

```
125.69809089960707
```

Une fois de plus, nous constatons la r√©duction drastique du WER obtenue en normalisant nos r√©f√©rences et nos pr√©dictions : le mod√®le de base obtient un WER de 168 % pour le test orthographique, alors que le WER normalis√© est de 126 %.

Voil√† qui est clair ! Ce sont les chiffres que nous voulons essayer de battre lors du *finetuning* du mod√®le de reconnaissance de la parole en Dhivehi. Poursuivez votre lecture pour vous familiariser avec un exemple de *finetuning* üöÄ
