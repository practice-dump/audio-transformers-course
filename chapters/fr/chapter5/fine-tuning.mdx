# Comment finetuner un syst√®me de reconnaissance automatique de la parole avec l'API Trainer

Dans cette section, nous allons couvrir un guide √©tape par √©tape pour *finetuner* Whisper pour la reconnaissance automatique de la parole sur le jeu de donn√©es Common Voice 13. Nous utiliserons la version "*small*" du mod√®le et un jeu de donn√©es relativement l√©ger, vous permettant d'ex√©cuter le *finetuning* assez rapidement sur n'importe quel GPU de plus de 16 Go avec de faibles besoins en espace disque, comme le GPU T4 de 16 Go fourni dans le volet gratuit de Google Colab.

Si vous disposez d'un GPU plus petit ou si vous rencontrez des probl√®mes de m√©moire pendant l'entra√Ænement, vous pouvez suivre les suggestions fournies pour r√©duire l'utilisation de la m√©moire. √Ä l'inverse, si vous avez acc√®s √† un GPU plus puissant, vous pouvez modifier les arguments d'entra√Ænement pour maximiser votre capacit√© de traitement. Ce guide est donc accessible quelles que soient les sp√©cifications de votre GPU !

De m√™me, ce guide porte plus pr√©cis√©ment sur le *finetuning* sur la langue Dhivehi mais les √©tapes couvertes ici se g√©n√©ralisent √† n'importe quelle langue du jeu de donn√©es Common Voice, et plus g√©n√©ralement √† n'importe quel jeu de donn√©es d'ASR disponible sur le *Hub*.
Vous pouvez modifier le code pour passer rapidement √† la langue de votre choix et *finetuner* un mod√®le Whisper dans votre langue maternelle üåç.

Commen√ßons et donnons le coup d'envoi de notre pipeline de *finetuning* !

## Pr√©parer l'environnement

Nous vous conseillons vivement de t√©l√©charger les *checkpoints* du mod√®le directement sur le [*Hub*](https://huggingface.co/).
Le *Hub* offre les avantages suivants :
- Un contr√¥le de version int√©gr√© : vous pouvez √™tre s√ªr qu'aucun *checkpoint* n'est perdu pendant l'entra√Ænement.
- Tensorboard : suivi des mesures importantes au cours de l'entra√Ænement.
- Cartes de mod√®le : documenter ce que fait un mod√®le et ses cas d'utilisation pr√©vus.
- Communaut√© : un moyen facile de partager et de collaborer avec la communaut√© ! ü§ó

Lier le *notebook* au *Hub* est tr√®s simple, il suffit d'entrer votre *token* d'authentification au *Hub* lorsque l'on vous y invite.
Votre *token* d'authentification est trouvable [ici](https://huggingface.co/settings/tokens).

```python
from huggingface_hub import notebook_login

notebook_login()
```

**Sortie :**

```bash
Login successful
Your token has been saved to /root/.huggingface/token
```

## Charger le jeu de donn√©es

[Common Voice 13](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0) contient environ dix heures de donn√©es Dhivehi √©tiquet√©es, dont trois sont des donn√©es de test. Il s'agit de tr√®s peu de donn√©es pour un *finetuning*, nous nous appuierons donc sur la connaissance multilingue acquise par Whisper pendant le pr√©-entra√Ænement.

En utilisant les ü§ó *Datasets*, le t√©l√©chargement et la pr√©paration des donn√©es sont extr√™mement simples. Nous pouvons t√©l√©charger et pr√©parer les ensembles de Common Voice 13 en une seule ligne de code. Puisque le Dhivehi est tr√®s pauvre en ressources, nous combinerons les splits `train` et `validation` pour obtenir environ sept heures de donn√©es d'entra√Ænement. Nous utiliserons les trois heures de donn√©es `test` comme notre ensemble de test :

```python
from datasets import load_dataset, DatasetDict

common_voice = DatasetDict()

common_voice["train"] = load_dataset(
    "mozilla-foundation/common_voice_13_0", "dv", split="train+validation"
)
common_voice["test"] = load_dataset(
    "mozilla-foundation/common_voice_13_0", "dv", split="test"
)

print(common_voice)
```

**Sortie :**

```
DatasetDict({
    train: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],
        num_rows: 4904
    })
    test: Dataset({
        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],
        num_rows: 2212
    })
})
```

<Tip>
    Vous pouvez changer l'identifiant de langue de `"dv"` pour un identifiant de langue de votre choix. Pour voir toutes les langues possibles dans Common Voice 13, consultez la carte du jeu de donn√©es sur le *Hub* : https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0
</Tip>

La plupart des jeux de donn√©es d'ASR ne fournissent que des √©chantillons audio en entr√©e (`audio`) et le texte transcrit correspondant (`sentence`).
Common Voice contient des m√©tadonn√©es suppl√©mentaires, telles que `accent` et `locale`, que nous pouvons ignorer pour l'ASR.
En gardant le *notebook* aussi g√©n√©ral que possible, nous ne consid√©rons que l'audio d'entr√©e et le texte transcrit en √©cartant les informations de m√©tadonn√©es suppl√©mentaires :

```python
common_voice = common_voice.select_columns(["audio", "sentence"])
```

## Extracteur de caract√©ristiques, tokeniser et processeur

Le pipeline d'ASR peut √™tre d√©compos√© en trois √©tapes :

1. L'extracteur de caract√©ristiques qui pr√©-traite les entr√©es audio brutes en spectrogrammes log-m√©l.
2. Le mod√®le qui effectue l'association s√©quence-s√©quence
3. Le tokenizer qui post-traite les *tokens* pr√©dits en texte.

Dans ü§ó *Transformers*, le mod√®le Whisper est associ√© √† un extracteur de caract√©ristiques et √† un tokenizer, appel√©s respectivement [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor) et [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer).
Pour nous simplifier la vie, ces deux objets sont regroup√©s dans une seule classe, appel√©e [WhisperProcessor](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor).
Nous pouvons appeler le WhisperProcessor pour effectuer √† la fois le pr√©traitement audio et le post-traitement des *tokens* de texte. Ce faisant, nous n'avons besoin de suivre que deux objets pendant l'entra√Æningment : le processeur et le mod√®le.

Lors d'un *finetun√©* multilingue, nous devons d√©finir la `"language"` et la `"task"` lors de l'instanciation du processeur.
La `"language"` doit √™tre fix√©e √† la langue audio source, et la t√¢che √† `"translate"` pour la reconnaissance vocale ou √† `"translate"` pour la traduction vocale. Ces arguments modifient le comportement du *tokens*, et doivent √™tre d√©finis correctement pour s'assurer que les √©tiquettes cibles sont encod√©es correctement.

Nous pouvons voir toutes les langues possibles support√©es par Whisper en important la liste des langues :

```python
from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE

TO_LANGUAGE_CODE
```

Si vous parcourez cette liste, vous remarquerez que de nombreuses langues sont pr√©sentes, mais que le dhivehi est l'une des rares √† ne pas l'√™tre !
Cela signifie que Whisper n'a pas √©t√© pr√©-entra√Æn√© sur le dhivehi. Cependant, cela ne signifie pas que nous ne pouvons pas *finetuner* Whisper sur cette langue.
En faisant cela, nous allons entra√Æner Whisper dans une nouvelle langue, une langue non support√©e par le *checkpoint* pr√©-entra√Æn√©. C'est plut√¥t cool, non ?

Lorsque vous le *finetunez* sur une nouvelle langue, Whisper fait un bon travail en tirant parti de sa connaissance des 96 autres langues sur lesquelles il a √©t√© pr√©-entra√Æn√©. En g√©n√©ral, toutes les langues modernes seront linguistiquement similaires √† au moins l'une des 96 langues que Whisper conna√Æt d√©j√†, nous nous inscrivons donc dans ce paradigme de repr√©sentation des connaissances interlinguistiques.

Ce que nous devons faire pour cette nouvelle langue, est de trouver la langue **la plus similaire** sur laquelle Whisper a √©t√© pr√©-entra√Æn√©. L'article de Wikip√©dia sur le dhivehi indique que cette langue est √©troitement li√©e √† la langue cinghalaise du Sri Lanka.
Si nous v√©rifions √† nouveau les codes de langue, nous pouvons voir que le cinghalais est pr√©sent dans le jeu de langues de Whisper, nous pouvons donc en toute s√©curit√© mettre notre argument de langue √† `"sinhalese"`.

Nous allons charger notre processeur √† partir du *checkpoint* pr√©-entra√Æn√©, en fixant la langue √† `"sinhalese"` et la t√¢che √† `"transcribe"` comme expliqu√© ci-dessus :

```python
from transformers import WhisperProcessor

processor = WhisperProcessor.from_pretrained(
    "openai/whisper-small", language="sinhalese", task="transcribe"
)
```

Il est utile de rappeler que dans la plupart des cas, vous constaterez que la langue sur laquelle vous souhaitez effectuer un *finetuning* se trouve dans l'ensemble des langues de pr√©-entra√Ænement, auquel cas vous pouvez simplement d√©finir cette langue directement comme langue audio source ! Notez que ces deux arguments doivent √™tre omis pour le *finetuning* en anglais o√π il n'y a qu'une seule option pour la langue (`"English"`) et la t√¢che (`"transcribe"`).

## Pr√©traitement des donn√©es

Jetons un coup d'oeil aux caract√©ristiques du jeu de donn√©es. Portez une attention particuli√®re √† la colonne `"audio"`, elle d√©taille le taux d'√©chantillonnage de nos entr√©es audio :

```python
common_voice["train"].features
```

**Sortie :**

```
{'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),
 'sentence': Value(dtype='string', id=None)}
```

Puisque notre audio d'entr√©e est √©chantillonn√© √† 48kHz, nous devons le sous-√©chantillonner √† 16kHz avant de le passer √† l'extracteur de caract√©ristiques de Whisper qui est la fr√©quence d'√©chantillonnage attendue par le mod√®le.

Nous allons r√©gler les entr√©es audio √† la bonne fr√©quence d'√©chantillonnage en utilisant la m√©thode [`cast_column`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.cast_column) du jeu de donn√©es. Cette op√©ration ne modifie pas l'audio sur place, mais signale aux jeux de donn√©es de r√©√©chantillonner les √©chantillons audio √† la vol√©e lorsqu'ils sont charg√©s :

```python
from datasets import Audio

sampling_rate = processor.feature_extractor.sampling_rate
common_voice = common_voice.cast_column("audio", Audio(sampling_rate=sampling_rate))
```

Nous pouvons maintenant √©crire une fonction pour pr√©parer nos donn√©es pour le mod√®le :

1. Nous chargeons et r√©√©chantillonnons les donn√©es audio √©chantillon par √©chantillon en appelant `sample["audio"]`. Comme expliqu√© ci-dessus, ü§ó *Datasets* effectue toutes les op√©rations de r√©√©chantillonnage n√©cessaires √† la vol√©e.
2. Nous utilisons l'extracteur de caract√©ristiques pour calculer les caract√©ristiques d'entr√©e du spectrogramme log-mel √† partir de notre tableau audio unidimensionnel.
3. Nous encodons les transcriptions en identifiants d'√©tiquettes √† l'aide d'un *tokenizer*.

```python
def prepare_dataset(example):
    audio = example["audio"]

    example = processor(
        audio=audio["array"],
        sampling_rate=audio["sampling_rate"],
        text=example["sentence"],
    )

    # compute input length of audio sample in seconds
    example["input_length"] = len(audio["array"]) / audio["sampling_rate"]

    return example
```

Nous pouvons appliquer la fonction de pr√©paration des donn√©es √† tous nos exemples d'entra√Ænement en utilisant la m√©thode `.map` de ü§ó *Datasets*. Nous allons supprimer les colonnes des donn√©es d'entra√Ænement brutes (l'audio et le texte), en ne laissant que les colonnes renvoy√©es par la fonction `prepare_dataset` :

```python
common_voice = common_voice.map(
    prepare_dataset, remove_columns=common_voice.column_names["train"], num_proc=1
)
```

Enfin, nous filtrons toutes les donn√©es d'entra√Ænement contenant des √©chantillons audio de plus de 30 secondes. Ces √©chantillons seraient sinon tronqu√©s par l'extracteur de caract√©ristiques de Whisper, ce qui pourrait affecter la stabilit√© de l'entra√Ænement. Nous d√©finissons une fonction qui renvoie `True` pour les √©chantillons de moins de 30 secondes, et `False` pour ceux qui sont plus longs :

```python
max_input_length = 30.0


def is_audio_in_length_range(length):
    return length < max_input_length
```

Nous appliquons notre fonction de filtrage √† tous les √©chantillons de notre jeu de donn√©es d'entra√Ænement par le biais de la m√©thode `.filter` de ü§ó *Datasets* :

```python
common_voice["train"] = common_voice["train"].filter(
    is_audio_in_length_range,
    input_columns=["input_length"],
)
```

V√©rifions la quantit√© de donn√©es d'entra√Ænement que nous avons supprim√©e gr√¢ce √† cette √©tape de filtrage :

```python
common_voice["train"]
```

**Sortie :**

```
Dataset({
    features: ['input_features', 'labels', 'input_length'],
    num_rows: 4904
})
```

Dans ce cas, nous avons en fait le m√™me nombre d'√©chantillons que pr√©c√©demment, donc il n'y a pas d'√©chantillons de plus de 30s.
Cela pourrait ne pas √™tre le cas si vous changez de langue, il est donc pr√©f√©rable de garder cette √©tape de filtrage en place pour plus de robustesse. Nos donn√©es sont maintenant pr√™tes √† √™tre entra√Æn√©es ! Continuons et regardons comment nous pouvons utiliser ces donn√©es pour le *finetuning*.

## Entra√Ænement et √©valuation

Maintenant que nous avons pr√©par√© nos donn√©es, nous sommes pr√™ts √† plonger dans le pipeline d'entra√Ænement.
[Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)
va faire le gros du travail √† notre place. Tout ce que nous avons √† faire est de :

- D√©finir un assembleur de donn√©es qui prend nos donn√©es pr√©trait√©es et pr√©pare des tenseurs PyTorch adapt√©s au mod√®le.

- M√©triques d'√©valuation : lors de l'√©valuation, nous voulons √©valuer le mod√®le en utilisant la m√©trique du taux d'erreur au niveau du mot (WER). Nous devons d√©finir une fonction `compute_metrics` qui g√®re ce calcul.

- Charger un *checkpoint* pr√©-entra√Æn√© et le configurer correctement pour l'entra√Ænement.

- D√©finir les arguments d'entra√Ænement : ils seront utilis√©s par le Trainer pour construire le plannificateur d'entra√Ænement.

Une fois le mod√®le *finetun√©*, nous l'√©valuerons sur les donn√©es de test pour v√©rifier que nous l'avons correctement entra√Æn√© √† transcrire la parole en Dhivehi.

### D√©finir un assembleur de donn√©es

L'assembleur de donn√©es pour un mod√®le audio s√©quence-√†-s√©quence est unique dans le sens o√π il traite les `input_features` et les `labels` ind√©pendamment : les `input_features` doivent √™tre trait√©es par l'extracteur de caract√©ristiques et les `labels` par le tokenizer.

Les `input_features` sont d√©j√† rembourr√©es √† 30s et converties en un spectrogramme log-Mel de dimension fixe, donc tout ce que nous avons √† faire est de les convertir en tenseurs PyTorch batch√©s. Nous le faisons en utilisant la m√©thode `.pad` de l'extracteur de caract√©ristiques avec `return_tensors=pt`. Notez qu'aucun rembourrage suppl√©mentaire n'est appliqu√© ici puisque les entr√©es sont de dimension fixe, les `input_features` sont simplement converties en tenseurs PyTorch.

D'un autre c√¥t√©, les `labels` ne sont pas rembourr√©s. Les s√©quences sont d'abord remplac√©es par la longueur maximale du batch √† l'aide de la m√©thode `.pad` du *tokenizer*. Les *tokens* de remplissage sont ensuite remplac√©s par `-100` de sorte que ces tokens ne sont **pas** pris en compte lors du calcul de la perte. Nous coupons ensuite le d√©but du *token* de transcription du d√©but de la s√©quence d'√©tiquettes comme nous l'ajouterons plus tard pendant l'entra√Ænement.

Nous pouvons utiliser le `WhisperProcessor` que nous avons d√©fini plus t√¥t pour effectuer √† la fois les op√©rations de l'extracteur de caract√©ristiques et du *tokenizer* :

```python
import torch

from dataclasses import dataclass
from typing import Any, Dict, List, Union


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        # diviser les entr√©es et les √©tiquettes car elles doivent √™tre de longueurs diff√©rentes et n√©cessitent des m√©thodes de remplissage diff√©rentes
        # traiter d'abord les entr√©es audio en renvoyant simplement des tenseurs Torch
        input_features = [
            {"input_features": feature["input_features"][0]} for feature in features
        ]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")

        # obtenir les s√©quences d'√©tiquettes tokenis√©es
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        # rembourrer les √©tiquettes √† la longueur maximale
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # remplacer le remplissage par -100 pour ignorer correctement les pertes
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # si le token bos est ajout√© lors de l'√©tape de tokenisation pr√©c√©dente, couper le token bos ici puisqu'il sera de toute fa√ßon ajout√© plus tard
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels

        return batch
```

Nous pouvons maintenant initialiser l'assembleur de donn√©es que nous venons de d√©finir :

```python
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
```

En avant !

### M√©triques d'√©valuation

Ensuite, nous d√©finissons la m√©trique d'√©valuation que nous utiliserons sur notre ensemble d'√©valuation. Nous utiliserons le taux d'erreur au niveaud du mot (WER) introduit dans la section [√©valuation](√©valuation), la m√©trique "de-facto" pour √©valuer les syst√®mes d'ASR.

Nous chargerons la m√©trique WER √† partir d' ü§ó *Evaluate* :

```python
import evaluate

metric = evaluate.load("wer")
```

Il suffit ensuite de d√©finir une fonction qui prend les pr√©dictions de notre mod√®le et renvoie la m√©trique WER. Cette fonction, appel√©e `compute_metrics`, remplace d'abord `-100` par le `pad_token_id` dans `label_ids` (annulant l'√©tape que nous avons appliqu√©e dans l'assembleur de donn√©es pour ignorer correctement les *tokens* rembourr√©s dans la perte). Il d√©code ensuite les identifiants pr√©dits et les identifiants d'√©tiquettes en cha√Ænes de caract√®res. Enfin, il calcule le WER entre les pr√©dictions et les √©tiquettes de r√©f√©rence. Ici, nous avons la possibilit√© d'√©valuer les transcriptions et les pr√©dictions "normalis√©es", dont la ponctuation et la casse ont √©t√© supprim√©es. Nous vous recommandons de proc√©der ainsi pour b√©n√©ficier de l'am√©lioration du WER obtenue par la normalisation des transcriptions.

```python
from transformers.models.whisper.english_normalizer import BasicTextNormalizer

normalizer = BasicTextNormalizer()


def compute_metrics(pred):
    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # remplacer -100 par pad_token_id
    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id

    # nous ne voulons pas grouper les *tokens* lors du calcul des m√©triques
    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)

    # calculer le Wer orthographique
    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)

    # calculer le WER normalis√©
    pred_str_norm = [normalizer(pred) for pred in pred_str]
    label_str_norm = [normalizer(label) for label in label_str]
    # afin de n'√©valuer que les √©chantillons correspondant √† des r√©f√©rences non nulles
    pred_str_norm = [
        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0
    ]
    label_str_norm = [
        label_str_norm[i]
        for i in range(len(label_str_norm))
        if len(label_str_norm[i]) > 0
    ]

    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)

    return {"wer_ortho": wer_ortho, "wer": wer}
```

### Charger un *checkpoint* pr√©-entra√Æn√©

Chargeons maintenant le *checkpoint* pr√©-entra√Æn√© de Whisper small. Encore une fois, ceci est trivial gr√¢ce √† l'utilisation de ü§ó *Transformers* !

```python
from transformers import WhisperForConditionalGeneration

model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
```

Nous allons mettre `use_cache` √† `False` pour l'entra√Ænement puisque nous utilisons [*gradient checkpointing*] (https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing) et que les deux sont incompatibles. Nous allons aussi surcharger deux arguments de g√©n√©ration pour contr√¥ler le comportement du mod√®le pendant l'inf√©rence : nous allons forcer la langue et les *tokens* de t√¢che pendant la g√©n√©ration en d√©finissant les arguments `language` et `task`, et aussi r√©activer le cache pour la g√©n√©ration afin d'acc√©l√©rer le temps d'inf√©rence :

```python
from functools import partial

# d√©sactiver le cache pendant l'entra√Ænement car il est incompatible avec le checkpointing du gradient
model.config.use_cache = False

# d√©finir la langue et la t√¢che pour la g√©n√©ration et r√©activer le cache
model.generate = partial(
    model.generate, language="sinhalese", task="transcribe", use_cache=True
)
```

## D√©finir la configuration de l'entra√Ænement

Dans la derni√®re √©tape, nous d√©finissons tous les param√®tres li√©s √† l'entra√Ænement. Ici, nous fixons le nombre d'√©tapes d'entra√Ænement √† 500.
Cela repr√©sente suffisamment d'√©tapes pour voir une grande am√©lioration du WER par rapport au mod√®le pr√©-entra√Æn√©, tout en s'assurant que le *finetuning* peut √™tre ex√©cut√© en environ 45 minutes sur un niveau gratuit de Google Colab. Pour plus de d√©tails sur les arguments d'entra√Ænement, reportez-vous √† la documentation de [Seq2SeqTrainingArguments] (https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments).

```python
from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-dv",  # nom sur le Hub
    per_device_train_batch_size=16,
    gradient_accumulation_steps=1,  # √† x2  pour chaque diminution de 2x de la taille du batch
    learning_rate=1e-5,
    lr_scheduler_type="constant_with_warmup",
    warmup_steps=50,
    max_steps=500,  # augmenter jusqu'√† 4000 si vous disposez de votre propre GPU ou d'un plan Colab payant
    gradient_checkpointing=True,
    fp16=True,
    fp16_full_eval=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=16,
    predict_with_generate=True,
    generation_max_length=225,
    save_steps=500,
    eval_steps=500,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    push_to_hub=True,
)
```

<Tip>
    Si vous ne voulez pas t√©l√©charger les *checkpoints* du mod√®le vers le *Hub*, mettez `push_to_hub=False`.
</Tip>

Nous pouvons transmettre les arguments d'entra√Ænement au *Trainer* avec notre mod√®le, notre jeu de donn√©es, notre assembleur de donn√©es et la fonction `compute_metrics` :

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor,
)
```

Et voil√†, nous sommes pr√™ts √† entra√Æner !

#### Entra√Ænement

Pour lancer l'entra√Ænement, il suffit d'ex√©cuter :

```python
trainer.train()
```

L'entra√Ænement prendra environ 45 minutes en fonction de votre GPU ou de celui allou√© par Google Colab. En fonction de votre GPU, il est possible que vous rencontriez une erreur CUDA `"out-of-memory"` lorsque vous commencez √† entra√Æner. Dans ce cas, vous pouvez r√©duire le `per_device_train_batch_size` par incr√©ments d'un facteur 2 et utiliser [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps) pour compenser.

**Sortie :**

| Training Loss | Epoch | Step | Validation Loss | Wer Ortho | Wer     |
|:-------------:|:-----:|:----:|:---------------:|:---------:|:-------:|
| 0.136         | 1.63  | 500  | 0.1727          | 63.8972   | 14.0661 |

Notre WER final est de 14,1 % ce qui n'est pas mal pour sept heures de donn√©es d'entra√Ænement et seulement 500 √©tapes d'entra√Ænement ! Cela repr√©sente une am√©lioration de 112 % par rapport au mod√®le pr√©-entra√Æn√© ! Cela signifie que nous avons pris un mod√®le qui n'avait aucune connaissance du dhivehi et que nous l'avons *finetun√©* pour reconna√Ætre l'dhivehi avec une pr√©cision ad√©quate en moins d'une heure ü§Ø.

La grande question est de savoir comment cela se compare √† d'autres syst√®mes ASR. Pour cela, nous pouvons consulter le [classement autoevaluate] (https://huggingface.co/spaces/autoevaluate/leaderboards?dataset=mozilla-foundation%2Fcommon_voice_13_0&only_verified=0&task=automatic-speech-recognition&config=dv&split=test&metric=wer) qui cat√©gorise les mod√®les par langue et par jeu de donn√©es, et les classe ensuite en fonction de leur WER.

En regardant le classement, nous voyons que notre mod√®le entra√Æn√© pour 500 √©tapes bat de mani√®re convaincante le *checkpoint* [Whisper Small](https://huggingface.co/openai/whisper-small) pr√©-entra√Æn√© que nous avons √©valu√© dans la section pr√©c√©dente. Bon travail üëè

Nous voyons qu'il y a quelques *checkpoints* qui font mieux que celui que nous avons entra√Æn√©. La beaut√© du *Hub* est qu'il s'agit d'une plateforme *collaborative*. Si nous n'avons pas le temps ou les ressources pour effectuer un entra√Ænement plus long nous-m√™mes, nous pouvons charger un *checkpoint* que quelqu'un d'autre dans la communaut√© a entra√Æn√© et a eu la gentillesse de partager (en s'assurant de le remercier pour cela !).
Vous pourrez charger ces *checkpoints* exactement de la m√™me mani√®re que les pr√©-entra√Æn√©s en utilisant la classe `pipeline` comme nous l'avons fait pr√©c√©demment ! Ainsi, rien ne vous emp√™che de s√©lectionner le meilleur mod√®le du leaderboard pour l'utiliser dans le cadre de votre t√¢che !

Nous pouvons automatiquement soumettre notre *checkpoint* au classement lorsque nous envoyons les r√©sultats de l'entra√Ænement au *Hub*. Nous devons simplement d√©finir les arguments de mot-cl√© appropri√©s (kwargs). Vous pouvez modifier ces valeurs pour qu'elles correspondent √† votre jeu de donn√©es, √† votre langue et au nom de votre mod√®le en cons√©quence :

```python
kwargs = {
    "dataset_tags": "mozilla-foundation/common_voice_13_0",
    "dataset": "Common Voice 13",  # a 'pretty' name for the training dataset
    "language": "dv",
    "model_name": "Whisper Small Dv - Sanchit Gandhi",  # a 'pretty' name for your model
    "finetuned_from": "openai/whisper-small",
    "tasks": "automatic-speech-recognition",
}
```

Les r√©sultats de l'entra√Ænement peuvent maintenant √™tre t√©l√©charg√©s vers le *Hub*. Pour ce faire, ex√©cutez la commande `push_to_hub` :

```python
trainer.push_to_hub(**kwargs)
```

Ceci sauvegardera les logs d'entra√Ænement et les poids des mod√®les sous `"votre-nom-d'utilisateur/le-nom-que-vous-avez-choisi"`. Pour cet exemple, regardez le t√©l√©chargement √† `sanchit-gandhi/whisper-small-dv`.

Bien que le mod√®le *finetun√©* donne des r√©sultats satisfaisants sur les donn√©es de test de Common Voice 13, il n'est en aucun cas optimal.
Le but de ce guide est de d√©montrer comment *finetuner* un mod√®le ASR en utilisant le *Trainer* pour la reconnaissance automatique de la parole multilingue.

Si vous avez acc√®s √† votre propre GPU ou si vous √™tes abonn√© √† un plan payant de Google Colab, vous pouvez augmenter `max_pas` √† 4000 pas pour am√©liorer davantage le WER en entra√Ænant plus de pas. Entra√Æner 4000 pas prendra environ 3 √† 5 heures en fonction de votre GPU et donnera des r√©sultats WER inf√©rieurs d'environ 3 % √† l'entra√Ænement de 500 pas. Si vous d√©cidez d'entra√Æner sur 4000 pas, nous vous recommandons √©galement de changer le planificateur de taux d'apprentissage pour un plan *lin√©aire* (set `lr_scheduler_type="linear"`), car cela permettra d'augmenter les performances sur de longues p√©riodes d'entra√Ænement.

Les r√©sultats pourraient probablement √™tre am√©lior√©s en optimisant les hyperparam√®tres d'entra√Ænement, tels que _learning rate_ et _dropout_, et en utilisant un *checkpoint* pr√©-entra√Æn√© plus grand (`medium` ou `large`). Nous laissons cet exercice au lecteur.

## Partager votre mod√®le

Vous pouvez maintenant partager ce mod√®le avec n'importe qui en utilisant le lien sur le *Hub*. Ils peuvent le charger avec l'identifiant `"votre-nom-d'utilisateur/le-nom-que-vous-avez-choisi"` directement dans l'objet `pipeline()`. Par exemple, pour charger le *checkpoint* *finetun√©* ["sanchit-gandhi/whisper-small-dv"](https://huggingface.co/sanchit-gandhi/whisper-small-dv) :

```python
from transformers import pipeline

pipe = pipeline("automatic-speech-recognition", model="sanchit-gandhi/whisper-small-dv")
```

## Conclusion

Dans cette section, nous avons couvert un guide √©tape par √©tape sur le *finetuning* du mod√®le Whisper pour la reconnaissance vocale en utilisants ü§ó *Jeux de donn√©es*, ü§ó *Transformers* et le *Hub*. Nous avons d'abord charg√© le sous-ensemble Dhivehi du jeu de donn√©es Common Voice 13 et l'avons pr√©trait√© en calculant des spectrogrammes log-mel et en tokenisant le texte. Nous avons ensuite d√©fini un assembleur de donn√©es, une m√©trique d'√©valuation et des arguments d'entra√Ænement, avant d'utiliser le *Trainer* pour entra√Æner et √©valuer notre mod√®le. Nous avons termin√© en t√©l√©chargeant le mod√®le *finetun√©* sur le *Hub*, et nous avons montr√© comment le partager et l'utiliser avec la classe `pipeline()`.

Si vous avez suivi jusqu'√† ce point, vous devriez maintenant avoir un *checkpoint* *finetun√©* pour la reconnaissance automatique de la parole, bien jou√© ! ü•≥
Plus important encore, vous √™tes √©quip√© de tous les outils n√©cessaires pour *finetuner* le mod√®le Whisper sur n'importe quel jeu de donn√©es ou domaine de reconnaissance vocale. Alors, qu'attendez-vous ? Choisissez l'un des jeux de donn√©es couverts dans la section [Choisir un jeu de donn√©es](choosing_dataset) ou s√©lectionnez un jeu de donn√©es de votre choix, et voyez si vous pouvez obtenir des performances de pointe ! Le classement vous attend...
