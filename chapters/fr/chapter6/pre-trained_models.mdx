# Mod√®les pr√©-entra√Æn√©s pour la synth√®se vocale

Par rapport aux t√¢ches de reconnaissance automatique de la parole et de classification audio, il y a beaucoup moins de *checkpoints* de mod√®les pr√©-entra√Æn√©s disponibles. Vous trouverez pr√®s de 300 sur le *Hub*. Parmi ces mod√®les pr√©-entra√Æn√©s, nous nous concentrerons sur deux architectures qui sont facilement disponibles dans la biblioth√®que ü§ó *Transformers* : SpeechT5 et Massive Multilingual Speech (MMS). Dans cette section, nous allons explorer comment utiliser ces mod√®les.

## SpeechT5 

[SpeechT5] (https://arxiv.org/abs/2110.07205) est un mod√®le publi√© par Junyi Ao et al. de Microsoft qui est capable de g√©rer une s√©rie de t√¢ches vocales. Bien que dans cette unit√©, nous nous concentrions sur l'aspect texte-parole, ce mod√®le peut √™tre adapt√© √† des t√¢ches de parole-texte (reconnaissance automatique de la parole ou identification du locuteur), ainsi qu'√† des t√¢ches de parole-parole (par exemple, am√©lioration de la parole ou conversion entre diff√©rentes voix). Ceci est d√ª √† la fa√ßon dont le mod√®le est con√ßu et pr√©-entra√Æn√©. 

Au c≈ìur de SpeechT5 se trouve un *transformer* encodeur-d√©codeur classique. Ce r√©seau mod√©lise une transformation de s√©quence √† s√©quence en utilisant des repr√©sentations cach√©es. Ce *transformer* est le m√™me pour toutes les t√¢ches prises en charge par SpeechT5.

Il est compl√©t√© par six pr√© ou post-r√©seaux modaux sp√©cifiques (parole/texte). La parole ou le texte en entr√©e (en fonction de la t√¢che) est pr√©trait√© par un pr√©-r√©seau correspondant afin d'obtenir les repr√©sentations cach√©es que le *transformer* peut utiliser. La sortie obtenue est ensuite transmise √† un post-r√©seau qui l'utilisera pour g√©n√©rer la sortie dans la modalit√© cible.

Voici √† quoi ressemble l'architecture (image tir√©e de l'article original) : 

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/speecht5/architecture.jpg" alt="SpeechT5 architecture from the original paper">
</div>

SpeechT5 est d'abord pr√©-entra√Æn√© √† l'aide de donn√©es vocales et textuelles non √©tiquet√©es en grande quantit√© afin d'acqu√©rir une repr√©sentation unifi√©e des diff√©rentes modalit√©s. Pendant la phase de pr√©-entra√Ænement, tous les pr√©-r√©seaux et post-r√©seaux sont utilis√©s simultan√©ment.

Apr√®s le pr√©-entra√Ænement, le r√©seau encodeur-d√©codeur *finetun√©* pour chaque t√¢che individuelle. √Ä cette √©tape, seuls les pr√©- et post-r√©seaux pertinents pour la t√¢che sp√©cifique sont utilis√©s. Par exemple, pour utiliser SpeechT5 pour la synth√®se vocale, vous aurez besoin du pr√©-r√©seau encodeur de texte pour les entr√©es de texte et des pr√©-r√©seaux et post-r√©seaux du d√©codeur de parole pour les sorties de parole. 

Cette approche permet d'obtenir plusieurs mod√®les *finetun√©s* pour diff√©rentes t√¢ches vocales qui b√©n√©ficient tous du pr√©-entra√Ænement initial sur des donn√©es non √©tiquet√©es.  

<Tip>

M√™me si les mod√®les *finetun√©s* commencent par utiliser le m√™me ensemble de poids provenant du mod√®le pr√©-entra√Æn√© partag√©, les versions finales sont toutes tr√®s diff√©rentes au bout du compte. Vous ne pouvez pas prendre un mod√®le ASR *finetun√©* et √©changer les pr√©- et post-r√©seaux pour obtenir un mod√®le de TTS fonctionnel, par exemple. SpeechT5 est flexible, mais pas √† ce point ;)

</Tip>

Voyons quels sont les pr√©- et post-r√©seaux que SpeechT5 utilise pour la t√¢che de TTS :

* Pr√©-r√©seau encodeur de texte : Une couche d'ench√¢ssement de texte qui fait correspondre les *tokens* de texte aux repr√©sentations cach√©es que l'encodeur attend. Ceci est similaire √† ce qui se passe dans un mod√®le NLP tel que BERT.
* Pr√©-r√©seau de d√©codage de la parole : Il prend en entr√©e un spectrogramme log-mel et utilise une s√©quence de couches lin√©aires pour compresser le spectrogramme en repr√©sentations cach√©es. 
* Post-r√©seau d√©codeur de la parole : Il pr√©dit un r√©sidu √† ajouter au spectrogramme de sortie et est utilis√© pour affiner les r√©sultats.

Une fois combin√©e, voici √† quoi ressemble l'architecture SpeechT5 pour la synth√®se vocale :

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/speecht5/tts.jpg" alt="SpeechT5 architecture for TTS">
</div>

Comme vous pouvez le constater, la sortie est un spectrogramme log mel et non une forme d'onde. Si vous vous souvenez, nous avons bri√®vement abord√© ce sujet dans l'[unit√© 3](../chapitre3/introduction#spectrogram-output). Il est courant que les mod√®les qui g√©n√®rent de l'audio produisent un spectrogramme log mel, qui doit √™tre converti en forme d'onde √† l'aide d'un r√©seau neuronal suppl√©mentaire appel√© vocodeur.

Voyons comment proc√©der.

Tout d'abord, chargeons le SpeechT5 *finetun√©* sur du TTS depuis le *Hub*, ainsi que l'objet processeur utilis√© pour la tokenisation et l'extraction de caract√©ristiques :

```python
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech

processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
```

Ensuite, le texte d'entr√©e est *tokenis√©*.

```python
inputs = processor(text="Don't count the days, make the days count.", return_tensors="pt")
```

Le SpeechT5 TTS n'est pas limit√© √† la cr√©ation de discours pour un seul locuteur. Il utilise en effet ce que l'on appelle des " ench√¢ssements de locuteurs, qui capturent les caract√©ristiques de la voix d'un locuteur particulier.

<Tip>

L'ench√¢ssement des locuteurs est une m√©thode permettant de repr√©senter l'identit√© d'un locuteur de mani√®re compacte, sous la forme d'un vecteur de taille fixe, quelle que soit la longueur de l'√©nonc√©. Ces ench√¢ssements capturent des informations essentielles sur la voix d'un locuteur, son accent, son intonation et d'autres caract√©ristiques uniques qui distinguent un locuteur d'un autre. De tels enregistrements peuvent √™tre utilis√©s pour la v√©rification du locuteur, la diarisation du locuteur, l'identification du locuteur, etc. 
Les techniques les plus courantes pour g√©n√©rer des ench√¢ssements de locuteurs sont les suivantes :

* Les I-Vecteurs (vecteurs d'identit√©) : ils sont bas√©s sur un mod√®le de m√©lange gaussien (GMM). Ils repr√©sentent les locuteurs sous la forme de vecteurs de faible dimension et de longueur fixe, d√©riv√©s des statistiques d'un GMM sp√©cifique au locuteur, et sont obtenus de mani√®re non supervis√©e. 
* Les vecteurs X : ils sont d√©riv√©s √† l'aide de r√©seaux neuronaux profonds (DNN) et capturent des informations sur le locuteur au niveau de l'image en incorporant le contexte temporel. 

[X-Vectors](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf) est une m√©thode de pointe qui montre des performances sup√©rieures sur des jeux de donn√©es d'√©valuation par rapport aux I-Vectors. Le r√©seau neuronal profond est utilis√© pour obtenir les X-Vecteurs : il est entra√Æn√© √† discriminer les locuteurs, et fait correspondre des √©nonc√©s de longueur variable √† des ench√¢ssements de dimension fixe. Vous pouvez √©galement charger un ench√¢ssement de locuteur X-Vector qui a √©t√© calcul√© √† l'avance et qui encapsule les caract√©ristiques d'√©locution d'un locuteur particulier.

</Tip>

Chargeons un tel ench√¢ssement de locuteur √† partir d'un jeu de donn√©es sur le *Hub*. Les ench√¢ssements ont √©t√© obtenus √† partir du [jeu de donn√©es CMU ARCTIC](http://www.festvox.org/cmu_arctic/) en utilisant [ce script](https://huggingface.co/mechanicalsea/speecht5-vc/blob/main/manifest/utils/prep_cmu_arctic_spkemb.py), mais n'importe quel ench√¢ssement X-Vector devrait fonctionner.

```python
from datasets import load_dataset

embeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")

import torch

speaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)
```

L'ench√¢ssement du locuteur est un tenseur de forme (1, 512). Cette repr√©sentation particuli√®re du locuteur d√©crit une voix f√©minine.

√Ä ce stade, nous disposons d√©j√† de suffisamment d'entr√©es pour g√©n√©rer un log mel spectrogramme en sortie :

```python
spectrogram = model.generate_speech(inputs["input_ids"], speaker_embeddings)
```

Il en r√©sulte un tenseur de forme (140, 80) contenant un log mel spectrogramme. La premi√®re dimension est la longueur de la s√©quence, et elle peut varier d'une ex√©cution √† l'autre car le pr√©-r√©seau d√©codeur de parole applique toujours un *dropout* √† la s√©quence d'entr√©e. Cela ajoute un peu de variabilit√© al√©atoire √† la parole g√©n√©r√©e.

Cependant, si nous cherchons √† g√©n√©rer une forme d'onde de la parole, nous devons sp√©cifier un vocodeur √† utiliser pour la conversion du spectrogramme en forme d'onde.
En th√©orie, vous pouvez utiliser n'importe quel vocodeur qui fonctionne sur des mel spectrogrammes 80-bin. De mani√®re pratique, ü§ó *Transformers* propose un vocodeur bas√© sur HiFi-GAN. Ses poids ont √©t√© aimablement fournis par les auteurs originaux de SpeechT5.

<Tip>

[HiFi-GAN](https://arxiv.org/pdf/2010.05646v2.pdf) est un r√©seau antagoniste g√©n√©ratif (GAN) de pointe con√ßu pour de la synth√®se vocale haute-fid√©lit√©. Il est capable de g√©n√©rer des formes d'ondes audio r√©alistes et de haute qualit√© √† partir de spectrogrammes.

√Ä un niveau √©lev√©, HiFi-GAN se compose d'un g√©n√©rateur et de deux discriminateurs. Le g√©n√©rateur est un r√©seau neuronal enti√®rement convolutionnel qui prend un mel spectrogramme en entr√©e et apprend √† produire des formes d'ondes audio brutes. Les discriminateurs ont pour r√¥le de faire la distinction entre l'audio r√©el et l'audio g√©n√©r√©. Les deux discriminateurs se concentrent sur des aspects diff√©rents de l'audio.

HiFi-GAN est entra√Æn√© sur un grand jeu de donn√©es d'enregistrements audio de haute qualit√©. Il utilise un entra√Ænement dit antagoniste, dans lequel les r√©seaux du g√©n√©rateur et du discriminateur sont en comp√©tition l'un contre l'autre. Au d√©part, le g√©n√©rateur produit un son de faible qualit√© et le discriminateur peut facilement le diff√©rencier du son r√©el. Au fur et √† mesure que l'entra√Ænement progresse, le g√©n√©rateur am√©liore sa sortie, dans le but de tromper le discriminateur. Le discriminateur, √† son tour, devient plus pr√©cis dans la distinction entre le son r√©el et le son g√©n√©r√©. Cette boucle de r√©troaction antagoniste permet aux deux r√©seaux de s'am√©liorer au fil du temps. En fin de compte, HiFi-GAN apprend √† g√©n√©rer un son de haute fid√©lit√© qui ressemble √©troitement aux caract√©ristiques des donn√©es entra√Æn√©es.

</Tip>

Le chargement du vocodeur est aussi simple que n'importe quel autre mod√®le de ü§ó *Transformers*.

```python
from transformers import SpeechT5HifiGan

vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
```

Il ne vous reste plus qu'√† le passer en argument lors de la g√©n√©ration de la parole, et les sorties seront automatiquement converties en forme d'onde de la parole.

```python
speech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)
```

√âcoutons le r√©sultat. La fr√©quence d'√©chantillonnage utilis√©e par SpeechT5 est toujours de 16 kHz.

```python
from IPython.display import Audio

Audio(speech, rate=16000)
```

Super ! 

N'h√©sitez pas √† jouer avec la d√©mo de SpeechT5 TSS, √† explorer d'autres voix, √† exp√©rimenter avec les entr√©es. Notez que ce *checkpoint* pr√©-entra√Æn√© ne prend en charge que la langue anglaise :

<iframe
	src="https://matthijs-speecht5-tts-demo.hf.space"
	frameborder="0"
	width="850"
	height="450">
</iframe>

## Massive Multilingual Speech (MMS)

Que faire si vous recherchez un mod√®le pr√©-entra√Æn√© dans une langue autre que l'anglais ? Massive Multilingual Speech (MMS) est un autre mod√®le qui couvre un large √©ventail de t√¢ches vocales, mais qui prend en charge un grand nombre de langues. Par exemple, il peut synth√©tiser la parole dans plus de 1 100 langues.

MMS pour la synth√®se vocale est bas√© sur [VITS Kim et al., 2021] (https://arxiv.org/pdf/2106.06103.pdf), qui est l'une des approches TTS les plus modernes.

VITS est un r√©seau de g√©n√©ration de parole qui convertit le texte en formes d'ondes vocales brutes. Il fonctionne comme un auto-encodeur variationnel conditionnel, estimant les caract√©ristiques audio √† partir du texte d'entr√©e. Tout d'abord, les caract√©ristiques acoustiques, repr√©sent√©es sous forme de spectrogrammes, sont g√©n√©r√©es. La forme d'onde est ensuite d√©cod√©e √† l'aide de couches convolutives transpos√©es adapt√©es de HiFi-GAN. 
Pendant l'inf√©rence, les encodages du texte sont sur√©chantillonn√©s et transform√©s en formes d'onde √† l'aide du module de flux et du d√©codeur HiFi-GAN.
Cela signifie que vous n'avez pas besoin d'ajouter un vocodeur pour l'inf√©rence, il est d√©j√† "int√©gr√©".  

Essayons le mod√®le et voyons comment nous pouvons synth√©tiser de la parole dans une langue autre que l'anglais, par exemple l'allemand.
Tout d'abord, nous allons charger le *checkpoint* du mod√®le et le *tokenizer* pour la bonne langue : 

```python
from transformers import VitsModel, VitsTokenizer

model = VitsModel.from_pretrained("Matthijs/mms-tts-deu")
tokenizer = VitsTokenizer.from_pretrained("Matthijs/mms-tts-deu")
```

Vous pouvez remarquer que pour charger le MMS, vous devez utiliser `VitsModel` et `VitsTokenizer`. C'est parce que le MMS pour la synth√®se vocale est bas√© sur le mod√®le VITS comme mentionn√© pr√©c√©demment. 

Prenons un exemple de texte en allemand, comme ces deux premi√®res lignes d'une chanson pour enfants : 

```python
text_example = (
    "Ich bin Schnappi das kleine Krokodil, komm aus √Ñgypten das liegt direkt am Nil."
)
```

Pour g√©n√©rer une forme d'onde, il faut pr√©traiter le texte √† l'aide du *tokenizer* et le transmettre au mod√®le : 

```python
import torch

inputs = tokenizer(text_example, return_tensors="pt")
input_ids = inputs["input_ids"]


with torch.no_grad():
    outputs = model(input_ids)

speech = outputs.audio[0]
```

√âcoutons :

```python
from IPython.display import Audio

Audio(speech, rate=16000)
```

Superbe ! Si vous souhaitez essayer MMS avec une autre langue, trouvez d'autres *checkpoints* `vits` appropri√©s sur le [*Hub*] (https://huggingface.co/models?filter=vits).

Voyons maintenant comment vous pouvez vous-m√™me *finetuner* un mod√®le TTS !