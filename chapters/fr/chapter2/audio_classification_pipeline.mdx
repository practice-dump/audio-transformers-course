# Classification audio avec un pipeline

La classification audio consiste √† attribuer une ou plusieurs √©tiquettes √† un enregistrement audio en fonction de son contenu. 
Les √©tiquettes peuvent correspondre √† diff√©rentes cat√©gories sonores, telles que la musique, la parole ou le bruit, ou √† des cat√©gories plus sp√©cifiques telles que le chant d‚Äôoiseaux ou les sons de moteur de voiture.
Avant de plonger dans les d√©tails du fonctionnement des *transformers* audio les plus populaires, et avant de finetuner un mod√®le personnalis√©, voyons comment vous pouvez utiliser un mod√®le pr√©-entra√Æn√© standard pour la classification audio avec seulement quelques lignes de code avec ü§ó *Transformers*.
Utilisons le m√™me jeu de donn√©es [MINDS-14](https://huggingface.co/datasets/PolyAI/minds14) que vous avez explor√© dans l'unit√© pr√©c√©dente. 
Si vous vous souvenez, MINDS-14 contient des enregistrements de personnes posant des questions √† un syst√®me bancaire √©lectronique dans plusieurs langues et dialectes, et a le `intent_class` pour chaque enregistrement. Nous pouvons classer les enregistrements par intention de l'appel.

Comme pr√©c√©demment, commen√ßons par charger le sous-ensemble `en-AU` pour essayer le pipeline, et sur√©chantillonnons-le √† un taux d'√©chantillonnage de 16 kHz, ce qui est ce que la plupart des mod√®les vocaux exigent.

```py
from datasets import load_dataset
from datasets import Audio

minds = load_dataset("PolyAI/minds14", name="en-AU", split="train")
minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

Pour classer un enregistrement audio dans un ensemble de classes, nous pouvons utiliser le pipeline `audio-classification` de ü§ó *Transformers*.
Dans notre cas, nous avons besoin d'un mod√®le qui a √©t√© finetun√© pour la classification des intentions, et en particulier sur le jeu de donn√©es MINDS-14. Heureusement pour nous, le *Hub* a un mod√®le qui fait exactement cela ! Chargeons-le en utilisant la fonction `pipeline()` :

```py
from transformers import pipeline

classifier = pipeline(
    "audio-classification",
    model="anton-l/xtreme_s_xlsr_300m_minds14",
)
```

Ce pipeline attend les donn√©es audio sous forme de tableau NumPy. Tout le pr√©traitement des donn√©es audio brutes sera commod√©ment g√©r√© pour nous par le pipeline. Choisissons un exemple pour l'essayer :

```py
example = minds[0]
```

Si vous vous souvenez de la structure du jeu de donn√©es, les donn√©es audio brutes sont stock√©es dans un tableau NumPy sous `["audio"]["array"]`, passons-les directement au `classifier` :

```py
classifier(example["audio"]["array"])
```

**Sortie :**

```out
[
    {"score": 0.9631525278091431, "label": "pay_bill"},
    {"score": 0.02819698303937912, "label": "freeze"},
    {"score": 0.0032787492964416742, "label": "card_issues"},
    {"score": 0.0019414445850998163, "label": "abroad"},
    {"score": 0.0008378693601116538, "label": "high_value_payment"},
]
```

Le mod√®le est tr√®s confiant que l'appelant avait l'intention d'apprendre √† payer sa facture. Voyons quelle est l'√©tiquette r√©elle pour cet exemple:

```py
id2label = minds.features["intent_class"].int2str
id2label(example["intent_class"])
```

**Sortie :**

```out
"pay_bill"
```

L'√©tiquette pr√©dite est correcte ! Ici, nous avons eu la chance de trouver un mod√®le capable de classer les √©tiquettes exactes dont nous avons besoin.
Souvent, lorsqu'il s'agit d'une t√¢che de classification, l'ensemble de classes d'un mod√®le pr√©-entra√Æn√© n'est pas exactement le m√™me que les classes que vous devez distinguer par le mod√®le. 
Dans ce cas, vous pouvez finetuner un mod√®le pr√©-entra√Æn√© pour le ¬´ calibrer ¬ª en fonction de votre ensemble exact d'√©tiquettes de classe. Nous apprendrons comment le faire dans les prochaines unit√©s. 
