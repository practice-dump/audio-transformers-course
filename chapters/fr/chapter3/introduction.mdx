# Unit√© 3 : Architectures de transformers pour l'audio

Dans ce cours, nous examinons principalement les *transformers* et comment ils peuvent √™tre appliqu√©s aux t√¢ches audio. Bien que vous n'ayez pas besoin de conna√Ætre les d√©tails internes de ces mod√®les, il est utile de comprendre les principaux concepts qui les font fonctionner. Nous faisons donc ici un rappel rapide. Pour une plong√©e profonde dans les *transformers*, consultez notre [cours de NLP](https://huggingface.co/learn/nlp-course/fr/chapter1/1).

## Comment fonctionne un transformer ?

Le *transformer* original a √©t√© con√ßu pour traduire du texte √©crit d'une langue √† une autre. Son architecture ressemble √† ceci :

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" alt="Original transformer architecture">
</div>

√Ä gauche se trouve l‚Äô**encodeur** et √† droite le **d√©codeur**.

- L'encodeur re√ßoit une entr√©e, dans ce cas une s√©quence de *tokens* de texte, et construit une repr√©sentation de celle-ci (ses caract√©ristiques). Cette partie du mod√®le est entra√Æn√©e pour acqu√©rir une compr√©hension √† partir de l'entr√©e.
- Le d√©codeur utilise la repr√©sentation de l'encodeur (les caract√©ristiques) ainsi que d'autres entr√©es (les *tokens* pr√©dits pr√©c√©demment) pour g√©n√©rer une s√©quence cible. Cette partie du mod√®le est entra√Æn√©e pour g√©n√©rer des extrants. Dans la conception originale, la s√©quence de sortie se composait de *tokens* de texte.
Il existe √©galement des mod√®les bas√©s sur des *transformers* n'utilisant que la partie encodeur (bon pour les t√¢ches qui n√©cessitent une compr√©hension de l'entr√©e, comme la classification), ou uniquement la partie d√©codeur (bon pour les t√¢ches telles que la g√©n√©ration de texte). Un exemple de mod√®le d'encodeur seul est BERT ; un exemple de mod√®le de d√©codeur seul est GPT2.
Une caract√©ristique cl√© des *transformers* est qu'ils sont construits avec des couches sp√©ciales appel√©es **couches d'attention**. Ces couches indiquent au mod√®le d'accorder une attention particuli√®re √† certains √©l√©ments de la s√©quence d'entr√©e et d'en ignorer d'autres lors du calcul des repr√©sentations d'entit√©s.

## Utilisation de *transformers* pour l'audio

Les mod√®les audio que nous aborderons dans ce cours ont g√©n√©ralement un *transformer* standard comme indiqu√© ci-dessus, mais avec une l√©g√®re modification du c√¥t√© de l'entr√©e ou de la sortie pour g√©rer des donn√©es audio au lieu de texte. Puisque tous ces mod√®les sont des *transformers* dans l'√¢me, ils ont pour la plupart une architecture commune et les principales diff√©rences sont dans la fa√ßon dont ils sont entra√Æn√©s et utilis√©s.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/transformers_blocks.png" alt="The transformer with audio input and output">
</div>

Pour les t√¢ches audio, les s√©quences d'entr√©e et/ou de sortie sont de l‚Äôaudio au lieu de texte :
- Reconnaissance automatique de la parole (ASR pour *Automatic speech recognition*) : l'entr√©e est la parole, la sortie est du texte.
- Synth√®se vocale (TTS pour *Text-to-speech*) : l'entr√©e est du texte, la sortie est de la parole.
- Classification audio : l'entr√©e est de l‚Äôaudio, la sortie est une probabilit√© de classe (une pour chaque √©l√©ment de la s√©quence ou une probabilit√© de classe unique pour la s√©quence enti√®re).
- Conversion vocale ou am√©lioration de la parole : l'entr√©e et la sortie sont de l‚Äôaudio.

Il existe diff√©rentes fa√ßons de g√©rer l'audio afin qu'il puisse √™tre utilis√© via un *transformer*. La principale consid√©ration est de savoir s'il faut utiliser l'audio dans sa forme brute (comme une forme d'onde) ou le traiter comme un spectrogramme √† la place.

## Entr√©es du mod√®le

L'entr√©e d'un mod√®le audio peut √™tre du texte ou du son. L'objectif est de convertir cette entr√©e en un ench√¢ssement pouvant √™tre trait√© par le *transformer*.

### Entr√©e textuelle

Un mod√®le de synth√®se vocale prend du texte comme entr√©e. Cela fonctionne comme le *transformer* d'origine en NLP : le texte d'entr√©e est d'abord tokenis√©, ce qui donne une s√©quence de *tokens* de texte. Cette s√©quence est envoy√©e via une couche d‚Äôench√¢ssement d'entr√©e pour convertir les *tokens* en vecteurs de 512 dimensions. Ces ench√¢ssements sont ensuite transmis dans l‚Äôencodeur du *transformer*.

### Entr√©e sous forme de forme d'onde

Un mod√®le de reconnaissance de la parole automatique prend l'audio comme entr√©e. Pour pouvoir utiliser un *transformer* pour l‚ÄôASR, nous devons d'abord convertir l'audio en une s√©quence d‚Äôench√¢ssements d'une mani√®re ou d'une autre.
Des mod√®les tels que **[Wav2Vec2](https://arxiv.org/abs/2006.11477)** et **[HuBERT](https://arxiv.org/abs/2106.07447)** utilisent la forme d'onde audio directement comme entr√©e du mod√®le. Comme vous l'avez vu dans [le chapitre sur les donn√©es audio (../chapter1/introduction), une forme d'onde est une s√©quence unidimensionnelle de nombres √† virgule flottante, o√π chaque nombre repr√©sente l'amplitude √©chantillonn√©e √† un moment donn√©. Cette forme d'onde brute est d'abord normalis√©e √† la moyenne nulle et √† la variance unitaire, ce qui permet de normaliser les √©chantillons audio sur diff√©rents volumes (amplitudes).

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/wav2vec2-input.png" alt="Wav2Vec2 uses a CNN to create embeddings from the input waveform">
</div>

Apr√®s la normalisation, la s√©quence d'√©chantillons audio est transform√©e en un ench√¢ssement √† l'aide d'un petit r√©seau neuronal convolutionnel, connu sous le nom d'encodeur de caract√©ristiques. Chacune des couches convolutives de ce r√©seau traite la s√©quence d'entr√©e, sous-√©chantillonnant l'audio pour r√©duire la longueur de la s√©quence, jusqu'√† ce que la couche convolutive finale produise un vecteur √† 512 dimensions avec l‚Äôench√¢ssement pour chaque 25 ms d'audio. Une fois que la s√©quence d'entr√©e a √©t√© transform√©e en une s√©quence de tels ench√¢ssements, le *transformer* traitera les donn√©es comme d'habitude.

### Entr√©e sous forme de spectrogramme

Un inconv√©nient de l'utilisation de la forme d'onde brute comme entr√©e est qu'elles ont tendance √† avoir de longues longueurs de s√©quence. Par exemple, trente secondes d'audio √† une fr√©quence d'√©chantillonnage de 16 kHz donnent une entr√©e de longueur `30 * 16000 = 480000`. Des longueurs de s√©quence plus longues n√©cessitent plus de calculs dans le *transformer* et donc une utilisation plus √©lev√©e de la m√©moire.
Pour cette raison, les formes d'onde audio brutes ne sont g√©n√©ralement pas la forme la plus efficace de repr√©senter une entr√©e audio. En utilisant un spectrogramme, nous obtenons la m√™me quantit√© d'informations, mais sous une forme plus comprim√©e.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/whisper-input.png" alt="Whisper uses a CNN to create embeddings from the input spectrogram">
</div>

Des mod√®les tels que **[Whisper](https://arxiv.org/abs/2212.04356)** convertissent d'abord la forme d'onde en un spectrogramme log-mel. Whisper divise toujours l'audio en segments de 30 secondes, et le spectrogramme log-mel pour chaque segment a la forme ` (80, 3000) ` o√π 80 est le nombre de bacs mel et 3000 est la longueur de la s√©quence. En convertissant en spectrogramme log-mel, nous avons r√©duit la quantit√© de donn√©es d'entr√©e, mais plus important encore, il s'agit d'une s√©quence beaucoup plus courte que la forme d'onde brute. Le spectrogramme log-mel est ensuite trait√© par un petit r√©seau convolutif en une s√©quence d'ench√¢ssements, allant ensuite dans le *transformer* comme d'habitude.

Dans les deux cas, l'entr√©e de la forme d'onde et du spectrogramme, il y a un petit r√©seau devant le *transformer* convertissant l'entr√©e en ench√¢ssement, puis le *transformer*prend le relais pour faire son travail.

## Sorties du mod√®le

L'architecture du *transformer* g√©n√®re une s√©quence de vecteurs √† √©tats cach√©s, √©galement appel√©s ench√¢ssement de sortie. Notre objectif est de transformer ces vecteurs en sortie textuelle ou audio.

### Sortie textuelle

L'objectif d'un mod√®le d‚ÄôASR est de pr√©dire une s√©quence de *tokens* de texte. Cela se fait en ajoutant une t√™te de mod√©lisation du langage (g√©n√©ralement une seule couche lin√©aire) suivie d'une softmax au-dessus de la sortie du *transformer*. On alors pr√©dit les probabilit√©s sur les *tokens* de texte dans le vocabulaire.

### Sortie sous forme de spectrogramme

Pour les mod√®les qui g√©n√®rent de l'audio, tels qu'un mod√®le de synth√®se vocale, nous devons ajouter des couches pouvant produire une s√©quence audio. Il est tr√®s courant de g√©n√©rer un spectrogramme, puis d'utiliser un r√©seau neuronal suppl√©mentaire, connu sous le nom de vocodeur, pour transformer ce spectrogramme en une forme d'onde.
Par exemple dans le mod√®le **[SpeechT5](https://arxiv.org/abs/2110.07205)** la sortie du *transformer* est une s√©quence de vecteurs √† 768 √©l√©ments. Une couche lin√©aire projette cette s√©quence dans un spectrogramme log-mel. Un post-r√©seau, compos√© de couches lin√©aires et convolutives suppl√©mentaires, affine le spectrogramme en r√©duisant le bruit. Le vocodeur cr√©e alors la forme d'onde audio finale.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/speecht5.png" alt="SpeechT5 outputs a spectrogram and uses a vocoder to create the waveform">
</div>

<Tip>
üí° Si vous prenez une forme d'onde existante et appliquez la transform√©e de Fourier √† court terme (TFCT), il est possible d'effectuer l'op√©ration inverse pour obtenir √† nouveau la forme d'onde d'origine. Cela fonctionne parce que le spectrogramme cr√©√© par la TFCT contient √† la fois des informations d'amplitude et de phase, et les deux sont n√©cessaires pour reconstruire la forme d'onde. Cependant, les mod√®les audio qui g√©n√®rent leur sortie sous forme de spectrogramme ne pr√©disent g√©n√©ralement que les informations d'amplitude, pas la phase. Pour transformer un tel spectrogramme en une forme d'onde, nous devons en quelque sorte estimer l'information de phase. C'est ce que fait un vocodeur.
</Tip>

### Sortie sous forme de forme d'onde

Il est √©galement possible pour les mod√®les de produire directement une forme d'onde au lieu d'un spectrogramme comme √©tape interm√©diaire, mais nous n'avons actuellement aucun mod√®le dans ü§ó *Transformers* qui le fait.

## Conclusion

En r√©sum√©: la plupart des *transformers* audio se ressemblent plus que diff√©rents. Ils sont tous construits sur la m√™me architecture et les m√™mes couches d'attention, bien que certains mod√®les n'utilisent que la partie encodeur du *transformer* tandis que d'autres utilisent √† la fois l‚Äôencodeur et le d√©codeur.
Vous avez √©galement vu comment obtenir des donn√©es audio dans et hors des *transformers*. Pour effectuer les diff√©rentes t√¢ches audio d'ASR, TTS, etc., nous pouvons simplement √©changer les couches qui pr√©traitent les entr√©es en ench√¢ssements, et √©changer les couches qui post-traitent les ench√¢ssements pr√©dites en sorties, tandis que le *backbone* du *transformer* reste le m√™me.
Examinons  dans les suite diff√©rentes fa√ßons dont ces mod√®les peuvent √™tre entra√Æn√©s √† effectuer une reconnaissance vocale automatique.
