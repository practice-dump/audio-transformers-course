# Architectures Seq2Seq

Les mod√®les avec CTC abord√©s dans la section pr√©c√©dente utilisent uniquement la partie encodeur du *transformer*. Lorsque nous ajoutons √©galement le d√©codeur pour cr√©er un mod√®le encodeur-d√©codeur, on parle alors de mod√®le **s√©quence √† s√©quence** ou seq2seq en abr√©g√©. Le mod√®le essocie une s√©quence d'un type de donn√©es √† une s√©quence d'un autre type de donn√©es.
Avec les *transformers* encodeur, l‚Äôencodeur fait une pr√©diction pour chaque √©l√©ment de la s√©quence d'entr√©e. Par cons√©quent, les s√©quences d'entr√©e et de sortie auront toujours la m√™me longueur. Dans le cas de mod√®les avec CTC tels que Wav2Vec2, la forme d'onde d'entr√©e est d'abord sous-√©chantillonn√©e, mais il y a toujours une pr√©diction pour chaque 20 ms d'audio.
Avec un mod√®le seq2seq, il n'y a pas une telle correspondance un-√†-un et les s√©quences d'entr√©e et de sortie peuvent avoir des longueurs diff√©rentes. Cela rend les mod√®les seq2seq adapt√©s aux t√¢ches de NLP telles que le r√©sum√© de texte ou la traduction entre diff√©rentes langues, mais aussi aux t√¢ches audio telles que la reconnaissance vocale.
L'architecture d'un d√©codeur est tr√®s similaire √† celle d'un encodeur, et les deux utilisent des couches similaires avec l'auto-attention comme caract√©ristique principale. Toutefois, le d√©codeur effectue une t√¢che diff√©rente de celle de l'encodeur. Pour voir comment cela fonctionne, examinons comment un mod√®le seq2seq peut effectuer une reconnaissance automatique de la parole.

## Reconnaissance automatique de la parole

L'architecture de **Whisper** est la suivante (figure provenant du blog d‚Äô[OpenAI](https://openai.com/blog/whisper/)) :

<div class="flex justify-center">
    <img src="https://huggingface.co/blog/assets/111_fine_tune_whisper/whisper_architecture.svg" alt="Whisper is a transformer encoder-decoder model">
</div>

Cela devrait vous sembler assez familier. Sur la gauche se trouve l‚Äô**encodeur du *transformer***. Il prend comme entr√©e un spectrogramme log-mel et encode ce spectrogramme pour former une s√©quence d'√©tats cach√©s de l‚Äôencodeur qui extraient des caract√©ristiques importantes de la parole. Ce tenseur √† √©tats cach√©s repr√©sente la s√©quence d'entr√©e dans son ensemble et code efficacement le ¬´ sens ¬ª du discours d'entr√©e.

<Tip>
üí° Il est courant que ces mod√®les seq2seq utilisent des spectrogrammes comme entr√©e. Cependant, un mod√®le seq2seq peut √©galement √™tre con√ßu pour fonctionner directement sur les formes d'onde audio.
</Tip>

La sortie de l‚Äôencodeur est ensuite pass√©e dans le **d√©codeur du *transformer***, illustr√© √† droite, √† l'aide d'un m√©canisme d‚Äô**attention crois√©e**. C'est comme l'auto-attention, mais assiste la sortie de l'encodeur. √Ä partir de ce moment, l'encodeur n'est plus n√©cessaire.
Le d√©codeur pr√©dit une s√©quence de *tokens* de texte de mani√®re **autor√©gressive**, un seul *token* √† la fois, √† partir d'une s√©quence initiale qui contient juste un *token* ¬´ start ¬ª (`SOT` dans le cas de Whisper). √Ä chaque pas de temps suivant, la s√©quence de sortie pr√©c√©dente est r√©introduite dans le d√©codeur en tant que nouvelle s√©quence d'entr√©e. De cette mani√®re, le d√©codeur √©met un nouveau *token* √† la fois, augmentant r√©guli√®rement la s√©quence de sortie, jusqu'√† ce qu'il pr√©dise qu'un *token* de fin ou un nombre maximum de pas de temps est atteint.
Alors que l'architecture du d√©codeur est en grande partie identique √† celle de l'encodeur, il existe deux grandes diff√©rences:
1. Le d√©codeur a un m√©canisme d'attention crois√©e qui lui permet de regarder la repr√©sentation de l'encodeur de la s√©quence d'entr√©e
2. L'attention du d√©codeur est causale : le d√©codeur n'est pas autoris√© √† regarder vers l'avenir.
Dans cette conception, le d√©codeur joue le r√¥le d'un **mod√®le de langage**, traitant les repr√©sentations √† l'√©tat cach√© de l'encodeur et g√©n√©rant les transcriptions de texte correspondantes. Il s'agit d'une approche plus puissante que la CTC (m√™me si la CTC est combin√©e avec un mod√®le de langage externe), car le syst√®me seq2seq peut √™tre entra√Æn√© de bout en bout avec les m√™mes donn√©es d'apprentissage et la m√™me fonction de perte, offrant une plus grande flexibilit√© et des performances g√©n√©ralement sup√©rieures.

<Tip>
üí° Alors qu'un mod√®le avec CTC produit une s√©quence de caract√®res individuels, les *tokens* pr√©dits par Whisper sont des mots complets ou des portions de mots. Il utilise le tokenizer de GPT-2 et dispose d‚Äôenviron 50 000 *tokens* uniques. Un mod√®le seq2seq peut donc produire une s√©quence beaucoup plus courte qu'un mod√®le CTC pour la m√™me transcription.
</Tip >

Une fonction de perte typique pour un mod√®le d‚ÄôASR seq2seq est la perte d'entropie crois√©e car la derni√®re couche du mod√®le pr√©dit une distribution de probabilit√© sur les *tokens* possibles. Ceci est g√©n√©ralement combin√© avec des techniques telles que [recherche en faisceau pour g√©n√©rer la s√©quence finale](https://huggingface.co/blog/how-to-generate). La m√©trique de la reconnaissance vocale est le WER (*word error rate*) ou taux d'erreur de mots, qui mesure le nombre de substitutions, d'insertions et de suppressions n√©cessaires pour transformer le texte pr√©dit en texte cible. Moins il y en a, meilleur est le score.

## Synth√®se vocale

Cela ne vous surprendra peut-√™tre pas : un mod√®le seq2seq pour la synth√®se vocale fonctionne essentiellement de la m√™me mani√®re que d√©crit ci-dessus, mais avec les entr√©es et les sorties invers√©es ! L'encodeur du *transformer* prend une s√©quence de *tokens* de texte et extrait une s√©quence d'√©tats masqu√©s qui repr√©sentent le texte d'entr√©e. Le d√©codeur du *transformer* applique une attention crois√©e √† la sortie de l‚Äôencodeur et pr√©dit un spectrogramme.

<Tip>
üí° Rappelons qu'un spectrogramme est fabriqu√© en prenant le spectre de fr√©quences de tranches de temps successives d'une forme d'onde audio et en les empilant ensemble. En d'autres termes, un spectrogramme est une s√©quence o√π les √©l√©ments sont des spectres de fr√©quence (log-mel), un pour chaque pas de temps.
</Tip>

Avec le mod√®le d‚ÄôASR, le d√©codeur a √©t√© d√©marr√© √† l'aide d'une s√©quence qui contient simplement le *token* sp√©cial ¬´ start ¬ª. Pour le mod√®le de synth√®se vocale, on peut commencer le d√©codage avec un spectrogramme de longueur 1 et rempli de 0 qui agit comme le ¬´ *token* de d√©part ¬ª. Compte tenu de ce spectrogramme initial et des attentions crois√©es sur les repr√©sentations √† l'√©tat cach√© de l‚Äôencodeur, le d√©codeur pr√©dit ensuite la prochaine tranche de temps pour ce spectrogramme, augmentant r√©guli√®rement le spectrogramme un pas de temps √† la fois.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/speecht5_decoding.png" alt="The audio waveform gets mapped to a shorter sequence of hidden-states">
</div>

Mais comment le d√©codeur sait-il quand s'arr√™ter ? Dans le mod√®le **SpeechT5**, cela est g√©r√© en faisant pr√©dire au d√©codeur une deuxi√®me s√©quence. Il contient la probabilit√© que le pas de temps actuel soit le dernier. Lors de la g√©n√©ration audio au moment de l'inf√©rence, si cette probabilit√© d√©passe un certain seuil (disons 0,5), le d√©codeur indique que le spectrogramme est termin√© et que la boucle de g√©n√©ration doit se terminer.
Une fois le d√©codage termin√© et que nous avons une s√©quence de sortie contenant le spectrogramme, SpeechT5 utilise un **post-net** qui est compos√© de plusieurs couches de convolution pour affiner le spectrogramme.
Lors de l'entra√Ænement du mod√®le de synth√®se vocale, les cibles sont aussi des spectrogrammes et la perte est la L1 ou la MSE. Au moment de l'inf√©rence, nous voulons convertir le spectrogramme de sortie en une forme d'onde audio afin que nous puissions r√©ellement l'√©couter. Pour cela, un mod√®le externe est utilis√©, le **vocodeur**. Ce vocodeur ne fait pas partie de l'architecture seq2seq et est entra√Æn√© s√©par√©ment.
Ce qui rend la synth√®se vocale difficile, est qu'il s'agit d'une association un-√†-plusieurs. Avec la reconnaissance de la parole, il n'y a qu'un seul texte de sortie correct correspondant au discours d'entr√©e, mais avec la synth√®se vocale, le texte d'entr√©e peut √™tre associ√© √† de nombreux sons possibles. Diff√©rents orateurs peuvent choisir de mettre l'accent sur diff√©rentes parties de la phrase, par exemple. Cela rend les mod√®les de synth√®se vocale difficiles √† √©valuer. Pour cette raison, la valeur de perte L1 ou MSE n'est pas vraiment significative : il existe plusieurs fa√ßons de repr√©senter le m√™me texte dans un spectrogramme. C'est pourquoi les mod√®les TTS sont g√©n√©ralement √©valu√©s par des auditeurs humains, en utilisant une m√©trique connue sous le nom de MOS (*mean opinion score*) ou score d'opinion moyen.

## Conclusion

L'approche seq2seq est plus puissante qu'un mod√®le d'encodeur. En s√©parant l'encodage de la s√©quence d'entr√©e du d√©codage de la s√©quence de sortie, l'alignement de l'audio et du texte est moins probl√©matique.  Le mod√®le apprend √† effectuer cet alignement gr√¢ce au m√©canisme d'attention.
Cependant, un mod√®le encodeur-d√©codeur est √©galement plus lent car le processus de d√©codage se produit une √©tape √† la fois, plut√¥t que tout √† la fois. Plus la s√©quence est longue, plus la pr√©diction est lente. Les mod√®les autor√©gressifs peuvent √©galement rester bloqu√©s dans des r√©p√©titions ou sauter des mots. Des techniques telles que la recherche en faisceau peuvent aider √† am√©liorer la qualit√© des pr√©dictions, mais aussi ralentir encore plus le d√©codage.
