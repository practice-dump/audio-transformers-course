# Architectures avec CTC

CTC ou classification temporelle connexionniste est une technique utilis√©e avec les *transformers* encodeur pour la reconnaissance automatique de la parole. Des exemples de tels mod√®les sont **Wav2Vec2**, **HuBERT** et **M-CTC-T**.
Un *transformer* encodeur est le type de *transformer* le plus simple car il utilise uniquement la partie encodeur du mod√®le. L'encodeur lit la s√©quence d'entr√©e (la forme d'onde audio) et l‚Äôassocie dans une s√©quence d'√©tats cach√©s, √©galement appel√©e ench√¢ssement de sortie.
Avec un mod√®le avec CTC, nous appliquons un association lin√©aire suppl√©mentaire sur la s√©quence des √©tats cach√©s pour obtenir des pr√©dictions d'√©tiquettes de classe. Les √©tiquettes de classe sont les **caract√®res de l'alphabet** (a, b, c, ...). De cette fa√ßon, nous sommes en mesure de pr√©dire n'importe quel mot dans la langue cible avec une petite t√™te de classification, car le vocabulaire ne contient que 26 caract√®res plus quelques *tokens* sp√©ciaux.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/wav2vec2-ctc.png" alt="Transformer encoder with a CTC head on top">
</div>

Jusqu'√† pr√©sent, cela est tr√®s similaire √† ce que nous faisons en NLP avec un mod√®le tel que BERT : un * transformer* encodeur associe nos *tokens* de texte dans une s√©quence d'√©tats cach√©s de l'encodeur, puis nous appliquons une association lin√©aire pour obtenir une pr√©diction d'√©tiquette de classe pour chaque √©tat cach√©.
Voici le hic : dans la parole, nous ne connaissons pas l'alignement des entr√©es audio et des sorties de texte. Nous savons que l'ordre dans lequel le discours est prononc√© est le m√™me que l'ordre dans lequel le texte est transcrit (l'alignement est dit monotone), mais nous ne savons pas comment les caract√®res de la transcription s'alignent sur l'audio. C'est l√† qu'intervient l'algorithme CTC.

<Tip>
üí° Dans les mod√®les de NLP, le vocabulaire est g√©n√©ralement compos√© de milliers de *tokens* qui d√©crivent non seulement des caract√®res individuels, mais des parties de mots ou m√™me des mots complets. Pour la CTC, un petit vocabulaire fonctionne mieux et nous essayons g√©n√©ralement de le limiter √† moins de 50 caract√®res. Nous ne nous soucions pas de la casse des lettres, donc seulement utiliser des majuscules (ou seulement des minuscules) est suffisant. Les chiffres sont √©pel√©s, par exemple ¬´ 20 ¬ª devient ¬´ vingt ¬ª. En plus des lettres, nous avons besoin d'au moins un *token* s√©parateur de mots (espace) et d'un *token* de rembourrage. Tout comme avec un mod√®le de NLP, le *token* de remplissage nous permet de combiner plusieurs exemples dans un batch, mais c'est aussi le *token* que le mod√®le pr√©dira pour les silences. En anglais, il est √©galement utile de garder le caract√®re `'` car `"it's"` et `"its"`ont des significations tr√®s diff√©rentes.
</Tip>

## O√π est mon alignement?

L‚ÄôASR consiste √† prendre l'audio en entr√©e et √† produire du texte en sortie. Nous avons quelques choix pour pr√©dire le texte:
- comme caract√®res
- comme phon√®mes
- comme mots

Un mod√®le d‚ÄôASR est entra√Æn√© sur un ensemble de donn√©es compos√© de paires `(audio, texte)` o√π le texte est une transcription humaine du fichier audio. En r√®gle g√©n√©rale, le jeu de donn√©es n'inclut aucune information de synchronisation indiquant quel mot ou syllabe appara√Æt o√π dans le fichier audio. Comme nous ne pouvons pas compter sur les informations de synchronisation pendant l'entra√Ænement, nous n'avons aucune id√©e de la fa√ßon dont les s√©quences d'entr√©e et de sortie doivent √™tre align√©es.
Supposons que notre entr√©e soit un fichier audio d'une seconde. Dans **Wav2Vec2**, le mod√®le sous-√©chantillonne l'entr√©e audio √† l'aide de l‚Äôencodeur ConvNet pour une s√©quence plus courte d'√©tats cach√©s, o√π il y a un vecteur d'√©tat cach√© pour chaque 20 millisecondes d'audio. Pour une seconde d'audio, nous transmettons ensuite une s√©quence de 50 √©tats cach√©s √† l‚Äôencodeur du *transformer*.  Les segments audio extraits de la s√©quence d'entr√©e se chevauchent partiellement, de sorte que m√™me si un vecteur √† √©tat cach√© est √©mis toutes les 20 ms, chaque √©tat cach√© repr√©sente en fait 25 ms d'audio.
L‚Äôencodeur du *transformer* pr√©dit une repr√©sentation des caract√©ristiques pour chacun de ces √©tats cach√©s, ce qui signifie que nous recevons du *transformer* une s√©quence de 50 sorties. Chacune de ces sorties a une dimensionnalit√© de 768. Dans cet exemple, la s√©quence de sortie de l‚Äôencodeur du transformer a donc la forme `(768, 50)`. Comme chacune de ces pr√©dictions couvre 25 ms de temps, ce qui est plus court que la dur√©e d'un phon√®me, il est logique de pr√©dire des phon√®mes ou des caract√®res individuels, mais pas des mots entiers. La CTC fonctionne mieux avec un petit vocabulaire, nous allons donc pr√©dire les caract√®res.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/cnn-feature-encoder.png" alt="The audio waveform gets mapped to a shorter sequence of hidden-states">
</div>

Pour faire des pr√©dictions de texte, nous associons chacune des sorties d'encodeur √† 768 dimensions √† nos √©tiquettes de caract√®res √† l'aide d'une couche lin√©aire (la ¬´ t√™te CTC ¬ª). Le mod√®le pr√©dit alors un tenseur `(50, 32)` contenant les logits, o√π 32 est le nombre de *tokens* dans le vocabulaire. Puisque nous faisons une pr√©diction pour chacune des caract√©ristiques de la s√©quence, nous nous retrouvons avec un total de pr√©dictions de 50 caract√®res pour chaque seconde d'audio.
Cependant, si nous pr√©disons simplement un caract√®re toutes les 20 ms, notre s√©quence de sortie pourrait ressembler √† ceci:

```text
BRIIONSAWWSOMEETHINGCLOSETOPANICONHHISOPPONENT'SSFAACEWHENTHEMANNFINALLLYRREECOGGNNIIZEDHHISSERRRRORR ...
``` 

Si vous regardez de plus pr√®s, cela ressemble un peu √† de l'anglais, mais beaucoup de caract√®res ont √©t√© dupliqu√©s. C'est parce que le mod√®le doit sortir *quelque chose* pour chaque 20 ms d'audio dans la s√©quence d'entr√©e, et si un caract√®re est √©tal√© sur une p√©riode sup√©rieure √† 20 ms, il appara√Ætra plusieurs fois dans la sortie. Il n'y a aucun moyen d'√©viter cela, d'autant plus que nous ne savons pas quel est l‚Äôhorodatage de la transcription pendant l‚Äôentra√Ænement. La CTC est un moyen de filtrer ces doublons.
En r√©alit√©, la s√©quence pr√©dite contient √©galement beaucoup de *tokens* de remplissage lorsque le mod√®le n'est pas tout √† fait s√ªr de ce que le son repr√©sente, ou pour l'espace vide entre les caract√®res. Nous avons supprim√© ces *tokens* de remplissage de l'exemple pour plus de clart√©. Le chevauchement partiel entre les segments audio est une autre raison pour laquelle les caract√®res sont dupliqu√©s dans la sortie.)

## L'algorithme CTC
La cl√© de l'algorithme CTC est l'utilisation d'un *token* sp√©cial, souvent appel√© ***token* blanc**. C'est juste un autre *token* que le mod√®le pr√©dira et cela fait partie du vocabulaire. Dans cet exemple, le *token* blanc est affich√© sous la forme `_`. Ce *token* sp√©cial sert de d√©limitation entre les groupes de caract√®res.
Le r√©sultat complet du mod√®le CTC pourrait ressembler √† ce qui suit :

```text
B_R_II_O_N_||_S_AWW_|||||_S_OMEE_TH_ING_||_C_L_O_S_E||TO|_P_A_N_I_C_||_ON||HHI_S||_OP_P_O_N_EN_T_'SS||_F_AA_C_E||_W_H_EN||THE||M_A_NN_||||_F_I_N_AL_LL_Y||||_RREE_C_O_GG_NN_II_Z_ED|||HHISS|||_ER_RRR_ORR||||
```

Le *token* `|` est le caract√®re s√©parateur de mots. Dans l'exemple, nous utilisons `|` au lieu d'un espace, ce qui permet de rep√©rer plus facilement o√π se trouvent les sauts de mots, mais cela sert le m√™me but.
Le caract√®re blanc de la CTC permet de filtrer les caract√®res en double. Par exemple, regardons le dernier mot de la s√©quence pr√©dite, ¬´ _ER_RRR_ORR ¬ª. Sans le *token* blanc, le mot ressemblait √† ceci:

```text
ERRRRORR
``` 

Si nous supprimions simplement les caract√®res en double, cela deviendrait ¬´ EROR ¬ª. Ce n'est pas l'orthographe correcte. Mais avec le *token* *blanc* d ela CTC, nous pouvons supprimer les doublons dans chaque groupe, de sorte que:

```text
_ER_RRR_ORR
``` 

devient:

```text
_ER_R_OR
``` 

Et maintenant, nous supprimons le jeton blanc `_` pour avoir le mot final :

```text
ERROR
``` 

Si nous appliquons cette logique √† l'ensemble du texte, y compris `|`, et rempla√ßons les caract√®res `|` survivants par des espaces, la sortie finale d√©cod√©e par CTC est la suivante :

```text
BRION SAW SOMETHING CLOSE TO PANIC ON HIS OPPONENT'S FACE WHEN THE MAN FINALLY RECOGNIZED HIS ERROR
``` 

Pour r√©capituler, le mod√®le pr√©dit un *token* (caract√®re) pour chaque 20 ms d'audio (partiellement chevauchant) √† partir de la forme d'onde d'entr√©e. Cela donne beaucoup de doublons. Gr√¢ce au *token* blanc de la CTC, nous pouvons facilement supprimer ces doublons sans d√©truire la bonne l'orthographe des mots. C'est un moyen tr√®s simple et pratique de r√©soudre le probl√®me de l'alignement du texte de sortie avec l'audio d'entr√©e.

<Tip>
üí° Dans le mod√®le Wav2Vec2, le *token* blanc est le m√™me que le *token* de remplissage `<pad>`. Le mod√®le pr√©dira beaucoup de ces *tokens* `<pad>`, par exemple lorsqu'il n'y a pas de caract√®re clair √† pr√©dire pour les 20 ms actuelles d'audio. L'utilisation du m√™me *token* pour le remplissage que pour les blancs simplifie l'algorithme de d√©codage et aide √† garder le vocabulaire petit.
</Tip>

L'ajout de la CTC √† un *transformer* encodeur est facile : la s√©quence de sortie de l‚Äôencodeur va dans une couche lin√©aire qui projette les caract√©ristiques acoustiques dans le vocabulaire. Le mod√®le est entra√Æn√© avec une perte de CTC sp√©ciale.
Un inconv√©nient de la CTC est qu'elle peut produire des mots qui *sonnent* corrects mais ne sont pas *orthographi√©s* correctement. Apr√®s tout, la t√™te de la CTC ne prend en compte que les caract√®res individuels, pas les mots complets. Une fa√ßon d'am√©liorer la qualit√© des transcriptions audio est d'utiliser un mod√®le de langage externe. Ce mod√®le de langage agit essentiellement comme un correcteur orthographique au-dessus de la sortie de la CTC.

## Quelle est la diff√©rence entre Wav2Vec2, HuBERT, M-CTC-T, etc. ?

Tous les mod√®les de *transformer* avec CTC ont une architecture tr√®s similaire. Ils utilisent l‚Äôencodeur du *transformer* (mais pas le d√©codeur) avec une t√™te CTC sur le dessus. Du point de vue de l'architecture, ils se ressemblent plus que ne sont diff√©rents.
Une diff√©rence entre Wav2Vec2 et M-CTC-T est que le premier fonctionne sur des formes d'onde audio brutes tandis que le second utilise des spectrogrammes mel comme entr√©e. Les mod√®les ont √©galement √©t√© entra√Æn√©s √† des fins diff√©rentes. M-CTC-T, par exemple, est entra√Æn√© √† la reconnaissance vocale multilingue et poss√®de donc une t√™te CTC relativement grande qui comprend des caract√®res chinois en plus d'autres alphabets.
Wav2Vec2 & HuBERT utilisent exactement la m√™me architecture mais sont entra√Æn√©s de mani√®re tr√®s diff√©rente. Wav2Vec2 est pr√©-entra√Æn√© comme la mod√©lisation du langage masqu√© de BERT, en pr√©disant les unit√©s vocales pour les parties masqu√©es de l'audio. HuBERT pousse l'inspiration de BERT un peu plus loin et apprend √† pr√©dire les ¬´ unit√©s de parole discr√®tes ¬ª, qui sont analogues aux *tokens* dans une phrase de texte, de sorte que la parole peut √™tre trait√©e en utilisant des techniques de NLP √©tablies.
Pour clarifier, les mod√®les mis en √©vidence ici ne sont pas les seuls mod√®les de *transformer* avec CTC. Il y en a beaucoup d'autres, mais maintenant vous savez qu'ils fonctionnent tous de la m√™me mani√®re.
