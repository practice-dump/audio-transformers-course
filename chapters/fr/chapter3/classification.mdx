# Architectures de classification d‚Äôaudio

L'objectif de la classification d‚Äôaudio est de pr√©dire une √©tiquette de classe pour une entr√©e audio. Le mod√®le peut pr√©dire une √©tiquette de classe unique qui couvre toute la s√©quence d'entr√©e, ou il peut pr√©dire une √©tiquette pour chaque trame audio (g√©n√©ralement toutes les 20 millisecondes d'entr√©e audio), auquel cas la sortie du mod√®le est une s√©quence de probabilit√©s d'√©tiquette de classe. Un exemple du premier cas est la d√©tection des chants des oiseaux. Un exemple du second est la s√©paration des locuteurs o√π le mod√®le pr√©dit quel locuteur parle √† un moment donn√©.

## Classification √† l'aide de spectrogrammes

L'un des moyens les plus simples d'effectuer une classification audio est de pr√©tendre qu'il s'agit d'un probl√®me de classification d'image !
Rappelons qu'un spectrogramme est un tenseur bidimensionnel de forme `(frequencies, sequence length)`. Dans le [chapitre sur les donn√©es audio](.. /chapter1/audio_data) nous avons trac√© ces spectrogrammes sous forme d'images. Devinez quoi ? Nous pouvons litt√©ralement traiter le spectrogramme comme une image et le transmettre √† un mod√®le de classification de type ConvNet standard tel qu‚Äôun ResNet ou ConvNext et obtenir de tr√®s bonnes pr√©dictions. M√™me chose avec un mod√®le de type *transformer* tel que le ViT.
C'est ce que fait **Audio Spectrogram Transformer**. Il utilise le ViT et lui transmet des spectrogrammes en entr√©e au lieu d'images. Gr√¢ce aux couches d'auto-attention du *transformer*, le mod√®le est mieux en mesure de capturer le contexte global qu'un ConvNet.
Tout comme le ViT, le mod√®le AST divise le spectrogramme audio en une s√©quence de patchs d'image partiellement chevauchants de 16√ó16 pixels. Cette s√©quence de patchs est ensuite projet√©e dans une s√©quence d‚Äôench√¢ssement, et ceux-ci sont donn√©s √† l‚Äôencodeur du *transformer* en entr√©e comme d'habitude. L‚ÄôAST est un mod√®le de *transformer* encodeur et la sortie est donc une s√©quence d'√©tats cach√©s, un pour chaque patch d'entr√©e 16√ó16. En plus de cela, il y a une couche de classification simple avec activation sigmo√Øde pour associer les √©tats cach√©s aux probabilit√©s de classification.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/ast.png" alt="The audio spectrogram transformer works on a sequence of patches taken from the spectrogram">
</div>

Image tir√©e du papier [*AST: Audio Spectrogram Transformer*](https://arxiv.org/pdf/2104.01778.pdf)

<Tip>
üí° M√™me si ici nous pr√©tendons que les spectrogrammes sont comme les images, il existe des diff√©rences importantes. Par exemple, d√©placer le contenu d'une image vers le haut ou vers le bas ne change g√©n√©ralement pas la signification de ce qui se trouve dans l'image. Cependant, d√©placer un spectrogramme vers le haut ou vers le bas changera les fr√©quences qui sont dans le son et changera compl√®tement son caract√®re. Les images sont invariantes sous translation mais les spectrogrammes ne le sont pas. Traiter les spectrogrammes comme des images peut tr√®s bien fonctionner dans la pratique, mais gardez √† l'esprit que ce n'est pas vraiment la m√™me chose.
</Tip>

## Tout transformer peut √™tre un classifieur

Dans une [section pr√©c√©dente](CTC), vous avez vu que CTC est une technique efficace pour effectuer une reconnaissance automatique de la parole √† l'aide d'un *transformer* encodeur.  De tels mod√®les sont d√©j√† des classifieurs, pr√©disant les probabilit√©s pour les √©tiquettes de classe √† partir d'un vocabulaire de *tokenizer*. Nous pouvons prendre un mod√®le avec CTC et le transformer en un classifieur d‚Äôaudio √† usage g√©n√©ral en changeant les √©tiquettes et en l'entra√Ænant avec une fonction de perte d'entropie crois√©e standard au lieu de la perte CTC sp√©ciale.
Par exemple, ü§ó *Transformers* a un mod√®le `Wav2Vec2ForCTC` mais aussi `Wav2Vec2ForSequenceClassification` et `Wav2Vec2ForAudioFrameClassification`. Les seules diff√©rences entre les architectures de ces mod√®les sont la taille de la couche de classification et la fonction de perte utilis√©e.
En fait, n'importe quel *transformer* encodeur audio peut √™tre transform√© en classifieur d‚Äôaudio en ajoutant une couche de classification au-dessus de la s√©quence d'√©tats cach√©s. Les classifieurs n'ont g√©n√©ralement pas besoin du d√©codeur du *transformer*.
Pour pr√©dire un score de classification unique pour l'ensemble de la s√©quence (`Wav2Vec2ForSequenceClassification`), le mod√®le prend la moyenne sur les √©tats masqu√©s et l'introduit dans la couche de classification. Le r√©sultat est une distribution de probabilit√© unique.
Pour cr√©er une classification distincte pour chaque trame audio (`Wav2Vec2ForAudioFrameClassification`), le classifieur est ex√©cut√© sur la s√©quence d'√©tats masqu√©s, et donc la sortie du classifieur est √©galement une s√©quence.
