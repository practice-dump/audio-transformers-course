# Pr√©traitement d'un jeu de donn√©es audio

Le chargement d'un jeu de donn√©es avec ü§ó *Datasets* n'est que la moiti√© du plaisir. 
Si vous pr√©voyez de l'utiliser pour entra√Æner un mod√®le ou pour ex√©cuter l'inf√©rence, vous devrez d'abord pr√©traiter les donn√©es. En g√©n√©ral, cela impliquera les √©tapes suivantes:
* R√©√©chantillonnage des donn√©es audio
* Filtrage du jeu de donn√©es
* Conversion des donn√©es audio en entr√©e attendue du mod√®le

## R√©√©chantillonnage des donn√©es audio

La fonction `load_dataset` t√©l√©charge des exemples audio avec le taux d'√©chantillonnage avec lequel ils ont √©t√© publi√©s.
Ce n'est pas toujours le taux d'√©chantillonnage attendu par un mod√®le que vous pr√©voyez d'entra√Æner ou d'utiliser pour l'inf√©rence. 
S'il existe un √©cart entre les taux d'√©chantillonnage, vous pouvez r√©√©chantillonner l'audio √† la fr√©quence d'√©chantillonnage attendue du mod√®le.
La plupart des mod√®les pr√©-entra√Æn√©s disponibles ont √©t√© pr√©-entra√Æn√©s sur des jeux de donn√©es audio √† une fr√©quence d'√©chantillonnage de 16 kHz.
Lorsque nous avons explor√© le jeu de donn√©es MINDS-14, vous avez peut-√™tre remarqu√© qu'il est √©chantillonn√© √† 8 kHz, ce qui signifie que nous devrons probablement le sur√©chantillonner.
Pour ce faire, utilisez la m√©thode `cast_column` de ü§ó Datasets. 
Cette op√©ration ne modifie pas l'audio sur place, mais plut√¥t les signaux aux jeux de donn√©es pour r√©√©chantillonner les exemples audio √† la vol√©e lorsqu'ils sont charg√©s. 
Le code suivant d√©finira la fr√©quence d'√©chantillonnage √† 16 kHz :

```py
from datasets import Audio

minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

Rechargez le premier exemple audio dans le jeu de donn√©es MINDS-14 et v√©rifiez qu'il a √©t√© r√©√©chantillonn√© √† la ¬´ fr√©quence d'√©chantillonnage ¬ª souhait√©e :

```py
minds[0]
```

**Sortie :**

```out
{
    "path": "/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav",
    "audio": {
        "path": "/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav",
        "array": array(
            [
                2.0634243e-05,
                1.9437837e-04,
                2.2419340e-04,
                ...,
                9.3852862e-04,
                1.1302452e-03,
                7.1531429e-04,
            ],
            dtype=float32,
        ),
        "sampling_rate": 16000,
    },
    "transcription": "I would like to pay my electricity bill using my card can you please assist",
    "intent_class": 13,
}

```
Vous remarquerez peut-√™tre que les valeurs de tableau sont maintenant √©galement diff√©rentes. C'est parce que nous avons maintenant deux fois plus de valeurs d'amplitude pour chacune d'entre elles que nous avions auparavant.

<Tip> 
üí° Quelques informations sur le r√©√©chantillonnage: Si un signal audio a √©t√© √©chantillonn√© √† 8 kHz, de sorte qu'il a 8000 lectures d'√©chantillon par seconde, nous savons que l'audio ne contient aucune fr√©quence sup√©rieure √† 4 kHz. 
Ceci est garanti par le th√©or√®me d'√©chantillonnage de Nyquist. Pour cette raison, nous pouvons √™tre certains qu'entre les points d'√©chantillonnage, le signal continu d'origine fait toujours une courbe lisse.
L'augmentation du pr√©l√®vement vers un taux d'√©chantillonnage plus √©lev√© consiste alors √† calculer des valeurs d'√©chantillonnage suppl√©mentaires qui se situent entre les valeurs existantes, en approximant cette courbe. 
Le sous-√©chantillonnage, cependant, n√©cessite que nous filtrons d'abord toutes les fr√©quences qui seraient sup√©rieures √† la nouvelle limite de Nyquist, avant d'estimer les nouveaux points d'√©chantillonnage. 
En d'autres termes, vous ne pouvez pas sous-√©chantillonner d'un facteur 2x en jetant simplement tous les autres √©chantillons - cela cr√©era des distorsions dans le signal appel√©es alias. 
Faire un r√©√©chantillonnage correct est d√©licat et mieux laiss√© √† des biblioth√®ques bien test√©es telles que librosa ou ü§ó *Datasets*.
</Tip>

## Filtrage du jeu de donn√©es

Vous devrez peut-√™tre filtrer les donn√©es en fonction de certains crit√®res. L'un des cas courants consiste √† limiter les exemples audio √† une certaine dur√©e. 
Par exemple, nous pourrions vouloir filtrer tous les exemples de plus de 20 secondes pour √©viter les erreurs de m√©moire insuffisante lors de l'entra√Ænement d'un mod√®le.
Nous pouvons le faire en utilisant la  m√©thode `filter` de ü§ó *Datasets* et en lui passant une fonction avec une logique de filtrage. Commen√ßons par √©crire une fonction qui indique quels exemples conserver et lesquels rejeter. 
Cette fonction, `is_audio_length_in_range', renvoie `True` si un √©chantillon est inf√©rieur √† 20s et `False` s'il est plus long que 20s.

```py
MAX_DURATION_IN_SECONDS = 20.0


def is_audio_length_in_range(input_length):
    return input_length < MAX_DURATION_IN_SECONDS
```

La fonction de filtrage peut √™tre appliqu√©e √† la colonne d'un jeu de donn√©es, mais nous n'avons pas de colonne avec une dur√©e de piste audio dans ce jeu de donn√©es. 
Cependant, nous pouvons en cr√©er un, filtrer en fonction des valeurs de cette colonne, puis le supprimer.

```py
# Utilisez librosa pour obtenir la dur√©e de l'exemple √† partir du fichier audio
new_column = [librosa.get_duration(path=x) for x in minds["path"]]
minds = minds.add_column("duration", new_column)

# utiliser la m√©thode `filter` de ü§ó Datasets pour appliquer la fonction de filtrage
minds = minds.filter(is_audio_length_in_range, input_columns=["duration"])

# supprimer la colonne d'assistance temporaire
minds = minds.remove_columns(["duration"])
minds
```

**Sortie :**

```out
Dataset({features: ["path", "audio", "transcription", "intent_class"], num_rows: 624})
```

Nous pouvons v√©rifier que le jeu de donn√©es a √©t√© filtr√© de 654 exemples √† 624.

## Pr√©traitement des donn√©es audio

L'un des aspects les plus difficiles de l'utilisation d'ensembles de donn√©es audio consiste √† pr√©parer les donn√©es dans le bon format pour la formation des mod√®les. 
Comme vous l'avez vu, les donn√©es audio brutes se pr√©sentent sous la forme d'un tableau de valeurs d'√©chantillon. 
Cependant, les mod√®les pr√©-entra√Æn√©s, que vous les utilisiez pour l'inf√©rence ou que vous souhaitiez les finetuner pour votre t√¢che, s'attendent √† ce que les donn√©es brutes soient converties en fonctionnalit√©s d'entr√©e. 
Les exigences pour les caract√©ristiques d'entr√©e peuvent varier d'un mod√®le √† l'autre. Elles d√©pendent de l'architecture du mod√®le et des donn√©es avec lesquelles il a √©t√© pr√©-entra√Æn√©. 
La bonne nouvelle est que, pour chaque mod√®le audio pris en charge, ü§ó *Transformers* offrent une classe d'extracteur de caract√©ristiques qui peut convertir les donn√©es audio brutes en caract√©ristiques d'entr√©e attendues par le mod√®le.

Alors, que fait un extracteur de caract√©ristiques avec les donn√©es audio brutes ? Jetons un coup d'≈ìil √† l'extracteur de caract√©ristiques de [Whisper](https://cdn.openai.com/papers/whisper.pdf) pour comprendre certaines transformations d'extraction de caract√©ristiques courantes. 
Whisper est un mod√®le pr√©-entra√Æn√© pour la reconnaissance vocale automatique (ASR) publi√© en septembre 2022 par Alec Radford et al. d'OpenAI.
Tout d'abord, l'extracteur de fonction Whisper pave / tronque un batch d'exemples audio de sorte que tous les exemples ont une longueur d'entr√©e de 30s. 
Les exemples plus courts sont compl√©t√©s √† 30s en ajoutant des z√©ros √† la fin de la s√©quence (les z√©ros dans un signal audio correspondent √† l'absence de signal ou au silence). Les exemples de plus de 30 ans sont tronqu√©s √† 30 s.
√âtant donn√© que tous les √©l√©ments du batch sont rembourr√©s/tronqu√©s √† une longueur maximale dans l'espace d'entr√©e, il n'y a pas besoin d'un masque d'attention. 
Whisper est unique √† cet √©gard, la plupart des autres mod√®les audio n√©cessitent un masque d'attention qui d√©taille o√π les s√©quences ont √©t√© rembourr√©es, et donc o√π elles doivent √™tre ignor√©es dans le m√©canisme d'auto-attention. 
Whisper est entra√Æn√© pour fonctionner sans masque d'attention et d√©duire directement des signaux vocaux o√π ignorer les entr√©es.
La deuxi√®me op√©ration effectu√©e par l'extracteur de fonctions Whisper consiste √† convertir les matrices audio rembourr√©es en spectrogrammes log-mel.
Ces spectrogrammes d√©crivent comment les fr√©quences d'un signal changent au fil du temps, exprim√©es sur l'√©chelle mel et mesur√©es en d√©cibels (la partie log) pour rendre les fr√©quences et les amplitudes plus repr√©sentatives de l'audition humaine.
Toutes ces transformations peuvent √™tre appliqu√©es √† vos donn√©es audio brutes avec quelques lignes de code. 
Allons de l'avant et chargeons l'extracteur de caract√©ristiques √† partir du *checkpoint* de Whisper pr√©-entra√Æn√© pour √™tre pr√™t pour nos donn√©es audio:

```py
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
```

Ensuite, vous pouvez √©crire une fonction pour pr√©-traiter un seul exemple audio en le faisant passer par le `feature_extractor`.

```py
def prepare_dataset(example):
    audio = example["audio"]
    features = feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"], padding=True
    )
    return features
```

Nous pouvons appliquer la fonction de pr√©paration des donn√©es √† tous nos exemples d‚Äôentra√Ænement en utilisant la m√©thode `map` de ü§ó *Datasets* :

```py
minds = minds.map(prepare_dataset)
minds
```

**Sortie :**

```out
Dataset(
    {
        features: ["path", "audio", "transcription", "intent_class", "input_features"],
        num_rows: 624,
    }
)
```

Aussi simple que cela, nous avons maintenant des spectrogrammes log-mel comme `input_features` dans le jeu de donn√©es.
Visualisons-le pour l'un des exemples de `minds` :

```py
import numpy as np

example = minds[0]
input_features = example["input_features"]

plt.figure().set_figwidth(12)
librosa.display.specshow(
    np.asarray(input_features[0]),
    x_axis="time",
    y_axis="mel",
    sr=feature_extractor.sampling_rate,
    hop_length=feature_extractor.hop_length,
)
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/log_mel_whisper.png" alt="Log mel spectrogram plot">
</div>

Vous pouvez maintenant voir √† quoi ressemble l'entr√©e audio du mod√®le Whisper apr√®s le pr√©traitement.

La classe d'extracteur de caract√©ristiques du mod√®le se charge de transformer les donn√©es audio brutes au format attendu par le mod√®le. Cependant, de nombreuses t√¢ches impliquant l'audio sont multimodales, par exemple la reconnaissance vocale. 
Dans de tels cas, ü§ó *Transformers* offrent √©galement des tokeniseurs sp√©cifiques au mod√®le pour traiter les entr√©es de texte. Pour une plong√©e approfondie dans les tokeniseurs, veuillez vous r√©f√©rer √† notre [cours de NLP](https://huggingface.co/learn/nlp-course/fr/chapter2/4).

Vous pouvez charger s√©par√©ment l'extracteur de caract√©ristiques et le tokeniseur pour Whisper et d'autres mod√®les multimodaux, ou vous pouvez charger les deux via un processeur. 
Pour rendre les choses encore plus simples, utilisez `AutoProcessor` pour charger l'extracteur de caract√©ristiques et le processeur d'un mod√®le √† partir d'un *checkpoint*, comme ceci :

```py
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("openai/whisper-small")
```

Nous avons illustr√© ici les √©tapes fondamentales de pr√©paration des donn√©es. Bien entendu, les donn√©es personnalis√©es peuvent n√©cessiter un pr√©traitement plus complexe.
Dans ce cas, vous pouvez √©tendre la fonction `prepare_dataset` pour effectuer n'importe quel type de transformations de donn√©es personnalis√©es.
Avec ü§ó *Datasets*, si vous pouvez l'√©crire en tant que fonction Python, vous pouvez l'appliquer √† votre jeu de donn√©es !
