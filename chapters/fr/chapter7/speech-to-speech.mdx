# Traduction parole-√†-parole 

La traduction parole-√†-parole (STST ou S2ST pour *Speech-to-speech translation*) est une t√¢che de traitement du langage parl√© relativement nouvelle. Elle consiste √† traduire le discours d'une langue en un discours dans une langue **diff√©rente** :

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/s2st.png" alt="Diagram of speech to speech translation">
</div>

La STST peut √™tre consid√©r√©e comme une extension de la t√¢che traditionnelle de traduction automatique : au lieu de traduire du **texte** d'une langue √† une autre, nous traduisons de la **parole** d'une langue √† une autre. La STST trouve des applications dans le domaine de la communication multilingue, en permettant √† des locuteurs de langues diff√©rentes de communiquer entre eux par l'interm√©diaire de la parole.

Supposons que vous souhaitiez communiquer avec une autre personne au-del√† de la barri√®re de la langue. Plut√¥t que d'√©crire l'information que vous souhaitez transmettre et de la traduire ensuite en texte dans la langue cible, vous pouvez la parler directement et demander √† un syst√®me STST de convertir votre discours dans la langue cible. Le destinataire peut alors r√©pondre en s'adressant au syst√®me STST, et vous pouvez √©couter sa r√©ponse. Cette m√©thode de communication est plus naturelle que la traduction automatique bas√©e sur le texte.

Dans ce chapitre, nous allons explorer une approche *cascad√©e* de la STST, en rassemblant les connaissances que vous avez acquises dans les unit√©s 5 et 6 du cours. Nous utiliserons un syst√®me de *traduction vocale* pour transcrire le discours source en texte dans la langue cible, puis un syst√®me de *text-to-speech* pour g√©n√©rer du discours dans la langue cible √† partir du texte traduit :

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/s2st_cascaded.png" alt="Diagram of s2st_cascaded speech to speech translation">
</div>

Nous aurions √©galement pu utiliser une approche en trois √©tapes, en commen√ßant par utiliser un syst√®me de reconnaissance automatique de la parole (ASR) pour transcrire la parole source en texte dans la m√™me langue, puis la traduction automatique pour traduire le texte transcrit dans la langue cible, et enfin la synth√®se vocale pour g√©n√©rer de la parole dans la langue cible. Cependant, l'ajout d'autres composants au pipeline se pr√™te √† la propagation d'erreurs, o√π les erreurs introduites dans un syst√®me sont aggrav√©es lorsqu'elles passent par les syst√®mes restants, et augmente √©galement le temps de latence, puisque l'inf√©rence doit √™tre effectu√©e pour davantage de mod√®les.

Bien que cette approche en deux √©tapes de la STST soit assez simple, elle permet d'obtenir des syst√®mes tr√®s efficaces. Le syst√®me en deux √©tapes √† trois √©tapes ASR + traduction automatique + TTS a √©t√© utilis√© pr√©c√©demment pour de nombreux produits STST commerciaux, y compris [*Google Translate*](https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html).
Il s'agit √©galement d'une m√©thode tr√®s efficace en termes de donn√©es et de calcul, puisque les syst√®mes de reconnaissance vocale et de synth√®se vocale existants peuvent √™tre coupl√©s pour produire un nouveau mod√®le de STST sans aucun entra√Ænement suppl√©mentaire.

Dans la suite de cette unit√©, nous nous concentrerons sur la cr√©ation d'un syst√®me de STST qui traduit la parole de n'importe quelle langue X en anglais. Les m√©thodes abord√©es peuvent √™tre √©tendues aux syst√®mes de STST qui traduisent de n'importe quelle langue X vers n'importe quelle langue Y, mais nous laissons cette extension au lecteur et fournissons des pointeurs le cas √©ch√©ant. Nous divisons ensuite la t√¢che de STST en ses deux composantes constitutives : traduction vocale et synth√®se vocale. Nous terminerons en les assemblant pour construire une d√©mo Gradio afin de pr√©senter notre syst√®me.


## Traduction vocale

Nous utilisons le mod√®le Whisper puisqu'il est capable de traduire plus de 96 langues vers l'anglais. Plus pr√©cis√©ment, nous chargerons le *checkpoint* [Whisper Base](https://huggingface.co/openai/whisper-base), qui contient 74 millions de param√®tres. Ce n'est en aucun cas le mod√®le Whisper le plus performant (le [plus grand *checkpoint* Whisper](https://huggingface.co/openai/whisper-large-v2) √©tant plus de 20 fois plus grand) mais comme nous concat√©nons deux syst√®mes autor√©gressifs, nous voulons nous assurer que chaque mod√®le peut g√©n√©rer relativement rapidement afin d'obtenir une vitesse d'inf√©rence raisonnable :

```python
import torch
from transformers import pipeline

device = "cuda:0" if torch.cuda.is_available() else "cpu"
pipe = pipeline(
    "automatic-speech-recognition", model="openai/whisper-base", device=device
)
```

C'est tr√®s bien ! Pour tester notre syst√®me de STST, nous allons charger un √©chantillon audio dans une autre langue que l'anglais. Chargeons le premier exemple de la partie italienne (`it`) du jeu de donn√©es [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) :

```python
from datasets import load_dataset

dataset = load_dataset("facebook/voxpopuli", "it", split="validation", streaming=True)
sample = next(iter(dataset))
```

Pour √©couter cet √©chantillon, nous pouvons soit le jouer en utilisant le *viewer* sur le *Hub* : [facebook/voxpopuli/viewer](https://huggingface.co/datasets/facebook/voxpopuli/viewer/it/validation?row=0)

Ou le lire √† l'aide de la fonction audio d'ipynb :

```python
from IPython.display import Audio

Audio(sample["audio"]["array"], rate=sample["audio"]["sampling_rate"])
```

Maintenant, d√©finissons une fonction qui prend cette entr√©e audio et retourne le texte traduit. Vous vous rappelez que nous devons passer l'argument `"task": "translate"` pour s'assurer que Whisper effectue de la traduction vocale et non de la reconnaissance vocale :

```python
def translate(audio):
    outputs = pipe(audio, max_new_tokens=256, generate_kwargs={"task": "translate"})
    return outputs["text"]
```

<Tip>

Whisper peut aussi √™tre "tromp√©" pour traduire de la parole dans n'importe quelle langue X vers n'importe quelle langue Y. Il suffit de mettre la t√¢che √† `"transcribe"` et `"language"` √† votre langue cible en argument. Par exemple pour l'espagnol, on mettrait : `generate_kwargs={"task" : "transcribe", "language" : "es"&rcub;`

</Tip>

Bien, v√©rifions rapidement que nous obtenons un r√©sultat raisonnable √† partir du mod√®le :

```python
translate(sample["audio"].copy())
```

```
' psychological and social. I think that it is a very important step in the construction of a juridical space of freedom, circulation and protection of rights.'
```

Ok, et si nous comparons cela au texte source :

```python
sample["raw_text"]
```

```
'Penso che questo sia un passo in avanti importante nella costruzione di uno spazio giuridico di libert√† di circolazione e di protezione dei diritti per le persone in Europa.'
```

Nous constatons que la traduction est plus ou moins conforme (vous pouvez la v√©rifier en utilisant Google Translate ou DeepL), √† l'exception de quelques mots suppl√©mentaires au d√©but de la transcription, lorsque le locuteur terminait sa phrase pr√©c√©dente.

Nous avons ainsi termin√© la premi√®re moiti√© de notre pipeline de STST en deux √©tapes, en mettant en pratique les comp√©tences acquises dans l'unit√© 5 lorsque nous avons appris √† utiliser le mod√®le Whisper pour la reconnaissance vocale et la traduction. Si vous souhaitez vous rafra√Æchir la m√©moire sur l'une des √©tapes que nous avons couvertes, lisez la section [Mod√®les pr√©-entra√Æn√©s pour la reconnaissance automatique de la parole](../chapiter5/asr_models) de l'unit√© 5.

## Synth√®se vocale

La deuxi√®me partie de notre syst√®me de STST en deux √©tapes consiste √† passer d'un texte anglais √† la parole en anglais. Pour cela, nous utiliserons le mod√®le pr√©-entra√Æn√© [SpeechT5 TTS](https://huggingface.co/microsoft/speecht5_tts). ü§ó *Transformers* n'a actuellement pas de `pipeline` TTS, donc nous devrons utiliser le mod√®le directement nous-m√™mes. Ce n'est pas grave, vous √™tes tous experts dans l'utilisation du mod√®le pour l'inf√©rence apr√®s l'unit√© 6 !

Tout d'abord, chargeons le processeur SpeechT5, le mod√®le et le vocodeur √† partir du *checkpoint* pr√©-entra√Æn√© :

```python
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan

processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")

model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
```

<Tip>
    Nous utilisons ici le *checkpoint* SpeechT5 entra√Æn√© sp√©cifiquement pour le TTS anglais. Si vous souhaitez traduire dans une autre langue que l'anglais, remplacez le *checkpoint* par un mod√®le SpeechT5 TTS *finetun√©* dans la langue de votre choix, ou utilisez un *checkpoint* MMS TTS pr√©-entra√Æn√© dans la langue cible.
</Tip>

Comme pour le Whisper, nous placerons le SpeechT5 et le vocodeur sur notre GPU, si nous en avons un :

```python
model.to(device)
vocoder.to(device)
```

Bien, chargeons les enregistrements du locuteur :

```python
embeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")
speaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)
```

Nous pouvons maintenant √©crire une fonction qui prend en entr√©e un prompt de texte et g√©n√®re le discours correspondant. Nous allons d'abord pr√©traiter l'entr√©e textuelle √† l'aide du processeur SpeechT5, en tokenisant le texte pour obtenir nos identifiants d'entr√©e. Nous transmettons ensuite les identifiants d'entr√©e et les ench√¢ssements de locuteurs au mod√®le SpeechT5, en pla√ßant chacun d'entre eux sur le GPU s'il est disponible. Enfin, nous renverrons le discours g√©n√©r√© au CPU pour qu'il puisse √™tre lu dans notre *notebook* :

```python
def synthesise(text):
    inputs = processor(text=text, return_tensors="pt")
    speech = model.generate_speech(
        inputs["input_ids"].to(device), speaker_embeddings.to(device), vocoder=vocoder
    )
    return speech.cpu()
```

V√©rifions qu'il fonctionne avec une entr√©e de texte fictive :

```python
speech = synthesise("Hey there! This is a test!")

Audio(speech, rate=16000)
```

√áa a l'air bien ! Passons maintenant √† la partie la plus excitante : assembler le tout.

## Creating a STST demo

Avant de cr√©er une d√©mo [Gradio](https://gradio.app) pour pr√©senter notre syst√®me de STST, effectuons d'abord une v√©rification rapide pour nous assurer que nous pouvons concat√©ner les deux mod√®les, en introduisant un √©chantillon audio et en obtenant un √©chantillon audio. Pour ce faire, nous concat√©nerons les deux fonctions d√©finies dans les deux sous-sections pr√©c√©dentes, de sorte que nous entrons l'audio source et r√©cup√©rons le texte traduit, puis nous synth√©tisons le texte traduit pour obtenir la parole traduite. Enfin, nous allons convertir la synth√®se vocale en un *array* `int16`, qui est le format de fichier audio de sortie attendu par Gradio. Pour ce faire, nous devons d'abord normaliser l'*array* audio par la plage dynamique de la cible de type (`int16`), puis convertir le type par d√©faut de NumPy (`float64`) vers la cible de type (`int16`) :

```python
import numpy as np

target_dtype = np.int16
max_range = np.iinfo(target_dtype).max


def speech_to_speech_translation(audio):
    translated_text = translate(audio)
    synthesised_speech = synthesise(translated_text)
    synthesised_speech = (synthesised_speech.numpy() * max_range).astype(np.int16)
    return 16000, synthesised_speech
```

V√©rifions que cette fonction concat√©n√©e donne le r√©sultat attendu :

```python
sampling_rate, synthesised_speech = speech_to_speech_translation(sample["audio"])

Audio(synthesised_speech, rate=sampling_rate)
```

Parfait ! Nous allons √† pr√©sent int√©grer tout cela dans une belle d√©mo Gradio afin d'enregistrer notre source vocale √† l'aide d'un microphone ou d'un fichier et de lire la pr√©diction du syst√®me :

```python
import gradio as gr

demo = gr.Blocks()

mic_translate = gr.Interface(
    fn=speech_to_speech_translation,
    inputs=gr.Audio(source="microphone", type="filepath"),
    outputs=gr.Audio(label="Generated Speech", type="numpy"),
)

file_translate = gr.Interface(
    fn=speech_to_speech_translation,
    inputs=gr.Audio(source="upload", type="filepath"),
    outputs=gr.Audio(label="Generated Speech", type="numpy"),
)

with demo:
    gr.TabbedInterface([mic_translate, file_translate], ["Microphone", "Audio File"])

demo.launch(debug=True)
```

Cela lancera une d√©mo Gradio similaire √† celle qui fonctionne sur le *Space* suivant :

<iframe src="https://course-demos-speech-to-speech-translation.hf.space" frameBorder="0" height="450" title="Gradio app" class="container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

Vous pouvez [dupliquer](https://huggingface.co/spaces/course-demos/speech-to-speech-translation?duplicate=true) cette d√©mo et l'adapter pour utiliser un *checkpoint* Whisper diff√©rent, un *checkpoint* TTS diff√©rent, ou travailler sur une autre langue que l'anglais et suivre les conseils fournis pour traduire dans la langue de votre choix !

## Pour aller plus loin

Bien que le syst√®me en deux √©tapes soit un moyen efficace de construire un syst√®me de STST, il souffre des probl√®mes de propagation d'erreurs et de latence additive d√©crits ci-dessus. Des travaux r√©cents ont explor√© une approche *directe* de la STST, qui ne pr√©dit pas de texte interm√©diaire et qui, au lieu de cela, √©tablit une correspondance directe entre la parole source et la parole cible. Ces syst√®mes sont √©galement capables de conserver les caract√©ristiques de la parole du locuteur source dans la parole cible (telles que la prosodie, la hauteur et l'intonation). Si vous souhaitez en savoir plus sur ces syst√®mes, consultez les ressources r√©pertori√©es dans la section [lectures compl√©mentaires](supplemental_reading).
