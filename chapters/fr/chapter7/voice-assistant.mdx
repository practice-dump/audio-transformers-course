# Cr√©er un assistant vocal

Dans cette section, nous allons rassembler trois mod√®les d√©j√† vus pour construire un assistant vocal de bout en bout baptis√© **Marvin** ü§ñ. Comme Alexa d'Amazon ou Siri d'Apple, Marvin est un assistant vocal r√©pondant √† un mot d√©clencheur particulier, √©coutant ensuite une requ√™te vocale et r√©pond enfin par une r√©ponse vocale.

Nous pouvons d√©composer le pipeline de l'assistant vocal en quatre √©tapes, chacune d'entre elles n√©cessitant un mod√®le autonome :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/voice_assistant.png">
 </div>

### 1. D√©tection du mot d√©clencheur

Les assistants vocaux √©coutent en permanence les entr√©es audio provenant du microphone de votre appareil, mais ils ne se mettent en action que lorsqu'un mot d√©clencheur (ou mot de r√©veil) particulier est prononc√©.

La t√¢che de d√©tection des mots d√©clencheurs est g√©r√©e par un petit mod√®le de classification audio sur l'appareil, qui est beaucoup plus petit et plus l√©ger que le mod√®le de reconnaissance vocale, ne comportant souvent que quelques millions de param√®tres, contre plusieurs centaines de millions pour la reconnaissance vocale. Il peut donc √™tre ex√©cut√© en continu sur l'appareil sans en √©puiser la batterie. Ce n'est que lorsque le mot de r√©veil est d√©tect√© que le mod√®le de reconnaissance vocale, plus grand, est lanc√©, avant d'√™tre √† nouveau arr√™t√©.

### 2. Transcription de la parole

L'√©tape suivante du processus consiste √† transcrire la requ√™te vocale en texte. Dans la pratique, le transfert des fichiers audio de votre appareil local vers le *cloud* est lent en raison de la nature volumineuse des fichiers audio. Il est donc plus efficace de les transcrire directement √† l'aide d'un mod√®le d'ASR sur l'appareil plut√¥t qu'√† l'aide d'un mod√®le dans le *cloud*. Le mod√®le sur l'appareil peut √™tre plus petit et donc moins pr√©cis qu'un mod√®le h√©berg√© dans le *cloud*, mais la vitesse d'inf√©rence plus rapide en vaut la peine puisque nous pouvons ex√©cuter la reconnaissance vocale quasiment en temps r√©el, notre √©nonc√© audio √©tant transcrit au fur et √† mesure que nous le pronon√ßons.

Nous sommes maintenant tr√®s familiers avec le processus de reconnaissance vocale, donc cela devrait √™tre un jeu d'enfant !

### 3. Requ√™ter le mod√®le de langage

Maintenant que nous savons ce que l'utilisateur a demand√©, nous devons g√©n√©rer une r√©ponse ! Les meilleurs mod√®les candidats pour cette t√¢che sont les *grands mod√®les de langage (LLM)*, car ils sont effectivement capables de comprendre la s√©mantique de la requ√™te textuelle et de g√©n√©rer une r√©ponse appropri√©e.

Puisque notre requ√™te textuelle est petite (juste quelques *tokens*), et les mod√®les de langage volumineux (plusieurs milliards de param√®tres), la fa√ßon la plus efficace d'ex√©cuter l'inf√©rence est d'envoyer notre requ√™te textuelle de notre appareil √† un LLM fonctionnant dans le *cloud*, de g√©n√©rer une r√©ponse textuelle, et de renvoyer la r√©ponse √† l'appareil.

### 4. Synth√©tiser la r√©ponse

Enfin, nous utiliserons un mod√®le de synth√®se vocale pour synth√©tiser la r√©ponse textuelle sous forme de parole. Cette op√©ration s'effectue sur l'appareil, mais il est tout √† fait possible d'ex√©cuter un mod√®le de synth√®se vocale dans le *cloud*, en g√©n√©rant la sortie audio et en la transf√©rant vers l'appareil.

Encore une fois, nous avons fait cela plusieurs fois maintenant, donc le processus sera tr√®s familier !

## D√©tection des mots d√©clencheur

La premi√®re √©tape du pipeline de l'assistant vocal consiste √† d√©tecter si le mot d√©clencheur a √©t√© prononc√©, et nous devons donc trouver un mod√®le pr√©-entra√Æn√© appropri√© pour cette t√¢che ! Vous vous souviendrez de la section sur les [mod√®les pr√©-entra√Æn√©s pour la classification audio](../chapiter4/classification_models) que [Speech Commands](https://huggingface.co/datasets/speech_commands) est un jeu de donn√©es de mots prononc√©s con√ßu pour √©valuer les mod√®les de classification audio sur plus de 15 mots de commande simples comme `up`, `down`, `yes` et `no`, ainsi qu'un label `silence` pour classer l'absence de parole. Prenez une minute pour √©couter les √©chantillons sur le visualisateur de jeux de donn√©es du *Hub* et vous familiariser √† nouveau avec le jeu de donn√©es : [visualisateur](https://huggingface.co/datasets/speech_commands/viewer/v0.01/train).

Nous pouvons utiliser un mod√®le de classification audio pr√©-entra√Æn√© sur le jeu de donn√©es Speech Commands et choisir l'un de ces mots de commande simples comme mot d√©clencheur. Parmi plus de 15 mots de commande possibles, si le mod√®le pr√©dit avec la plus grande probabilit√© le mot que nous avons choisi, nous pouvons √™tre quasiment certains que ce mot de d√©clencheur a √©t√© prononc√©.

Rendez-vous sur le *Hub* et cliquez sur l'onglet *Models* : https://huggingface.co/models. Cela va faire appara√Ætre tous les mod√®les sur le *Hub* :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/all_models.png">
 </div>

Vous remarquerez sur le c√¥t√© gauche que nous avons une s√©lection d'onglets que nous pouvons s√©lectionner pour filtrer les mod√®les par t√¢che, biblioth√®que, jeu de donn√©es, etc. Faites d√©filer vers le bas et s√©lectionnez la t√¢che *Audio Classification* dans la liste des t√¢ches audio :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/by_audio_classification.png">
 </div>

Nous sommes maintenant en pr√©sence d'un sous-ensemble de plus de 500 mod√®les de classification audio sur le *Hub*. Pour affiner cette s√©lection, nous pouvons filtrer les mod√®les par jeu de donn√©es. Cliquez sur l'onglet *Datasets* et dans la barre de recherche, tapez *speech_commands*. Vous pouvez cliquer sur ce bouton pour filtrer tous les mod√®les de classification audio vers ceux qui ont √©t√© *finetun√©* sur le jeu de donn√©es Speech Commands :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/by_speech_commands.png">
 </div>

Bien , nous constatons que nous disposons de six mod√®les pr√©-entra√Æn√©s pour ce jeu de donn√©es et cette t√¢che sp√©cifiques (bien que de nouveaux mod√®les puissent √™tre ajout√©s si vous lisez √† une date ult√©rieure !) Vous reconna√Ætrez le premier de ces mod√®les comme le [*Spectrogram Transformer*](https://huggingface.co/MIT/ast-finetuned-speech-commands-v2) que nous avons utilis√© dans l'exemple de l'unit√© 4. Nous utiliserons √† nouveau ce *checkpoint* pour notre t√¢che de d√©tection de mots d√©clencheur.

Chargeons le *checkpoint* √† l'aide de la classe `pipeline` :

```python
from transformers import pipeline
import torch

device = "cuda:0" if torch.cuda.is_available() else "cpu"

classifier = pipeline(
    "audio-classification", model="MIT/ast-finetuned-speech-commands-v2", device=device
)
```

Nous pouvons v√©rifier sur quelles √©tiquettes le mod√®le a √©t√© entra√Æn√© en v√©rifiant l'attribut `id2label` dans la configuration du mod√®le :

```python
classifier.model.config.id2label
```

Nous voyons que le mod√®le a √©t√© entra√Æn√© sur 35 √©tiquettes de classe, y compris quelques mots d√©clencheurs simples que nous avons d√©crits ci-dessus, ainsi que quelques objets particuliers comme `bed`, `house` et `cat`. Nous voyons qu'il y a un nom dans ces √©tiquettes de classe : id 27 correspond √† l'√©tiquette **`marvin`** :

```python
classifier.model.config.id2label[27]
```

```
'marvin'
```

Parfait ! Nous pouvons utiliser ce nom comme mot d√©lchencheur de notre assistant vocal, de la m√™me mani√®re qu'on utilise "Alexa" pour l'Alexa d'Amazon, ou "Hey Siri" pour le Siri d'Apple. Parmi toutes les √©tiquettes possibles, si le mod√®le pr√©dit "marvin" avec la probabilit√© de classe la plus √©lev√©e, nous pouvons √™tre s√ªrs que le mot de r√©veil que nous avons choisi a √©t√© prononc√©. 

Nous devons maintenant d√©finir une fonction qui √©coute en permanence le microphone de notre appareil et qui transmet continuellement l'audio au mod√®le de classification pour inf√©rence. Pour ce faire, nous allons utiliser une fonction d'aide pratique fournie avec ü§ó *Transformers* appel√©e [`ffmpeg_microphone_live`](https://github.com/huggingface/transformers/blob/fb78769b9c053876ed7ae152ee995b0439a4462a/src/transformers/pipelines/audio_utils.py#L98).

Cette fonction transmet de petits morceaux d'audio d'une longueur sp√©cifi√©e `chunk_length_s` au classifieur. Pour s'assurer que nous obtenons des fronti√®res lisses entre les morceaux d'audio, nous lan√ßons une fen√™tre coulissante √† travers notre audio avec un pas de `chunk_length_s / 6`. 
Pour que nous n'ayons pas √† attendre que tout le premier morceau soit enregistr√© avant de commencer l'inf√©rence, nous d√©finissons √©galement une longueur d'entr√©e audio temporaire minimale `stream_chunk_s` qui est transmise au mod√®le avant que le temps `chunk_length_s` ne soit atteint.

La fonction `ffmpeg_microphone_live` renvoie un objet *generator*, produisant une s√©quence de morceaux audio qui peuvent chacun √™tre transmis au mod√®le de classification pour faire une pr√©diction. Nous pouvons passer ce g√©n√©rateur directement au `pipeline`, qui √† son tour retourne une s√©quence de pr√©dictions de sortie, une pour chaque morceau d'entr√©e audio. Nous pouvons inspecter les probabilit√©s des classes pour chaque morceau d'audio, et arr√™ter notre boucle de d√©tection de mot de r√©veil lorsque nous d√©tectons que le mot de r√©veil a √©t√© prononc√©.

Nous utiliserons un crit√®re tr√®s simple pour d√©terminer si notre mot de r√©veil a √©t√© prononc√© : si l'√©tiquette de classe ayant la probabilit√© la plus √©lev√©e est celle de notre mot, et que cette probabilit√© d√©passe un seuil `prob_threshold`, nous d√©clarons que le mot d√©clencheur a √©t√© prononc√©. L'utilisation d'un seuil de probabilit√© pour contr√¥ler notre classifieur garantit que le mot d√©clencheur  n'est pas pr√©dit par erreur si l'entr√©e audio est un bruit, ce qui est typiquement le cas lorsque le mod√®le est tr√®s incertain et que toutes les probabilit√©s d'√©tiquettes de classe sont faibles. Vous voudrez peut-√™tre ajuster ce seuil de probabilit√© ou explorer des moyens plus sophistiqu√©s par le biais d'une m√©trique bas√©e sur l'[*entropie*](https://fr.wikipedia.org/wiki/Entropie_de_Shannon).

```python
from transformers.pipelines.audio_utils import ffmpeg_microphone_live


def launch_fn(
    wake_word="marvin",
    prob_threshold=0.5,
    chunk_length_s=2.0,
    stream_chunk_s=0.25,
    debug=False,
):
    if wake_word not in classifier.model.config.label2id.keys():
        raise ValueError(
            f"Wake word {wake_word} not in set of valid class labels, pick a wake word in the set {classifier.model.config.label2id.keys()}."
        )

    sampling_rate = classifier.feature_extractor.sampling_rate

    mic = ffmpeg_microphone_live(
        sampling_rate=sampling_rate,
        chunk_length_s=chunk_length_s,
        stream_chunk_s=stream_chunk_s,
    )

    print("Listening for wake word...")
    for prediction in classifier(mic):
        prediction = prediction[0]
        if debug:
            print(prediction)
        if prediction["label"] == wake_word:
            if prediction["score"] > prob_threshold:
                return True
```

Essayons cette fonction pour voir comment elle fonctionne ! Nous allons mettre `debug=True` pour afficher la pr√©diction pour chaque morceau d'audio. Laissons le mod√®le fonctionner pendant quelques secondes pour voir le type de pr√©dictions qu'il fait lorsqu'il n'y a pas de parole, puis pronon√ßons clairement le mot de r√©veil *marvin* et regardons sa pr√©diction monter en fl√®che √† pr√®s de 1 :

```python
launch_fn(debug=True)
```

```text
Listening for wake word...
{'score': 0.055326107889413834, 'label': 'one'}
{'score': 0.05999856814742088, 'label': 'off'}
{'score': 0.1282748430967331, 'label': 'five'}
{'score': 0.07310110330581665, 'label': 'follow'}
{'score': 0.06634809821844101, 'label': 'follow'}
{'score': 0.05992642417550087, 'label': 'tree'}
{'score': 0.05992642417550087, 'label': 'tree'}
{'score': 0.999913215637207, 'label': 'marvin'}
```

G√©nial ! Comme nous nous y attendions, le mod√®le g√©n√®re des pr√©dictions erron√©es pendant les premi√®res secondes. Il n'y a pas d'entr√©e vocale, donc le mod√®le fait des pr√©dictions presque al√©atoires, mais avec une tr√®s faible probabilit√©. D√®s que nous pronon√ßons le mot de r√©veil, le mod√®le pr√©dit *marvin* avec une probabilit√© proche de 1 et termine la boucle, signalant que le mot de r√©veil a √©t√© d√©tect√© et que le syst√®me ASR doit √™tre activ√© !

## Transcription de la parole

Une fois de plus, nous utiliserons le mod√®le Whisper pour notre syst√®me de transcription de la parole. Plus pr√©cis√©ment, nous chargerons le *checkpoint* [*Whisper Base English*](https://huggingface.co/openai/whisper-base.en), car il est suffisamment petit pour donner une bonne vitesse d'inf√©rence avec une pr√©cision de transcription raisonnable. Nous utiliserons une astuce pour obtenir une transcription presque en temps r√©el en √©tant astucieux sur la fa√ßon dont nous transmettons nos entr√©es audio au mod√®le. Comme auparavant, vous pouvez utiliser n'importe quel mod√®le de reconnaissance vocale sur le [*Hub*](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&library=transformers&sort=trending), y compris Wav2Vec2, MMS ASR ou d'autres tailles de Whisper :

```python
transcriber = pipeline(
    "automatic-speech-recognition", model="openai/whisper-base.en", device=device
)
```

<Tip>
    Si vous utilisez un GPU, vous pouvez augmenter la taille du mod√®le √† utiliser, ce qui permettra d'obtenir une meilleure pr√©cision de transcription tout en respectant le seuil de latence requis. Il suffit de remplacer l'identifiant du mod√®le par : <code>"openai/whisper-small.en"</code>.
</Tip>

Nous pouvons maintenant d√©finir une fonction pour enregistrer l'entr√©e de notre microphone et transcrire le texte correspondant. Avec la fonction d'aide `ffmpeg_microphone_live`, nous pouvons contr√¥ler le degr√© de "temps r√©el" de notre mod√®le de reconnaissance vocale. L'utilisation d'un `stream_chunk_s` plus petit se pr√™te √† une reconnaissance vocale davantage en temps r√©el, puisque nous divisons notre audio d'entr√©e en plus petits morceaux et que nous les transcrivons √† la vol√©e. Cependant, cela se fait au d√©triment de la pr√©cision, puisqu'il y a moins de contexte pour le mod√®le √† d√©duire. 

Lors de la transcription de la parole, nous devons √©galement avoir une id√©e du moment o√π l'utilisateur **s'arr√™te** de parler, afin de pouvoir mettre fin √† l'enregistrement. Pour des raisons de simplicit√©, nous mettrons fin √† l'enregistrement apr√®s le premier `chunk_length_s` (qui est fix√© √† 5 secondes par d√©faut), mais vous pouvez essayer d'utiliser un mod√®le de [d√©tection de l'activit√© vocale (VAD pour *voice activity detection*)](https://huggingface.co/models?pipeline_tag=voice-activity-detection&sort=trending) pour pr√©dire quand l'utilisateur s'est arr√™t√© de parler.

```python
import sys


def transcribe(chunk_length_s=5.0, stream_chunk_s=1.0):
    sampling_rate = transcriber.feature_extractor.sampling_rate

    mic = ffmpeg_microphone_live(
        sampling_rate=sampling_rate,
        chunk_length_s=chunk_length_s,
        stream_chunk_s=stream_chunk_s,
    )

    print("Start speaking...")
    for item in transcriber(mic, generate_kwargs={"max_new_tokens": 128}):
        sys.stdout.write("\033[K")
        print(item["text"], end="\r")
        if not item["partial"][0]:
            break

    return item["text"]
```

Essayons et voyons comment nous nous d√©brouillerons ! Une fois que le microphone est activ√©, commencez √† parler et regardez votre transcription appara√Ætre en semi-temps r√©el :

```python
transcribe()
```

```text
Start speaking...
 Hey, this is a test with the whisper model.
```

Bien, vous pouvez ajuster la longueur maximale de l'audio `chunk_length_s` en fonction de la rapidit√© ou de la lenteur avec laquelle vous parlez (augmentez-la si vous avez l'impression de ne pas avoir assez de temps pour parler, diminuez-la si vous √™tes rest√© dans l'expectative √† la fin), et le `stream_chunk_s` pour le facteur temps r√©el. Il suffit de passer ces arguments √† la fonction `transcribe`.

## Requ√™ter le mod√®le de langue

Maintenant que notre requ√™te vocale a √©t√© transcrite, nous voulons g√©n√©rer une r√©ponse significative. Pour ce faire, nous utiliserons un LLM h√©berg√© sur le *cloud*. Plus pr√©cis√©ment, nous choisissons un LLM sur le *Hub* et utilisons l'[API Inference](https://huggingface.co/inference-api) pour interroger facilement le mod√®le.

Trouvons notre LLM sur le *Hub* en utilisant l'[ü§ó *Open LLM Leaderboard*](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), un *Space* qui classe les LLM par performance sur quatre t√¢ches de g√©n√©ration. Nous effectuerons une recherche par "instruct" pour filtrer les mod√®les qui ont √©t√© *finetun√©* car devraient mieux fonctionner pour notre t√¢che de requ√™tage :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/llm_leaderboard.png">
 </div>

Nous utiliserons le *checkpoint* [tiiuae/falcon-7b-instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) par [TII](https://www.tii.ae/), un *transformer* d√©codeur de param√®tres 7B *finetun√©* sur un m√©lange de jeux de donn√©es de chat et d'instructions. Vous pouvez utiliser n'importe quel LLM du *Hub* ayant l'"*Hosted inference API*" activ√©. Il suffit de regarder le widget sur le c√¥t√© droit de la carte du mod√®le :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/inference_api.png">
 </div>

L'API Inference nous permet d'envoyer une requ√™te HTTP depuis notre machine locale vers le LLM h√©berg√© sur le *Hub*, et renvoie la r√©ponse sous la forme d'un fichier `json`. Tout ce que nous devons fournir est notre *token* d'authentification au *Hub* et l'identifiant du mod√®le du LLM que nous souhaitons interroger :

```python
from huggingface_hub import HfFolder
import requests


def query(text, model_id="tiiuae/falcon-7b-instruct"):
    api_url = f"https://api-inference.huggingface.co/models/{model_id}"
    headers = {"Authorization": f"Bearer {HfFolder().get_token()}"}
    payload = {"inputs": text}

    print(f"Querying...: {text}")
    response = requests.post(api_url, headers=headers, json=payload)
    return response.json()[0]["generated_text"][len(text) + 1 :]
```

Essayons-le avec une entr√©e de test !

```python
query("What does Hugging Face do?")
```

```
'Hugging Face is a company that provides natural language processing and machine learning tools for developers. They'
```

Vous remarquerez √† quel point l'inf√©rence est rapide en utilisant l'API Inference. Nous n'avons qu'√† envoyer un petit nombre de *tokens* de texte de notre machine locale au mod√®le h√©berg√©, le co√ªt de communication est donc tr√®s faible. Le LLM est h√©berg√© sur des GPU, de sorte que l'inf√©rence s'ex√©cute tr√®s rapidement. Enfin, la r√©ponse g√©n√©r√©e est transf√©r√©e du mod√®le √† notre machine locale, toujours avec un faible co√ªt de communication.

## Synth√©tiser la parole

Et maintenant, nous sommes pr√™ts √† obtenir la sortie vocale finale ! Une fois de plus, nous utiliserons le mod√®le [SpeechT5 TTS](https://huggingface.co/microsoft/speecht5_tts) de Microsoft mais vous pouvez utiliser n'importe quel mod√®le TTS de votre choix. Chargeons le processeur et le mod√®le :

```python
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan

processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")

model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts").to(device)
vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan").to(device)
```

Et aussi l'ench√¢ssement des locuteurs :

```python
from datasets import load_dataset

embeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")
speaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)
```

Nous allons r√©utiliser la fonction `synthesise` que nous avons d√©finie dans le chapitre [pr√©c√©dent](speech-to-speech) :

```python
def synthesise(text):
    inputs = processor(text=text, return_tensors="pt")
    speech = model.generate_speech(
        inputs["input_ids"].to(device), speaker_embeddings.to(device), vocoder=vocoder
    )
    return speech.cpu()
```

V√©rifions rapidement que cela fonctionne comme pr√©vu :

```python
from IPython.display import Audio

audio = synthesise(
    "Hugging Face is a company that provides natural language processing and machine learning tools for developers."
)

Audio(audio, rate=16000)
```

Joli travail üëç

## Marvin ü§ñ

Maintenant que nous avons d√©fini une fonction pour chacune des quatre √©tapes du pipeline de l'assistant vocal, il ne reste plus qu'√† les assembler pour obtenir notre assistant vocal de bout en bout. Nous allons simplement concat√©ner les quatre √©tapes, en commen√ßant par la d√©tection du mot de r√©veil (`launch_fn`), la transcription de la parole, le requ√™tage du LLM, et enfin la synth√®se de la parole.

```python
launch_fn()
transcription = transcribe()
response = query(transcription)
audio = synthesise(response)

Audio(audio, rate=16000, autoplay=True)
```

Essayez-le avec quelques demandes ! Voici quelques exemples pour vous aider √† d√©marrer :
* *Quel est le pays le plus chaud du monde ?
* *Comment fonctionnent les *transformers* ?
* *Connais-tu l'espagnol ?

Et avec cela, nous avons notre assistant vocal complet, r√©alis√© √† l'aide des outils audio que vous avez appris tout au long de ce cours, avec une pinc√©e de magie LLM √† la fin. Il y a plusieurs extensions que nous pourrions faire pour am√©liorer l'assistant vocal. Tout d'abord, le mod√®le de classification audio classifie 35 √©tiquettes diff√©rentes. Nous pourrions utiliser un mod√®le de classification binaire plus petit et plus l√©ger qui pr√©dit uniquement si le mot de r√©veil a √©t√© prononc√© ou non. Deuxi√®mement, nous pr√©chargeons tous les mod√®les √† l'avance et les laissons tourner sur notre appareil. Si nous voulions √©conomiser de l'√©nergie, nous ne chargerions chaque mod√®le qu'au moment o√π il est n√©cessaire, et nous le d√©chargerions par la suite. Troisi√®mement, il manque un mod√®le de d√©tection de l'activit√© vocale dans notre fonction de transcription, qui transcrit pendant une dur√©e fixe, parfois trop longue, parfois trop courte.

## G√©n√©ralisation ü™Ñ

Jusqu'√† pr√©sent, nous avons vu comment g√©n√©rer des sorties vocales avec notre assistant vocal Marvin. Pour terminer, nous allons montrer comment nous pouvons g√©n√©raliser ces sorties vocales au texte, √† l'audio et √† l'image.

Nous utiliserons [*Transformers Agents*](https://huggingface.co/docs/transformers/transformers_agents) pour cr√©er notre assistant.
*Transformers Agents* fournit une API de langage naturel au-dessus des biblioth√®ques ü§ó *Transformers* et *Diffusers*, interpr√©tant une entr√©e de langage naturel en utilisant un LLM avec des prompts soigneusement con√ßus, et en utilisant un ensemble d'outils pour fournir des sorties multimodales.

Allons-y et instan√ßons un agent. Il y a [trois LLM disponibles](https://huggingface.co/docs/transformers/transformers_agents#quickstart) pour *Transformers Agents*, dont deux sont open-source et gratuits sur le *Hub*. Le troisi√®me est un mod√®le d'OpenAI qui n√©cessite une cl√© API OpenAI. Nous utiliserons le mod√®le gratuit [Bigcode Starcoder](https://huggingface.co/bigcode/starcoder) dans cet exemple, mais vous pouvez √©galement essayer l'un ou l'autre des autres LLM disponibles :

```python
from transformers import HfAgent

agent = HfAgent(
    url_endpoint="https://api-inference.huggingface.co/models/bigcode/starcoder"
)
```

Pour utiliser l'agent, il suffit d'appeler `agent.run` avec notre prompt de texte. A titre d'exemple, nous allons lui faire g√©n√©rer une image d'un chat üêà (qui, esp√©rons-le, ait l'air un peu mieux que cet emoji) :

```python
agent.run("Generate an image of a cat")
```

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/generated_cat.png">
 </div>

<Tip>
    Notez que la premi√®re fois que vous appelez cette fonction, les poids du mod√®le sont t√©l√©charg√©s, ce qui peut prendre un certain temps en fonction de votre connexion internet.
</Tip>

C'est aussi simple que cela ! L'agent a interpr√©t√© notre prompt et a utilis√© [Stable Diffusion](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation) pour g√©n√©rer l'image, sans que nous ayons √† nous soucier du chargement du mod√®le, de l'√©criture de la fonction ou de l'ex√©cution du code.

Nous pouvons maintenant remplacer notre fonction de requ√™te LLM et l'√©tape de synth√®se de texte par notre *Transformers Agent* dans notre assistant vocal, puisque l'Agent va s'occuper de ces deux √©tapes pour nous :

```python
launch_fn()
transcription = transcribe()
agent.run(transcription)
```

Essayez de prononcer le m√™me prompt "G√©n√©rer une image d'un chat" (en anglais "Generate an image of a cat") et voyez comment le syst√®me s'en sort. Si vous posez √† l'agent une simple requ√™te de type question/r√©ponse, l'agent r√©pondra par un texte. Vous pouvez l'encourager √† g√©n√©rer des sorties multimodales en lui demandant de renvoyer une image ou de la parole. Par exemple, vous pouvez lui demander de g√©n√©rer une image d'un chat, la l√©gender et prononcer la l√©gende ("Generate an image of a cat, caption it, and speak the caption").

Bien que l'agent soit plus flexible que notre premi√®re it√©ration de l'assistant Marvin ü§ñ, cette g√©n√©ralisation peut conduire √† des performances inf√©rieures sur les requ√™tes standard de l'assistant vocal. Pour r√©cup√©rer des performances, vous pouvez essayer d'utiliser un LLM plus performant, comme celui d'OpenAI, ou d√©finir un ensemble d'[outils personnalis√©s](https://huggingface.co/docs/transformers/transformers_agents#custom-tools) sp√©cifiques √† la t√¢che d'assistant vocal.
