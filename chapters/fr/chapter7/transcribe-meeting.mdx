# Transcrire une r√©union

Dans cette derni√®re section, nous utiliserons le mod√®le Whisper pour g√©n√©rer la transcription d'une conversation ou d'une r√©union entre deux ou plusieurs locuteurs. Nous l'associerons ensuite √† un mod√®le de *s√©paration des locuteurs* pour pr√©dire "qui a parl√© quand". En faisant correspondre les horodatages des transcriptions Whisper avec les horodatages du mod√®le de s√©paration des locuteurs, nous pouvons pr√©dire une transcription de r√©union de bout en bout avec des heures de d√©but et de fin enti√®rement format√©es pour chaque locuteur. Il s'agit d'une version basique des services de transcription de r√©unions que vous avez pu voir en ligne sur [Otter.ai](https://otter.ai) et co :

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/diarization_transcription.png">
 </div>

## S√©paration des locuteurs

La s√©paration des locuteur (t√¢che de *diarization* en anglais) consiste √† prendre une entr√©e audio non √©tiquet√©e et √† pr√©dire "qui a parl√© quand".
Ce faisant, nous pouvons pr√©dire les horodatages de d√©but / fin pour chaque tour de parole, correspondant au moment o√π chaque locuteur commence √† parler et au moment o√π il termine.

ü§ó *Transformers* n'a actuellement pas de mod√®le de *diarization* inclus, mais il y a des *checkpoints* sur le *Hub* qui peuvent √™tre utilis√©s avec une relative facilit√©. Dans cet exemple, nous utiliserons le mod√®le pr√©-entra√Æn√© de [pyannote.audio](https://github.com/pyannote/pyannote-audio). Commen√ßons par l'installation du paquet avec pip :

```bash
pip install --upgrade pyannote.audio
```

Bien, les poids de ce mod√®le sont h√©berg√©s sur le *Hub*. Pour y acc√©der, nous devons d'abord accepter les conditions d'utilisation du mod√®le de *diarization* : [pyannote/speaker-diarization](https://huggingface.co/pyannote/speaker-diarization). Et ensuite les conditions d'utilisation du mod√®le de segmentation : [pyannote/segmentation](https://huggingface.co/pyannote/segmentation).

Une fois termin√©, nous pouvons charger le pipeline de s√©paration des locuteurs localement sur notre appareil :

```python
from pyannote.audio import Pipeline

diarization_pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization@2.1", use_auth_token=True
)
```

Essayons-le sur un √©chantillon de fichier audio ! Pour cela, nous allons charger un √©chantillon du jeu de donn√©es [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr) qui consiste en deux locuteurs diff√©rents qui ont √©t√© concat√©n√©s ensemble pour donner un seul fichier audio :

```python
from datasets import load_dataset

concatenated_librispeech = load_dataset(
    "sanchit-gandhi/concatenated_librispeech", split="train", streaming=True
)
sample = next(iter(concatenated_librispeech))
```

Nous pouvons √©couter l'audio pour voir √† quoi cela ressemble :

```python
from IPython.display import Audio

Audio(sample["audio"]["array"], rate=sample["audio"]["sampling_rate"])
```

Nous pouvons clairement entendre deux locuteurs diff√©rents, avec une transition d'environ 15 secondes. Passons ce fichier audio au mod√®le de s√©paration pour obtenir les heures de d√©but et de fin du locuteur. Notez que pyannote.audio s'attend √† ce que l'entr√©e audio soit un tenseur PyTorch de la forme `(channels, seq_len)`, nous devons donc effectuer cette conversion avant d'ex√©cuter le mod√®le :

```python
import torch

input_tensor = torch.from_numpy(sample["audio"]["array"][None, :]).float()
outputs = diarization_pipeline(
    {"waveform": input_tensor, "sample_rate": sample["audio"]["sampling_rate"]}
)

outputs.for_json()["content"]
```

```text
[{'segment': {'start': 0.4978125, 'end': 14.520937500000002},
  'track': 'B',
  'label': 'SPEAKER_01'},
 {'segment': {'start': 15.364687500000002, 'end': 21.3721875},
  'track': 'A',
  'label': 'SPEAKER_00'}]
```

Cela semble plut√¥t bien ! Nous pouvons voir que le premier locuteur est pr√©dit comme parlant jusqu'√† la marque de 14,5 secondes, et le second locuteur √† partir de 15,4s. Il ne nous reste plus qu'√† obtenir notre transcription !

## Transcription de la parole

Pour la troisi√®me fois dans cette unit√©, nous allons utiliser le mod√®le Whisper pour notre syst√®me de transcription de la parole. Plus pr√©cis√©ment, nous chargerons le *checkpoint* [Whisper Base](https://huggingface.co/openai/whisper-base), car il est suffisamment petit pour donner une bonne vitesse d'inf√©rence avec une pr√©cision de transcription raisonnable. Comme auparavant, vous pouvez utiliser n'importe quel *checkpoint *de reconnaissance vocale disponible sur le [*Hub*](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&library=transformers&sort=trending), y compris Wav2Vec2, MMS ASR ou d'autres tailles de Whisper :

```python
from transformers import pipeline

asr_pipeline = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-base",
)
```

Obtenons la transcription de notre √©chantillon audio, en retournant √©galement les horodatages au niveau du segment afin que nous connaissions les heures de d√©but et de fin de chaque segment. Vous vous souviendrez de l'unit√© 5 que nous devons passer l'argument `return_timestamps=True` pour activer la t√¢che de pr√©diction de l'horodatage de Whisper :

```python
asr_pipeline(
    sample["audio"].copy(),
    generate_kwargs={"max_new_tokens": 256},
    return_timestamps=True,
)
```

```text
{
    "text": " The second and importance is as follows. Sovereignty may be defined to be the right of making laws. In France, the king really exercises a portion of the sovereign power, since the laws have no weight. He was in a favored state of mind, owing to the blight his wife's action threatened to cast upon his entire future.",
    "chunks": [
        {"timestamp": (0.0, 3.56), "text": " The second and importance is as follows."},
        {
            "timestamp": (3.56, 7.84),
            "text": " Sovereignty may be defined to be the right of making laws.",
        },
        {
            "timestamp": (7.84, 13.88),
            "text": " In France, the king really exercises a portion of the sovereign power, since the laws have",
        },
        {"timestamp": (13.88, 15.48), "text": " no weight."},
        {
            "timestamp": (15.48, 19.44),
            "text": " He was in a favored state of mind, owing to the blight his wife's action threatened to",
        },
        {"timestamp": (19.44, 21.28), "text": " cast upon his entire future."},
    ],
}
```

Ok, nous constatons que chaque segment de la transcription a une heure de d√©but et une heure de fin, avec un changement de locuteur au bout de 15,48 secondes. Nous pouvons maintenant associer cette transcription aux horodatages des locuteurs que nous avons obtenus √† partir de notre mod√®le de *diarization* pour obtenir notre transcription finale.

## Speechbox

Pour obtenir la transcription finale, nous allons aligner les timestamps du mod√®le de diarisation avec ceux du mod√®le Whisper.
Le mod√®le de s√©paration pr√©dit que le premier locuteur se termine √† 14,5 secondes et que le second locuteur commence √† 15,4 secondes, alors que Whisper pr√©dit les limites des segments √† 13,88, 15,48 et 19,44 secondes respectivement. Comme les timestamps de Whisper ne correspondent pas parfaitement √† ceux du mod√®le de s√©paration, nous devons trouver lesquelles de ces limites sont les plus proches de 14,5 et 15,4 secondes, et segmenter la transcription par locuteur en cons√©quence. Plus pr√©cis√©ment, nous trouverons l'alignement le plus proche entre les horodatages de la s√©paration et de la transcription en minimisant la distance absolue entre les deux.

Heureusement pour nous, nous pouvons utiliser ü§ó *Speechbox* pour effectuer cet alignement. Tout d'abord, installons `speechbox` depuis *main* :

```bash
pip install git+https://github.com/huggingface/speechbox
```

Nous pouvons maintenant instancier notre pipeline combin√© de *diarization* et de transcription, en passant nos deux mod√®les √† la classe [`ASRDiarizationPipeline`](https://github.com/huggingface/speechbox/tree/main#asr-with-speaker-diarization) :

```python
from speechbox import ASRDiarizationPipeline

pipeline = ASRDiarizationPipeline(
    asr_pipeline=asr_pipeline, diarization_pipeline=diarization_pipeline
)
```

<Tip>
    Vous pouvez √©galement instancier le `ASRDiarizationPipeline` directement √† partir d'un mod√®le pr√©-entra√Æn√© en sp√©cifiant l'identifiant d'un mod√®le ASR sur le *Hub* : `pipeline = ASRDiarizationPipeline.from_pretrained("openai/whisper-base")`
</Tip>

Passons le fichier audio au pipeline et voyons ce que nous obtenons :

```python
pipeline(sample["audio"].copy())
```

```text
[{'speaker': 'SPEAKER_01',
  'text': ' The second and importance is as follows. Sovereignty may be defined to be the right of making laws. In France, the king really exercises a portion of the sovereign power, since the laws have no weight.',
  'timestamp': (0.0, 15.48)},
 {'speaker': 'SPEAKER_00',
  'text': " He was in a favored state of mind, owing to the blight his wife's action threatened to cast upon his entire future.",
  'timestamp': (15.48, 21.28)}]
```

Excellent ! Le premier locuteur est segment√© comme parlant de 0 √† 15,48 secondes, et le second locuteur de 15,48 √† 21,28 secondes, avec les transcriptions correspondantes pour chacun.

Nous pouvons formater les horodatages de mani√®re un peu plus agr√©able en d√©finissant deux fonctions d'aide. La premi√®re convertit un *tuple* d'horodatages en une cha√Æne de caract√®res, arrondie √† un nombre d√©fini de d√©cimales. La seconde combine l'identifiant du locuteur, l'horodatage et les informations textuelles sur une seule ligne, et s√©pare chaque locuteur sur sa propre ligne pour faciliter la lecture :

```python
def tuple_to_string(start_end_tuple, ndigits=1):
    return str((round(start_end_tuple[0], ndigits), round(start_end_tuple[1], ndigits)))


def format_as_transcription(raw_segments):
    return "\n\n".join(
        [
            chunk["speaker"] + " " + tuple_to_string(chunk["timestamp"]) + chunk["text"]
            for chunk in raw_segments
        ]
    )
```

Ex√©cutons √† nouveau le pipeline, en formatant cette fois la transcription selon la fonction que nous venons de d√©finir :

```python
outputs = pipeline(sample["audio"].copy())

format_as_transcription(outputs)
```

```text
SPEAKER_01 (0.0, 15.5) The second and importance is as follows. Sovereignty may be defined to be the right of making laws.
In France, the king really exercises a portion of the sovereign power, since the laws have no weight.

SPEAKER_00 (15.5, 21.3) He was in a favored state of mind, owing to the blight his wife's action threatened to cast upon
his entire future.
```

Et voil√† ! Nous avons ainsi s√©par√© et transcrit notre audio et renvoy√© des transcriptions segment√©es par locuteur.
Bien que l'algorithme de distance minimale pour aligner les horodatages diaris√©s et les horodatages transcrits soit simple, il fonctionne bien dans la pratique. Si vous souhaitez explorer des m√©thodes plus avanc√©es pour combiner les horodatages, le code source de `ASRDiarizationPipeline` est un bon point de d√©part : [speechbox/diarize.py](https://github.com/huggingface/speechbox/blob/96d2d1a180252d92263f862a1cd25a48860f1aed/src/speechbox/diarize.py#L12).
